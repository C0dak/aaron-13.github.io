# NUMA架构

------

### 服务器系统架构

从系统架构来看服务器大体分为以下三类

**1. 即对称多处理器结构(SMP:Symmetric Multi-Processor)**

在SMP架构中，每个CPU对称工作，各CPU共享相同的物理内存，每个CPU访问内存中的任何地址所需时间是相同的，因此，SMP也被称为一致存储器访问结构(UMA: Uniform Memory Access)

对SMP服务器进行扩展的主要方法有：增加内存，更高的CPU工作频率，添加CPU，改善I/O性能。

但是，SMP架构中的所有资源(CPU,IO,I/O等)都是共享的，导致了它的扩展能力非常有限，最受限制的则是内存，由于每个CPU必须通过相同的内存总线访问相同的内存资源，因此随着CPU数量的增加，内存访问冲突将迅速增加，最终会造成CPU资源的浪费，使CPU性能的有效性大大降低，实验证明，SMP架构时，CPU利用率最好的情况时2到4个CPU。


**大规模并行处理结构(MPP: Massive Parallel Processing)**

由多个SMP服务器(每个SMP服务器称节点)通过节点互联网络连接而成，每个节点只访问自己的本地资源(内存，存储等)，是一种完全无共享(share Nothing)结构，因而扩展能力最好，理论上其扩展能力无限制。

在MPP系统中，每个SMP节点也可以运行自己的操作系统，数据库等，但是每个节点内的CPU不能访问另一个节点的内存，节点之间的信息交互只能通过节点互联网络实现。

但是MPP服务器还需要一种更复杂的机制来调度和平衡各节点的负载和并行处理过程。目前一些基于MPP基数的服务器往往通过系统级软件(如数据库)来屏蔽这种复杂性。

MPP的节点互连机制是在不同的SMP服务器外部通过I/O实现的，每个节点只访问本地内存和存储，节点之间的信息交互与节点本身的处理是并行进行的。因此，MPP在增加节点时性能基本上是可以实现线性扩展。


**非一致内存访问结构(NUMA: Non-Uniform Memory Access)**

![numa-01.png](https://aaron-13.github.io/images/numa-01.png)

在NUMA架构中有多个CPU模块，每个CPU模块由多个CPU组成，并且具有独立的本地内存，I/O槽口等。由于其节点之间可以通过互联模块(如称为Crossbar Switch)进行连接和信息互换，因此每个CPU可以访问整个系统的内存。显然，访问本地内存的速度将远远高于访问远地内存的速度，这也是非一致内存访问的由来。

从架构来看，NUMA与MPP具有很多相似之处，它们都由多个节点组成，每个节点都具有自己的CPU，内存，I/O，节点之间都可以通过节点互联机制进行信息交互，但是又有很大不同：

+ 节点互联机制
	NUMA的节点互联机制是在同一个物理服务器内部实现的，当某个CPU需要进行远地内存访问时，必须等待，这也是NUMA服务器无法实现CPU增加时性能线性扩展的主要原因

+ 内存访问机制
	在NUMA服务器内部，任何一个CPU可以访问整个系统的内存，但远地访问的性能远远低于本地内存访问，因此在开发过程中尽量避免远地内存访问



### NUMA的优势和局限性

1. 优势
	利用NUMA技术，可以较好解决原来SMP系统的扩展问题，在一个物理服务器内可以支持上百个CPU。如HP的Superdome，SUN15K，IBMp690等

2.局限性
	在NUMA架构中，由于访问远地内存的延时超过本地内存，因此当CPU数量增加时，系统性能无法线性增加。


### 处理器组

NUMA可以解决手动配置处理器组
![numa-02.png](https://aaron-13.github.io/images/numa-02.png)



## NUMA Memory Policy

memory policy是决定NUMA系统上从哪个节点分配内存的策略，它是一类提供给能更好利用NUMA系统进行内存分配的应用程序使用的编程接口。


Linux分四种类型的policy，分别是：

System Default Policy，它是在没用应用下面其他policy时模式policy，具体行为是：系统启动过程中，采用interleave策略分配内存。即在所有可满足需求的节点上交叉分配，防止启动时在某个节点上负载过重；在系统启动后，采用local allocation，即在task运行的CPU所在的node上进行内存分配。

Task/Process Policy，它是task用来指定其内存分配时的策略，如果没有定义，将fall back到system default policy

VMA Policy：它是指定在某段VMA进行内存分配时策略，如果没有定义，将fall back到Task/Process Policy，如果也没有定义则fall back到System default policy

Shared Policy：它是指定在分配某个内存对象时的policy，这个内存对象可能被多个task共享，而VMA policy只限定某个task的某段VMA


Linux内存分配方法包含三部分： mode，optional mode flags，an optional set of nodes

mode决定policy的具体行为，the optional mode flags决定mode的行为，an optional set of nodes可以看作是以上行为的参数


mode有四种：

+ Default Mode-MPOL_DEFAULT

+ MPOL_BIND 指定在哪几个节点上进行内存分配

+ MPOL_PREFERRED 指定首先在perferred的节点上进行内存分配，如果失败在搜索其他节点

+ MPOL_INTERLEAVED 指定在an optional set of nodes几个节点上，以页为单位，交叉分配内存


optional mode flags：

+ MPOL_F_STATIC_NODES: 指定在policy定以后，如果task或VMA设置的可分配nodes发生了改变，用户传递过来的nodemask不应被remap

+ MPOL_F_RELATIVE_NODES：与上个flag相反，该情况下，用户传递过来的nodemask应被remap


Linux为内存分配策略提供了三个API：

设置内存分配策略
	long set_mempolicy(int mode,const unsigned long *nmask,unsigned long maxnode);

GET内存分配策略及相关信息，flags决定mode的行为是get哪些信息
	long get_mempolicy(int *mode,const unsigned long *nmask,unsigned long maxnode,void *addr,int flags);

安装内存分配策略
	long mbind(void *start,unsigned long len,int mode,const unsigned long *nmask,unsigned long maxnode,unsigned flags);


Linux识别到NUMA架构后，默认的分配内存方式就是：优先尝试在请求线程当前所处的CPU的local内存上分配空间，如果local内存不足，优先淘汰local内存中无用的Page(Inactive,Unmapped)


[MySQL-The MySQL "swap insanity" problem and the effects of the NUMA architecture](http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/)

[PostgreSQL-PostgreSQL,NUMA and zone reclaim mode on linux](http://frosty-postgres.blogspot.com/2012/08/postgresql-numa-and-zone-reclaim-mode.html)

[Oracle-Non-Uniform Memory Access(NUMA) architecture with Oracle database by examples](http://blog.yannickjaquier.com/hpux/non-uniform-memory-access-numa-architecture-with-oracle-database-by-examples.html)

[Java-optimizing Linux Memory Management for Low-Latency/High-throughput Database](http://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases)

究其原因都和:"因为CPU亲和策略导致的内存分配不平均"及"NUMA Zone Claim内存回收"有关，而和数据库种类并没有直接联系。


**MySQL在NUMA架构上会出现的问题**

[The MySQL "swap insanity" problem and the effects of the NUMA architecture](http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/)

[A brief on NUMA and MySQL](http://blog.jcole.us/2012/04/16/a-brief-update-on-numa-and-mysql/)


+ CPU规模因摩尔定律指数级发展，而总线发展缓慢，导致多核通过一条总线共享内存成为瓶颈

+ CPU访问自己Chip上所插的内存时速度快，而访问其他CPU所关联的内存的速度相较慢三倍左右

+ 于是Linux内核默认使用CPU亲和的内存分配策略，使内存页尽可能的和调用线程处在同一个Core/Chip中

+ 由于内存页没有动态调整策略，使得大部分内存页都集中在CPU 0上，又因为Reclaim默认策略优先淘汰/Swap本Chip上的内存，使得大量有用内存被换出，当被换出页被访问时问题就以数据库响应时间飙高甚至阻塞的形式出现

![numa-05.png](https://aaron-13.github.io/images/numa-05.png)


**解决方案**

```
numctl --interleave=all
```

在MySQL进程启动前，使用`sysctl -q -w vm.drop_caches=3`清空文件缓存所占用的空间

Innodb在启动时，就完成整个`Innodb_buffer_pool_size`的内存分配


不过这套方案也只是减少了NUMA内存分配不均，导致的MySQL Swap问题出现的可能性，如果当系统上其他进程，或者MySQL本身需要大量内存时，Innodb Buffer Pool的那些Page同样还是会被Swap到存储上，于是又在此基础上出现了另外几个进阶方案：

配置`vm.zone_reclaim_mode=0`使得内存不足时去remote memory分配优先于swap out local page

`echo -15 > /proc/<pid_of_mysqld>/oom_adj`调低MySQL进程被OOM_killer强制Kill的可能
当值为-17时，MySQL进程就不会被OOM_killer强制杀掉

memlock

对MySQL使用Huge Page



**NUMA Interleave真的好么**

![numa-06.png](https://aaron-13.github.io/images/numa-06.png)

Interleave模式下的程序性能要比默认的亲和模式要高，有时甚至高达30%。究其原因是Linux服务器大多数workload分布都是随机的: 即每个线程在处理各个外部请求对应的逻辑时，所需要访问的内存是在物理上随机分配的。而Interleave模式就恰恰是针对这种特性将内存page随机打散到各个CPU core上，使得每个CPU的负载和Remote Access的出现频率都是均匀分布。相较NUMA默认的内存分配模式，死板的把内存都优先分配在线程所在Core上的做法，显然普遍适用性要强很多。

也就是说，像MySQL这种外部请求随机性强，各线程访问内存在地址了上平均分布的这种应用，Interleave的内存分配模式相较默认模式可以带来一定程度的性能提升。

真正造成程序在NUMA系统上性能瓶颈的并不是Remote Accesss带来的响应时间损耗，而是内存的不合理分布导致Remote Access将inter-connect这个小水管塞满所造成的结果。而Interleave恰好，把这种不合理分布情况下的Remote Access请求平均分布在各个水管中，所以这也是Interleave效果好的一个原因。

目前Linux的内存分配机制在NUMA架构的CPU上还有一定的改进空间，例如: Dynamic Memory Location,Page replication。


**Dynamic Memory Location**

MySQL的线程分为两种，用户线程(SQL 执行线程)和内部线程(内部功能，如：flush，io，master等)。对于用户线程来说随机性相当强，但对于内部线程来说他们的行为以及要访问的内存区域其实是相对固定且可以预测的。如果能对于这把这部分内存集中到这些内存线程所在的Core上的时候，就能减少大量Remote Access，潜在的提升如Page flush，Purge等功能的吞吐量，甚至可以提高MySQL Crash后Recovery的速度(Recovery是单线程)
Dynamic Memory Location这种技术还停留在实验阶段，其难点：要做到按照线程的行为动态的调整Page在memory的分布，就势必要做线程和内存的实时监控(profile)。对于Memory Access这种非常异常频繁的底层操作来说增加profile入口的性能损耗是极大的。Carrefour算法和Linux社区的Auto NUMA patch都是积极尝试，什么时候profile出现硬件级别，类似于CPU中PMU时，动态内存规划就会展现很大的价值。


**Page Replication**

一些动态加载的库，把他们放在任何一个线程所在CPU都会导致其他CPU上线程的执行效率下降。而这些共享数据往往读写比非常高，如果能把这些数据的副本在每个Memory Zone内都放置一份，理论上会带来较大的性能提升，同时也减少在inter-connect上出现的瓶颈，实际上，carrefour也做了这样的尝试，但缺乏硬件级别和操作系统原生级别的支持，Page Replication在数据一致性上维护的成本显得比带来的提升更多。


NUMA是CPU发展的一种必然趋势，但是NUMA的出现使得操作系统不得不关注内存访问速度不平均的问题。
分配策略的初衷是好的，为了内存更接近需要他的线程，但是没有考虑到数据库这种大规模内存使用的应用场景，同时缺乏动态调整的功能，使得这种悲剧在内存分配的那一刻就埋下了伏笔。