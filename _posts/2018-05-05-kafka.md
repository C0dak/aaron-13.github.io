# Kafka

------

##背景介绍

### 简介
Kafka是一种分布式的，基于发布/订阅的消息系统，主要设计目标如下:
1. 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级别以上数据也能保证常数时间的访问性能
2. 高吞吐率，即使在非常连接的机器上也能做到单机支持每秒100k条消息的传输
3. 支持Kafka Server间的消息区分,及分布式消费，同时保证每个partition内的消息顺序传输
4. 同时支持离线数据处理和实时数据处理

### 为何要用消息系统
- 解耦
    在项目启动之初预测项目后面的需求是困难的。消息队列处理过程中间插入了一个隐含的，基于数据的接口层，两边的处理过程都要实现这一接口。允许独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束

- 冗余
    有些情况下，处理数据的过程会失败，除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化知道它们已经被完全处理，通过这一方式规避了数据丢失的风险。

- 扩展性
    因为消息队列解耦了处理过程，所以增大消息入队和处理的频率很容易，只要另外增加处理过程即可

- 灵活性和峰值处理能力
    当访问量剧增时候，应用仍然能继续发挥作用。但是这样的突发流量并不常见；使用消息队列能够使关键组件顶住突发的访问压力，而不会因为超负荷请求而崩溃

- 可恢复性
    当体系的一部分组件失效，不会影响整个系统。消息队列降低了进程间的耦合度，所以即使另一个处理消息的进程挂掉，加入队列中的消息仍然可在系统恢复后被处理

- 送达保证
    消息队列提供的冗余机制保证了消息能被实际的处理，只要一个进程读取了该队列即可。在此基础上，部分消息系统提供了一个"只送到一次"保证。无论有多少进程在从队列中领取数据，每个消息只能被处理一次。

- 顺序保证
    在大多使用场景下，数据处理的顺序都很重要。消息队列本来就是排序的，并且能保证数据按照特定的顺序来处理。部分消息系统保证消息通过FIFO的顺序来处理，因此消息在队列中的位置就是从队列中检索他们的位置

- 缓冲
    在任何重要的系统中，都会有需要不同的处理时间的元素。消息队列通过一个缓冲层来帮助任务最高效率的执行--写入队列的处理会尽可能的快速，而不受从队列读的预备处理的约束。该缓冲有助于控制盒优化数据流经过系统的速度

- 理解数据流
    在一个分布式系统里，要得到一个关于用户操作会用多长时间及其原因的总体印象，是比较困难的。消息队列通过消息被处理的频率，来方便的辅助确定那些表现不佳的处理过程和领域

- 异步通信
    消息队列提供了异步处理机制，允许把一个消息放入队列，但并不立即处理它。

### 常用Message Queue对比

    + RabbitMQ
        RabbitMQ是使勇Erlang编写的一个开源的消息队列，本身支持很多的协议： AMQP，XMPP，SMTP，STOMP，它非常重量级，更适合企业级的开发。
        同时实现了Broker架构，这意味着消息在发送给客户端时先在中心队列排队。对路由，负载均衡或者数据持久化都有很好的支持

    + Redis
        Redis是一个基于Key-Value对的NoSQL数据库。本身支持MQ功能，可以当做一个轻量级的队列服务来使用。对于RabbitMQ和Redis的入队和出队操作，各执行100万次，每10万次记录一次执行时间，数据分别为128bytes，512bytes，1k，10k。实验表明：入队时，当数据比较小时Redis的性能高于RabbitMQ，而如果数据大小超过10K，Redis则慢的无法忍受，出队时，redis性能好于MQ

    + ZeroMQ
        ZeroMQ号称最快的消息队列系统，尤其针对大吞吐量的需求场景。ZMQ能够实现RabbitMQ不擅长的高级复杂队列，但需要自己组合多种技术框架。ZeroMQ具有一个独特的非中间件的模式，不需要安装和运行一个消息服务器或中间件，因为应用程序将扮演这个服务角色。只需要引用ZeroMQ程序库，可以使用NuGet安装，然后就可以在应用程序之间发送消息。但是ZeroMQ仅提供非长久性的队列，宕机会导致数据丢失。

    + ActiveMQ
        类似于ZeroMQ，能够以代理人和点对点的技术实现队列
        类似于RabbitMQ，少量代码就可以高效实现高级应用场景

    + Kafka/Jafka
        一个高性能跨语言分布式发布/订阅消息队列系统，Jafka是Kafka的一个升级版。具有以下特征：快速持久化，可以在O(1)的系统开销下进行消息持久化；高吞吐，在一台普通的机器上就能实现10W/s的吞吐速率；完全的分布式系统，Broker，Producer，Consumer都原生自动支持分布式，自动实现负载均衡；支持Hadoop数据并行加载。

## Kafka解析

### Terminology术语
    - Broker
        Kafka集群包含一个或多个服务器，这种服务器被称为broker

    - Topic
        每条发布到Kafka集群的消息都有一个类别，这个类别被称为topic。(物理上不同topic的消息分开存储，逻辑上一个topic的消息虽保存于一个或多个borker，单用户只需指定消息的topic即可生产或消费数据而不必关心数据存储于何处)

    - Partition
        partition是物理上的概念，每个topic包含一个或多个partition，创建topic时，可指定partition数量，每个partition对应于一个文件夹，该文件夹下存储该partition的数据和索引文件

    - Consumer
        消费消息，每个consumer属于一个特定的consumer group(可为每个consumer指定group name，若不指定group name则属于默认的group)。使用consumer high level API时，同一topic的一条消息只能被同一个consumer group内的consumer消费，但多个consumer group可同时消费这一消息

## Kafka架构
![Kafka](https://aaron-13.github.io/images/kafka_structure.png)

### Push vs Pull
    作为一个messaging system，Kafka遵循传统的方式，选择有producer向broker push消息并由consummer从broker pull消息。一些logging-centric system，比如Facebook的Scribe和Cloudera的Flume采用非常不同的push模式。
    push模式很难使用消费速率不同的消费者，因为消息发送速率是由broker决定的。Push模式的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型表现就是网络堵塞，拒绝服务。Pull模式则可以根据consumer的消费能力以适当的速率消费消息

### Topic & Partition
    Topic在逻辑上可以被认为是一个queue。每条消费都必须指定它的topic，可以简单理解为必须指明要把这条消息放进哪个queue里。为了使得Kafka的吞吐率可以水平扩展，物理上把topic分成一个或多个partition，每个partition在物理上对应一个文件夹。该文件夹中存储这个partition的所有消息和索引

    每条消息被发送到broker时，会根据partition规则选择被存储到哪一个partition。如果partition规则设置的合理，所有消息可以均匀分布到不同的partition里，这样就实现了水平扩展。在创建topic时，可以在$KAFKA_HOME/config/server.properties中指定这个partition的数量，当然也可以在topic创建之后去修改partition数量
    `num.partitions=3`
    因为Kafka读取特定晓得时间复杂度O(1)，即与文件大小无关，所以这里删除文件与fakfa性能无关，选择赠言的删除策略只与磁盘以及具体的需求有关。另外。Kafka会为每一个Consumergroup保留一些metadata信息--当前消费的信息的position，也即offsett。这个offset由consumer控制。正常情况下，consumer会在消费完一条消息后线性增加这个offset。当然，consumer可将offset设成一个较小的值，重新消费一些消息，因为offset由consumer控制，所以Kafka broker是无状态的，不需要标记哪些消息被那些consumer过，不需要通过broker去保证同一个consumer group只有一个consumer能消费某一条消息，因此也就不需要锁机制，这也为Kafka的高吞吐率提供了有力保障

### Replication & Leader election
    Kafka从0.8开始提供partition级别的replication，replication的数量可在$KAFKA_HOME/config/server.properties中配置
    `default.repliation.factor=1`
    该Replication与leader election配置提供了自动的failover机制。replication对Kafka的吞吐率是有一定影响的，但极大地增强了可用性。默认，Kafka的replication数量是1。每个partition都有一个唯一的leader，所有的读写操作都在leader上完成，leader批量从leader上 pull数据。一般情况下partition的数量大于等于broker数量，并且所有partition的leader均匀分布在broker上。follower上的日志和其leader上的完全一样

    Kafka处理失败需要明确定定义一个broker是否alive。对于Kafka而言，Kafka存活包含两个条件，一是它必须维护与Zookeeper的session(通过zookeeper的heartbeat机制实现)，二是follower必须能够及时将leader的writing复制过来，不能落后太多。
    leader会track "in sync"的node list。如果一个follower宕机，或者落后太多，leader将把它从"in sync"list中移除。即followeer复制的消息落后于leader后的条数超过预定值，该值可在$KAFKA_HOME/config/server.properties中配置。
    ```
    replica.lag.max.messages=4000
    replica.lag.time.max.ms=10000
    ```
    Kafka只解决了"fail/recover",不处理"Byzantine"问题
    Kafka的复制机制既不是同步复制，也不是单纯的异步复制。"in sync"list的方式很好的均衡了确保数据不丢失以及吞吐率。follower可以批量的从leader复制数据，这样极大地提高复制性能，极大减少了follower与leader的差距


## Kafka集群的安装与使用

### 1.准备工作
    下载相关软件包
        JDK
        zookeeper
        kafka
### 2.搭建ZK集群
    **安装jdk**

    **配置安装zookeeper集群**
        mkdir zookeeper/server-{1,2,3}
        
        将zookeeper解压到三个目录中
        tar xf zookeeper.tar.gz -C ...

        在目录下分别创建data和datalog目录，用于存放快照日志和事务日志

    **修改zk配置文件**
        server1/conf/zoo.cfg
        ```plain
        tickTime=2000                       # 作为zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔
        initLimit=10                        # 配置zk接收客户端(即zk服务器集群中连接到leader的follower客户端)初始化连接最长容忍几个心跳间隔
        syncLimit=5                         # 标识Leader与Followerr之间发送消息，请求和应答时间长度，最长不能超过多少个tickTime
        dataDir=.../server1/data            # 快照日志存储路径
        dataLogDir=.../server1/datalog      # 事务日志的存储路径，如果不配置会会存储到dataDir中，严重影响zk性能
        clientPort=2181                     # 监听端口
        server.1=127.0.0.1:2888:3888        # 节点IP，交换数据端口，某个节点挂掉后专门用来选举的端口               
        server.2=127.0.0.1:2889:2889
        server.3=127.0.0.1:2890:2890
        ```
        三个配置文件基本相同，修改clientPort端口号

    **创建myid文件**
        # server1
        echo 1 > .../server1/data/myid

        # server2
        echo 2 > .../server2/data/myid

        # server3
        echo 3 > .../server3/data/myid

    **启动服务**
        .../server1/bin/zkServer.sh start 
        .../server2/bin/zkServer.sh start
        .../server3/bin/zkServer.sh start

        .../server1/bin/zkServer.sh status 查看状态


### 搭建Kafka集群
    **创建项目目录**
        mkdir kafka/kafkalog-{1,2,3}
    
    **将kafka解压到目录中**
        tar xf kafka.tar.gz -C ...

    **修改配置文件**
        server1.properties配置文件
        ```
        broker.id=1
        listeners=PLAINEXT://127.0.0.1:9092
        port=9092
        host.name=127.0.0.1
        log.dirs=.../kafka/kafkalog1
        zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183
        ```
        server2.properties和server3.properties配置文件和server1.properties差不多
        修改broker.id，port和log.dirs

    **启动服务**
        bin/kafka-server-start.sh -daemon config/server1.properties
        bin/kafka-server-start.sh -daemon config/server2.properties
        bin/kafka-server-start.sh -daemon config/server3.properties

    **测试服务**
        创建一个test主题，分区数是3，副本数3
        bin/kafka-topics.sh --create --zookeeper 127.0.0.1:2181 --replication --partitions 3 --topic test

        启动生产者
        bin/kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic test

        启动消费者
        bin/kafka-console-consumer.sh --zookeeper 127.0.0.1:2181 --topic test --from-beginning

        其他命令
        后台启动
        bin/kafka-server-start.sh -daemon config/serrver1.properties

        停止命令
        bin/kafka-server-stop.sh -daemon config/server1.properties

        查看topic列表
        bin/kafka-topics.sh --list --zookeeper 127.0.0.1:2181

        创建topic
        bin/kafka-topics.sh --create --zookeeper 127.0.0.1:2182 --replication-factor 1 --partitions 5 --topic test

        查看test主题
        bin/kafka-topics.sh --describe --zookeeper 127.0.0.1:2181 --topic test

        为主题test增加两个分区数量
        bin/kafka-topics.sh --alter --zookeeper localhost:2181 --topic test --partitions 2

        启动生产者
        bin/kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic test

        启动消费者 
        --from-beginning代表启动后将这个主题之前的消息都显示出来
        bin/kafka-console-consumer.sh --zookeeper 127.0.0.1:2181 --topic test --from-beginning

        查看消费者列表
        bin/kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --lsit

        查看某个消费者组的偏移量offset
        bin/kafka-consumer-groups.sh --bootstrap-server 192.168.1.1:9092 --describe --group my-group


## Kafka管理界面
[kafka-manager](https://github.com/yahoo/kafka-manager)    




















