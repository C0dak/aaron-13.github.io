# 输出插件

------

示例

```
output {
	elasticsearch {
		hosts => ["192.168.1.1"]
		index => "logstash-%{type}-%{+YYYY.MM.dd}"
		document_type => "%{type}"
		flush_size => 20000
		idle_flush_time => 10
		sniffing => true
		template_overwrite => true
	}
}
```

**批量发送**

过去的版本中，主要由插件的`flush_size`和`idle_flush_time`两个参数共同控制LogStash向Elasticsearch发送批量数据的行为。LogStash会在20000条数据时一次性发送出去，如果10秒钟之内没有20000条，也全部发送

默认情况下，flush_size是500条，idle_flush_time是1秒。

从5.0开始，这个行为有了另一个前提: `flush_size`的大小不能超过LogStash运行时的命令行参数设置的`batch_size`,否则将以`batch_size`为批量发送的大小


**索引名**

写入的ES索引的名称，这里可以使用变量。为了更贴合日志场景，LogStash提供了`%{+YYYY.MM.dd}`这种写法。在语法解析的时候，看到以+号开头的，就会自动认为后面是时间格式，尝试用时间格式来解析后续字符串。

此外，注意索引名中不能有大写字母，否则ES在日志中会报InvalidIndexNameException，但是LogStash不会报错。


**轮询**

LogStash1.4.2在transport和http协议的情况下是固定连接指定host发送数据。从1.5.0开始，host可以设置数组，它会从节点列表中选取不同的节点发送数据。达到Round-Robin负载均衡的效果。


一个小集群里，使用node协议最方便。Logstash以Elasticsearch的client节点身份(不存数据不参加选举)运行。

![elk-01.png](https://aaron-13.github.io/images/elk-01.png)

LogStash-1.5以后，也不再分发一个内嵌的elasticsearch服务器。如果想变更node协议下的这些配置，在`$PWD/elasticsearch.yml`文件里写自定义配置即可，logstash会尝试自动加载这个文件。

对于拥有很多索引的大集群，可以使用transport协议。logstash进程会转发所有数据到指定的某台主机上。node协议下的进程是可以接收到整个elasticsearch集群状态信息的，当进程收到一个事件时，就知道这个时间应该存在集群内哪个机器的分片上，所以，会直接连接该机器发送这条数据。而transport协议下的进程不会保存这个信息。在集群状态更新(节点变化，索引变化 都会发送全量更新)时，就不会对所有的logstash进程也发送这种信息。

+ output端顺序执行，没有对日志type进行判断的各插件配置都会全部执行一次，在output段对typt进行判断的语法:

```
output {
	if [type] == "nginxaccess" {
		elasticsearch { }
	}
}

```


**模板**

Elasticsearch支持给索引预定义设置和mapping。LogStash自带一个优化好的模板:

```
{
    "template" : "logstash-*",
    "version" : 50001,
    "settings" : {
        "index.refresh_interval" : "5s"
    },
    "mappings" : {
        "_default_" : {
            "_all" : {"enabled" : true, "norms" : false},
            "dynamic_templates" : [ {
                "message_field" : {
                    "path_match" : "message",
                    "match_mapping_type" : "string",
                    "mapping" : {
                        "type" : "text",
                        "norms" : false
                    }
                }
            }, {
                "string_fields" : {
                    "match" : "*",
                    "match_mapping_type" : "string",
                    "mapping" : {
                        "type" : "text", "norms" : false,
                        "fields" : {
                            "keyword" : { "type": "keyword"  }
                        }
                    }
                }
            }  ],
            "properties" : {
                "@timestamp": { "type": "date", "include_in_all": false  },
                "@version": { "type": "keyword", "include_in_all": false  },
                "geoip"  : {
                    "dynamic": true,
                    "properties" : {
                        "ip": { "type": "ip"  },
                        "location" : { "type" : "geo_point"  },
                        "latitude" : { "type" : "half_float"  },
                        "longitude" : { "type" : "half_float"  }
                    }
                }
            }
        }
    }
}

```


+ template for index-pattern
只有匹配`logstash-*`的索引才会应用这个模板。可以把自定义的名字放在`logstash-`后面，变成`index => "logstash-custom-%{+YYYY.MM.dd}"`

+ refresh_interval for indexing
Elasticsearch是一个近实时搜索引擎。实际上是每1秒钟刷新一次数据。对于日志分析应用，可以将logstash自带的模板修改成5秒钟。还可以根据需要继续放大刷新间隔，提高数据写入性能

+ multi-field with keyword
Elasticsearch会自动使用自己的默认分词器(空格，点，斜线等分割)来分析字段。分词器对于搜索和评分非常重要，但是大大降低了索引写入和聚合请求的性能。所以logstash模板定义了一种叫"多字段"(multi-field)类型的字段。这种类型会自动添加一个".keyword"结尾的字段，并给这个字段设置为不启用分词器。简单说，你想获取url字段的聚合结果的时候，不要直接使用"url"，而是用"url.keyword"作为字段名。当你还对分词字段发起聚合和排序请求的时候，直接提示无法构建fielddata了。
在Logstash 5.0中，同时还保留携带了针对Elasticsearch 2.x的template文件，在那里，通过旧版本的mapping配置，达到和新版本相同的行为效果:对应统计字段明确设置"index":"not_analyzed","doc_values":true,以及对分此字段加上对fielddata的{"format":"disabled"}。

+ geo_point
Elasticsearch支持geo_point类型，geo distance聚合等等。比如：请求某个geo_point点方圆10千米内数据点的总数。在Kibana的tilemap类型面板中，就会用到这个类型的数据

+ half_float
Elasticsearch 5.0新引入了half_float类型，比标准的float类型占用更少的资源，提供更好地性能。


**其他模板配置建议**

+ order
如果想定制template，有以下几种选择:
1 在logstash/outputs/elasticsearch配置中开启manage_template => false选项

2 在logstash/outputs/elasticsearch配置中开启 template => "/path/to/tmpl.json"选项，让logstash发送写的template文件

3 避免变更logstash里的配置，而是另外发送一个template，利用elasticsearch的templates order功能

Elasticsearch在创建一个索引的时候，如果发现这个索引同时匹配上了多个template，那么就会先应用order数值小的template设置，然后再应用一遍order数值高的最为覆盖，最终达到一个merge的效果

```
{
	"ordre": 1,
	"template": "logstash-*",
	"settings": {
		"index.refresh_interval": "20s"
	}
}
```

然后运行 `curl -XPUT http://localhost:9200/_template/template_newid -d '@/path/to/tmpl.json'`即可
logstash默认的模板，order是0，id是logstash。


### 发送邮件

示例
```
output {
	email {
		to => "admin@website.com,root@website.com"
		cc => "other@website.com"
		via => "smtp"
		subject => "Warning: %{title}"
		options => {
			smtpIporHost => "localhost",
			port => "25",
			domain => 'localhost.localdomain',
			userName => aaron,
			password => pass,
			authenticationType => aaron, # (plain,login and cram_md5)
			starttls => true
		}
		htmlbody => ""
		body => ""
		attachments => ['/path/to/filename']
	}
}
```

以上示例适用于logstash1.5，options =>的参数配置在Logstash2.0之后的版本已被移除

```
output {
	email {
		port => "25"
		address => "smtp.126.com"
		username => "test@126.com"
		password => "pass"
		authentication => "plain"
		use_tls => true
		from => "test@126.com"
		subject => "Warninng: %{title}"
		to => "test@qq.com"
		via => "smtp"
		body => "%{message}"
	}
}
```

outputs/email插件支持SMTP协议和sendmail两种方式，通过via参数设置



### 调用命令执行(Exec)
outputs/exec插件的运用如下，将logstash切割成的内容作为参数传递给命令。这样，在每个事件到达该插件的时候，都会触发这个命令的执行。

```
output {
	exec {
		command => "sendsms.pl \"%{message}\" -t %{user}"
	}
}

```

这种方式是每次都重新开始执行一次命令并退出。本身是比较慢速的处理方式(程序加载，网络建联等都有一定的时间消耗)。最好只用于少量的信息处理场景，比如不适用nagios的其他报警方式。



### 保存成文件(File)
通过日志收集系统将分散在数百台服务器上的数据集中存储在某中心服务器上。和Logstash::Inputs::File不同，Logstash::Outputs::File里可以使用sprintf format格式来自动定义输出带日期命名的路径

```
output {
	file {
		path => "/path/to/%{+YYYY.MM.dd}/%{host}.log.gz"
		message_format => "%{message}"
		gzip => true
	}
}

```

插件默认是输出这个event的JSON形式数据的。这可能与大多数情况下使用者的期望不符，可能只希望以原始格式保存。所以要定义为`%{message}`，前提在之前的filter插件中，没有使用remove_field或者update等参数删除或修改`%{message}`字段内容

gzip格式是一个非常奇特而友好的格式，其格式包括有:

+ 10字节的头，包含幻数，版本号以及时间戳

+ 可选的扩展头，如原文件名

+ 文件体，包括DEFAULT压缩的数据

+ 8字节的尾注，包括CRC-32校验和以及未压缩的原始数据长度


**注意**

1 按照Logstash标准，其实应该可以把数据格式的定义改在codec插件中完成，但是logstash-output-file插件内部实现中跳过了@codec.decode这步，所以`codec设置无法生效`

2 按照Logstash标准，配置参数的值可以使用event sprintf格式，但是logstash-output-file插件对event.sprintf(@path)的结果，还附加了一步`inside_file_root?`校验，这个file_root是通过直接对path参数分割/ 符号得到的，如果在sprintf格式中带有/符号，那么被切分后的结果就无法正确解析



### zabbix

bin/logstash-plugin install logstash-output-zabbix

信息以键值对的方式输出到zabbix服务器。事件的时间戳会自动关联zabbix项目

![zabbix-01.png](https://aaron-13.github.io/images/zabbix-01.png)

每个`host`是个标识，每个项目都关联到对应的主机。
现在该插件还不能批量发送数据。

该插件会记录一个警告如果一个必要的字段丢失，不会尝试重发，如果zabbix服务停止，但是会记录一个错误信息

![zabbix-02.png](https://aaron-13.github.io/images/zabbix-02.png)

+ codec 
codec类型的默认值是`plain`









