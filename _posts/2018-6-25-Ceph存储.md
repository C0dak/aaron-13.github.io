# CEPH

------

## Ceph简介
Ceph存储集群至少需要一个Ceph Monitor和两个OSD守护进程。而运行Ceph文件系统的客户端，必须要有元数据服务器(Metadata Server)
+ Ceph OSDs: Ceph OSD守护进程的功能是存储数据，处理数据的复制，恢复，回填，再均衡，并通过检查其他OSD守护进程的心跳来向Ceph Monitors提供一些监控信息。当Ceph存储集群设定为有2个副本时，至少需要2个OSD守护进程，集群才能达到active+clean状态(Ceph默认有3个副本，但可以调整副本数)
+ Monitors： Ceph Monitor维护着展示集群状态的各种图表，包括监视器图，OSD图，归置(PG)组,和CRUSH图。Ceph保存着发生在Monitors,OSD和PG上的每一次状态变更的历史信息(称为epoch)
+ MDSs： Ceph元数据服务器(MDS)为Ceph文件系统存储元数据(Ceph块设备和Ceph对象存储不使用MDS)。元数据服务器使得POSIX文件系统的而用户们，可以在不对Ceph存储集群造成负担的前提下，执行诸如ls，find等基本命令

Ceph把客户端数据保存为存储池内的对象。通过使用CRUSH算法，Ceph可以计算出哪个归置(PG)组应该持有指定的对象(Object),然后进一步计算出哪个OSD守护进程持有该归置组。CRSH算法使得Ceph存储集群能够动态的伸缩，再均衡和恢复

[CRUSH](https://aaron-13.github.io/2018/06/CRUSH算法)

### 硬件推荐
+ CPU
Ceph元数据服务器对CPU敏感，会动态地重分步它们的负载，所以元数据服务器应该有足够的处理能力(4 core+)
Ceph的OSD运行着RADOS服务，用CRUSH计算数据存放位置，复制数据，维护它自己的集群运行图副本，因此OSD要一定的处理能力(2core+)
监视器只简单维护着集群运行图的副本，因此对CPU不敏感；但必须考虑到机器以后是否还会运行Ceph监视器意外的CPU密集型任务。

+ RAM内存
元数据服务器和监视器必须可以尽快地提供它们的数据，应该有足够的内存，至少每进程1GB。OSD的日常运行不需要那么多内存(每个进程500MB左右)；然而在恢复期间它们占用内存较大(每TB数据需要约1TB需要1GB内存)，通常内存越多越好

+ 数据存储
来自操作系统的并行操作和到单个硬盘的多个守护进程并发读，写请求操作会极大地降低性能。文件系统局限性。文件系统局限性也要考虑： btfs尚未稳定到可以用于生产环境，但它可同时记日志并写入数据，而xfs和ext4不行

SSD用于存储对象太昂贵，但是把OSD的日志存到SSD，把对象数据存储到独立的硬盘可以明显提升性能。osd journal选项的默认值是/var/lib/ceph/osd/$cluster-$id/journal,你可以把它挂载到一个SSD或SSD分区，这样它就不再是和对象数据一样存储在同一个硬盘上的文件了。

提升CephFS文件系统性能的一种方法是从CephFS文件内容里分离出元数据。Ceph提供了默认的metadata存储池来储存CephFS元数据，所以不需要给CephFS元数据创建存储池，但是可以给它创建一个劲指向某主机SSD的CRUSH运行图。

OSD数量较多的主机上会派生出大量线程，尤其是在恢复和重均衡期间。很多Linux内核默认的最大线程数较小，如果遇到这类问题，可以把kernel.pid_max值调高
```
kernel.pid_max = 4194303
```

最低配置
ceph-osd
+ processor: 64bit AMD-64
+ RAM: ~1GB for 1TB of storage per daemon
+ Volume Storage: 1x storage dirve per daemon
+ Journal: 1x SSD partition per daemon(optional)
+ Network: 2x 1GB Ethernet NICs

ceph-mon
+ Processor: 64bit AMD-64
+ RAM: 1GB per daemon
+ Disk Space: 10GB per daemon
+ Network: 2x 1GB Ethernet NICs

ceph-mds
+ Processor: 64bit AMD-64
+ RAM: 1GB minimum per daemon
+ Disk Space: 1MB per daemon
+ Network: 2x 1GB Ethernet NICs


## 安装
```
CentOS 7
sudo subscription-manager repos --enable=rhel-7-server-extras-rpms

sudo yum install -y yum-utils && sudo yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ && sudo yum install --nogpgcheck -y epel-release && sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 && sudo rm /etc/yum.repos.d/dl.fedoraproject.org*

ceph.repo
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-{ceph-release}/{distro}/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

yum update && sudo yum install ceph-deploy

```

### CEPH节点安装
```
安装NTP
yum install ntp ntpdate ntp-doc

安装SSH服务器
yum install openssh-server

创建CEPH用户
ceph-deploy工具必须以普通用户登录Ceph节点，且此用户拥有无密码使用sudo的权限，因为它需要在安装软件及配置文件过程中，不必输入密码
较新版的ceph-deploy支持使用--username选项提供可无密码使用sudo的用户名。使用ceph-deploy --username {username}命令时，指定的用户必须能通过无密码SSH连接到Ceph节点，因为Ceph-deploy中途不会提示输入密码。
建议在集群内的所有ceph节点上给ceph-deploy创建特定的用户。不要用"ceph"这个名称。用户名"ceph"保留给Ceph守护进程。如果Ceph节点上已经有"ceph"用户，升级前必须先删除这个用户

1.在各Ceph节点创建新用户
ssh user@ceph-server
sudo useradd -d /home/{username} -m {username}
sudo passwd {username}

2.确保各Ceph节点上新创建的用户有sudo权限
echo "{username} ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/{username}
sudo chmod 0440 /etc/sudoers.d/{username}

允许无密码SSH登录
1.生成SSH密钥对，但不要用sudo或root用户。
ssh-keygen

Generating public/private key pair.
Enter file in which to save the key (/ceph-admin/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /ceph-admin/.ssh/id_rsa.
Your public key has been saved in /ceph-admin/.ssh/id_rsa.pub.

2.将公钥拷贝到各Ceph节点
ssh-copy-id -i ~/.ssh/id_rsa.pub username@node1
ssh-copy-id {username}@node2
ssh-copy-id {username}@node3

3.推荐做法: 修改ceph-deploy管理节点上的~/.ssh/config文件，这样ceph-deploy就能用创建的用户名登录ceph节点了，而无需每次执行cephp-deploy都要指定--username {username}。
Host node1
    Hostname node1
    User {username}
Host node2
    Hostname node2
    User {username}
Host node3
    Hostname node3
    User {username}

开放端口
firewalld
sudo firewall-cmd --zone=public --add-port=6789/tcp --permanent

iptables
sudo iptables -A INPUT -i {iface} -p tcp -s {ip-address}/{netmask} --dport 6789 -j ACCEPT

sbin/service iptables save

SELINUX
sudo setenforce 0

优先级/首选项
sudo yum install yum-plugin-priorities --enablerepo=rhel-7-server-optional-rpms
```

## 存储集群快速入门
`mkdir my-cluster && cd my-cluster`
*如果ceph节点默认设置了requiretty会遇到报错。可禁用此功能: 执行sudo visudo，找到Defaults requiretty选项，把它改为Defaults:ceph !requiretty*

### 创建集群
```
如果配置错误，可用以下命令清除配置
ceph-deploy purgedata {ceph-node} [{ceph-node}]
ceph-deploy forgetkeys

将安装包一起清除
ceph-deploy purge {ceph-node} [{ceph-node}]

如果执行了purge，必须重新安装ceph

在管理节点上，进入目录，用ceph-deploy执行如下步骤:
1.创建集群
    ceph-deploy new {initial-monitor-node(s)}
    在当前目录下，应该有一个Ceph配置文件，一个Monitor密钥环和一个日志文件

2.把Ceph配置文件的默认副本数从3改为2，这样只有两个OSD也可以达到active+clean状态，把下行加入到[global]段：
    osd pool default size = 2

3.如果有多个网卡，可以把public network写入ceph配置文件global段：
    public network = {ip-address}/{netmask}

4.安装ceph
    ceph-deploy install {ceph-node} [{ceph-node} ... ]
    如： ceph-deploy install admin-node node1 node2 node3
    ceph-deploy将在各节点安装Ceph。注：如果执行ceph-deploy purge，必须重新执行这一步来安装ceph

5.配置初始monitor(s),并收集密钥
    ceph-deploy mon create-initial

完成这些操作后，当前目录会出现密钥环
    {cluster-name}.client.admin.keyring
    {cluster-name}.bootstrap-ods.keyring
    {cluster-name}.bootstrap-mds.keyring
    {cluster-name}.bootstrap-rgw.keyring

    只有在安装Hammer或更高版时才会创建bootstrap-rgw密钥环

    如果此步骤失败并报错，请确认ceph.conf中为monitor指定的IP为Public IP，而不是Private IP

1.添加两个OSD。使用目录代替硬盘用于OSD守护进程。如何为OSD及其日志使用独立硬盘或分区，参考http://docs.ceph.org.cn/rados/deployment/ceph-deploy-osd

    ssh node2
    sudo mkdir /var/local/osd0
    exit

    ssh node3
    sudo mkdir /var/local/osd1
    exit

    从管理节点执行ceph-deploy来准备OSD
    ceph-deploy osd prepare {ceph-node}:/path/to/directory
    如： ceph-deploy osd prepare node2:/var/local/osd0 node3:/var/local/osd1

    最后，激活OSD
    ceph-deploy osd activate {ceph-node}:/path/to/directory
    如： ceph-deploy osd activate node2:/var/local/osd0 node3:/var/local/osd1

2.用ceph-deploy把配置文件和admin密钥拷贝到管理节点和ceph节点。这样每次执行ceph命令行时就无需指定monitor地址和ceph.celient.admin.keyring
    ceph-deploy admin {admin-node} {ceph-node}
    如： ceph-deploy admin admin-node node1 node2 node3
    ceph-deploy和本地管理主机(admin-node)通信时，必须通过主机名可达，必要时可修改/etc/hosts,加入管理主机名

3.确保对ceph.client.admin.keyring有正确的操作权限
    sudo chmod +r /etc/ceph/ceph.client.admin.keyring

4.检查集群健康状态
    ceph health
```

### 扩展集群
```
将node1上添加一个OSD守护进程和一个元数据服务器。然后分别在node2和node3上添加Ceph Monitor，以形成Monitors的法定人数

添加OSD
    ssh node1
    sudo mkdir /var/local/osd2
    exit

然后，从ceph-deploy节点准备OSD
    ceph-deploy osd prepare {ceph-node}:/path/to/directory
    如: ceph-deploy osd prepare node1:/var/local/osd2

激活OSD
    ceph-deploy osd activate {ceph-node}:/path/to/directory

观察Ceph集群重均衡，归置组迁移过程
    ceph -w
```

### 添加元数据服务器
```
至少需要一个元数据服务器才能使用CephFS
    ceph-deploy mds create {ceph-node}
    如： ceph-deploy mds create node1

注意：当前生产环境下的Ceph只能运行一个元数据服务器。
```

### 添加RGW例程
```
要使用Ceph的Ceph对象网关组件，必须部署RGW例程。用以下方法创建RGW例程：
    ceph-deploy rgw create {gateway-node}
    如: ceph-deploy rgw create node1
注： 这个功能是从Hammer版和ceph-deploy v1.5.23才开始有的

RGW例程默认会监听7480端口，可以更改该节点ceph.conf内与RGW相关的配置
    [client]
    rgw frontends = civetweb port=80
    rgw frontedns = civetweb port=[::]:80
```

### 添加MONITORS
```
Ceph存储集群需要至少一个Monitor才能运行，为达到高可用，典型的Ceph粗初级群会运行多个Monitors，这样在单个Monitor失败时不会影响Ceph存储集群的可用性。Ceph使用PASOX算法，此算法要求有多半形成法定人数

新增两个监视器到Ceph集群
    ceph-deploy mon add {ceph-node}
    如： ceph-deploy mon add node2 node3

新增Monitor后，Ceph会自动开始同步并形成法定人数，可使用如下命令检查法定人数状态
    ceph quorum_status --format json-pretty

注：当ceph集群中有多个Monitor，各monitor主机上都应该配置NTP
```

### 存入/检出对象数据
要把对象存储到Ceph存储集群，客户端必须做到:
1.指定对象名
2.指定存储池

Ceph客户端检出最新集群运行图，用CRUSH算法计算出如何把对象映射到归置组，然后动态地计算如何把归置组分配到OSD。要定位对象，只需要对象名称和存储池名称即可，如：
    ceph osd map {poolname} {object-name}

### 练习
创建一个对象，用rados put命令加上对象名，一个有数据的测试文件路径，并指定存储池。如：
```
echo {Test-data} > testfile.txt
rados put {object-name} {file-path} --pool=data
rados put test-object-1 testfile.txt --pool=data
```

为确认Ceph存储集群存储了此对象，可执行：
`rados -p data ls`

定位对象：
```
ceph osd map {pool-name} {object-name}
ceph osd map data test-object-1

Ceph应该会输出对象位置，如：
osdmap e537 pool 'data' (0) object 'test-object-1' -> pg 0.d1743484 (0.4) -> up [1,0] acting [1,0]
```

使用'rados rm'命令可删除次测试对象
`rados rm test-object-1 --pool=data`


## 块设备快速入门
块设备也叫RBD或RADOS块设备
可以在虚拟机上运行ceph-client节点，但是不能在Ceph存储集群(除非也用VM)相同的物理节点上执行下列步骤:

### 安装CEPH
```
1.确认使用合适的内核版本
    lsb_relaease
    uname -a

2.在管理节点上，通过ceph-deploy把Ceph安装到ceph-client节点
    ceph-deploy install ceph-client

3.在管理节点上，用ceph-deploy把Ceph配置文件和ceph.client.admin.keyring拷贝到ceph-client
    ceph-deploy admin ceph-client

    ceph-deploy工具会把密码换复制到/etc/ceph目录，要确保此密钥环文件有读权限(sudo chmod +r /etc/ceph/ceph.client.admin.keyring)
```

### 安装块设备
```
1.在ceph-client节点上创建一个块设备image
    rbd create foo --size 4096 [-m {mon-ip}] [-k /path/to/ceph.client.admin.keyring]

2.在ceph-client节点上，把image映射为块设备
    sudo rbd map foo --name client.admin [-m {mon-IP}] [-k /path/to/ceph.client.admin.keyring]

3.在ceph-client节点上，创建文件系统后就可以使用块设备了
    sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo

4.在ceph-client节点上挂载此文件系统
    sudo mkdir /mnt/ceph-block-device
    sudo mount /dev/rbd/rbd/foo /mnt/ceph-block-device
    cd /mnt/ceph-block-device
```


## CEPH文件系统快速入门

### 准备工作
```
1.确认使用合适的内核版本
    lsb_release -a
    uname -r

2.在管理节点上，通过ceph-deploy把Ceph安装到ceph-client节点上
    ceph-deploy install ceph-client

3.确保Ceph 存储集群在运行，且处于active + clean状态，同时确保至少一个ceph元数据服务器在运行
    ceph -s [-m {monitor-ip-address}] [-k {path/to/ceph.client.admin.keyring}]
```

### 创建文件系统
```
ceph osd pool create cephfs_data <ps_num>
ceph osd pool create cephfs_metadata <pg_num>
ceph fs new <fs_name> cephfs_metadata cephfs_data
```

### 创建密钥文件
```
Ceph存储集群默认启动认证，应该有个包含密钥的配置文件(但不是密钥本身)，用下述方法获取某一用户的密钥:
1.在密钥环文件中找到与某用户对应的密钥，如:
    cat ceph.client.admin.keyring

2.找到用于挂载Ceph文件系统的用户，复制其密钥
    [client.admin]
        key = AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==

3.打开文本编辑器

4.把密钥粘贴进去

5.保存文件，并把其用户名name作为一个属性(如admin.secret)

6、确保此文件对用户有合适的权限，对其他用户不可见
```

### 内核驱动
```
把Ceph Fs挂载为内核驱动
    sudo mkdir /mnt/mycephfs
    sudo mount -t ceph {ip-addresss-of-monitor}:6789:/ /mnt/mycephfs

Ceph存储集群默认需要认证，所以挂载时需要指定用户名name和密钥文件secretfile，如：
    sudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs -o name=admin,secretfile=admin.secret

注： 从管理节点而非服务器节点挂载Ceph FS文件系统
```

### 用户空间文件系统(FUSE)
```
把Ceph FS挂载为用户空间文件系统(FUSE)
    sudo mkdir ~/mycephfs
    sudo ceph-fuse -m {ip-address-of-monitor}:6789 ~/mycephfs

Ceph存储集群默认要求认证，需指定相应的密钥环文件，除非它在默认位置(即/etc/ceph)
    sudo ceph-fuse -k ./ceph.client.admin.keyring -m 192.168.0.1:6789 ~/mycephfs
```

**Ceph FS还不像Ceph块设备和Ceph对象设备存储那么稳定**


## CEPH对象存储快速入门
从firefly(v0.80)起，Ceph存储集群显著地简化了Ceph对象网关的安装和配置。网关守护进程内嵌了Civetweb,无需额外安装web服务器或配置FastCGI。此外，可以直接使用ceph-deploy来安装网关软件包，生成密钥，配置数据目录以及创建一个网关实例

*Civetweb默认使用7480端口。要么直接打开7480端口，要么在Ceph配置文件中设置首选端口(如80)*

### 安装CEPH对象网关
```
1.在client-node上执行预安装步骤。如果打算使用Civetweb的默认端口7480，必须通过firewalld-cmd或iptables来打开它。
2.从管理节点的工作目录，在client-node上安装Ceph对象网关软件包，如:
    ceph-deploy install --rgw <client-node> [<client-node>...]
```

### 新建CEPH对象网关实例
```
从管理节点的工作目录，在client-node上创建一个Ceph对象网关实例，如：
    ceph-deploy rgw create

一旦网关开始运行，可以通过7480端口来访问，如: http://client-node:7480
```

### 配置CEPH对象网关实例
```
1.通过修改Ceph配置文件可以更改默认端口(如改为80)。增加名为[client.rgw.<client-node>]的小节，把<client-node>替换成自己Ceph客户端节点的短名称(即hostname -s的输出)。例如，节点名为client-node，在[global]节点后增加一个类似下面的小节:
    [client.rgw.client-node]
    rgw_frontends = "civetweb port=80"

2.为了使新端口的设置生效，需重启Ceph对象网关
    C7: sudo systemctl restart ceph-radosgw.service  
    C6: sudo service radosgw restart id=rgw.<short-hostname>

3.检查节点的防火墙，确保所选用的端口处于开放状态
    sudo firewall-cmd --list-all sudo firewall-cmd --zone=public --add-port 80/tcp --permanent
    sudo firewall-cmd --reload
```




