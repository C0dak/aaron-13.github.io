# CEPH

------

## Ceph简介
Ceph存储集群至少需要一个Ceph Monitor和两个OSD守护进程。而运行Ceph文件系统的客户端，必须要有元数据服务器(Metadata Server)
+ Ceph OSDs: Ceph OSD守护进程的功能是存储数据，处理数据的复制，恢复，回填，再均衡，并通过检查其他OSD守护进程的心跳来向Ceph Monitors提供一些监控信息。当Ceph存储集群设定为有2个副本时，至少需要2个OSD守护进程，集群才能达到active+clean状态(Ceph默认有3个副本，但可以调整副本数)
+ Monitors： Ceph Monitor维护着展示集群状态的各种图表，包括监视器图，OSD图，归置(PG)组,和CRUSH图。Ceph保存着发生在Monitors,OSD和PG上的每一次状态变更的历史信息(称为epoch)
+ MDSs： Ceph元数据服务器(MDS)为Ceph文件系统存储元数据(Ceph块设备和Ceph对象存储不使用MDS)。元数据服务器使得POSIX文件系统的而用户们，可以在不对Ceph存储集群造成负担的前提下，执行诸如ls，find等基本命令

Ceph把客户端数据保存为存储池内的对象。通过使用CRUSH算法，Ceph可以计算出哪个归置(PG)组应该持有指定的对象(Object),然后进一步计算出哪个OSD守护进程持有该归置组。CRSH算法使得Ceph存储集群能够动态的伸缩，再均衡和恢复

[CRUSH](https://aaron-13.github.io/2018/06/CRUSH算法)

### 硬件推荐
+ CPU
Ceph元数据服务器对CPU敏感，会动态地重分步它们的负载，所以元数据服务器应该有足够的处理能力(4 core+)
Ceph的OSD运行着RADOS服务，用CRUSH计算数据存放位置，复制数据，维护它自己的集群运行图副本，因此OSD要一定的处理能力(2core+)
监视器只简单维护着集群运行图的副本，因此对CPU不敏感；但必须考虑到机器以后是否还会运行Ceph监视器意外的CPU密集型任务。

+ RAM内存
元数据服务器和监视器必须可以尽快地提供它们的数据，应该有足够的内存，至少每进程1GB。OSD的日常运行不需要那么多内存(每个进程500MB左右)；然而在恢复期间它们占用内存较大(每TB数据需要约1TB需要1GB内存)，通常内存越多越好

+ 数据存储
来自操作系统的并行操作和到单个硬盘的多个守护进程并发读，写请求操作会极大地降低性能。文件系统局限性。文件系统局限性也要考虑： btfs尚未稳定到可以用于生产环境，但它可同时记日志并写入数据，而xfs和ext4不行

SSD用于存储对象太昂贵，但是把OSD的日志存到SSD，把对象数据存储到独立的硬盘可以明显提升性能。osd journal选项的默认值是/var/lib/ceph/osd/$cluster-$id/journal,你可以把它挂载到一个SSD或SSD分区，这样它就不再是和对象数据一样存储在同一个硬盘上的文件了。

提升CephFS文件系统性能的一种方法是从CephFS文件内容里分离出元数据。Ceph提供了默认的metadata存储池来储存CephFS元数据，所以不需要给CephFS元数据创建存储池，但是可以给它创建一个劲指向某主机SSD的CRUSH运行图。

OSD数量较多的主机上会派生出大量线程，尤其是在恢复和重均衡期间。很多Linux内核默认的最大线程数较小，如果遇到这类问题，可以把kernel.pid_max值调高
```
kernel.pid_max = 4194303
```

最低配置
ceph-osd
+ processor: 64bit AMD-64
+ RAM: ~1GB for 1TB of storage per daemon
+ Volume Storage: 1x storage dirve per daemon
+ Journal: 1x SSD partition per daemon(optional)
+ Network: 2x 1GB Ethernet NICs

ceph-mon
+ Processor: 64bit AMD-64
+ RAM: 1GB per daemon
+ Disk Space: 10GB per daemon
+ Network: 2x 1GB Ethernet NICs

ceph-mds
+ Processor: 64bit AMD-64
+ RAM: 1GB minimum per daemon
+ Disk Space: 1MB per daemon
+ Network: 2x 1GB Ethernet NICs


## 安装
```
CentOS 7
sudo subscription-manager repos --enable=rhel-7-server-extras-rpms

sudo yum install -y yum-utils && sudo yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ && sudo yum install --nogpgcheck -y epel-release && sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 && sudo rm /etc/yum.repos.d/dl.fedoraproject.org*

ceph.repo
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-{ceph-release}/{distro}/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

yum update && sudo yum install ceph-deploy

```

### CEPH节点安装
```
安装NTP
yum install ntp ntpdate ntp-doc

安装SSH服务器
yum install openssh-server

创建CEPH用户
ceph-deploy工具必须以普通用户登录Ceph节点，且此用户拥有无密码使用sudo的权限，因为它需要在安装软件及配置文件过程中，不必输入密码
较新版的ceph-deploy支持使用--username选项提供可无密码使用sudo的用户名。使用ceph-deploy --username {username}命令时，指定的用户必须能通过无密码SSH连接到Ceph节点，因为Ceph-deploy中途不会提示输入密码。

```