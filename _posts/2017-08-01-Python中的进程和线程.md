# Python中的进程(process)和线程(thread)

------

多任务的实现有三种:

+ 多进程模式

+ 多线程模式

+ 多进程多线程模式

线程是最小的执行单元，而进程由至少一个线程组成，如何调度进程和线程，由操作系统决定。


**多进程(multiprocessing)**

Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程(父进程)复制了一份(子进程)，然后，分别在父进程和子进程内返回。

子进程永远返回0，而父进程返回子进程的ID。这样做的理由: 一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。

Python中的os模块封装了常见的系统调用

```python
import os
print("process %s start" % os.getpid())

pid = os.fork()

if pid == 0:
	print("this is child thread %s,the parent thread is %s" % (os.getpid(),os.getppid()))
else:
	print("this is parent thread %s,the child thread is %s" % (os.getpid(),pid))
```
![fork-01.png](https://aaron-13.github.io/images/fork-01.png)

有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，fork子进程来进行http请求的处理。

但是这个fork在windows上是没有的。于是出现了处理fork的通用模块，以保证在处理不同操作系统间的调用。

multiprocessing模块就是跨平台版本的多进程模块。
multiprocessing模块提供了一个Process类来代表一个进程对象:

```python
#!/usr/bin/python

import os
from Multiprocessing import Process

def run_proc(name):
	print("run child process %s %s" % (name,os.getpid()))

if __name__ == '__main__':
	print ("run parent process %s" % os.getpid() )
	p = Process(run_proc,args=("test")
	print("child process start %s" )
	p.start()
	p.join()
	print("child process end")
```

![fork-02.png](https://aaron-13.github.io/images/fork-02.png)

创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用start()方法启动
join()方法可以等待子进程结束后再继续往下执行，通常用于进程间的同步。


**进程池Pool**

如果要启动大量子进程，可以用进程池的方式批量创建子进程: 

```python
#!/usr/bin/python

from multiprocessing import Pool
import os

def func(name):
	print("run task %s %s" % (name,os.getpid()))
	star = time.time()
	time.sleep(random.random() * 3)
	end = time.time()
	print ("finish task %s use %0.f second" % (name,(end - start)))

if __name__ == "__main__":
	print("parent process is %s" % os.getpid())
	p = Pool(4)
	for i in range(5):
		p.apply_async(func,args=(i,))
	print ("waiting for subprocess done")
	p.close()
	p.join()
	print ("all subprocess done")
```

![fork-03.png](https://aaron-13.github.io/images/fork-03.png)

对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须调用close(),调用close()之后就不能继续添加新的process了。

注意输出结果，task0,1,2,3是立刻执行的，而task4等待前面某个task完成后才执行，这是因为Pool默认大小设置为4，代表着最多同时执行4个进程。这是Pool有意设计的限制，并不是操作系统的限制。
Pool的默认大小是CPU的核数。


**子进程**

很多时候，子进程并不是自身，而是一个外部进程。创建了子进程后，还需要控制子进程的输入和输出。当试图通过python进行一些运维工作的时候，subprocess就很有用。

subprocess模块可以让我们非常方便的启动一个子进程，然后控制其输入和输出。

```python
#!/usr/bin/python

import subprocess

print("$ nslookup www.google.com")

p = subprocess.call(['nslookup','www.google.com'])
print('exit code: ',p)

```

如果子进程还需要输入，则可以通过communicate()方法输入:

```python
import subprocess

p = subprocess.Popen(['nslookup'],stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
output,err = p.communicate(b"set q=mx\n www.google.com\nexit\n")
print(output.decode('utf-8'))
print("exit code: ",p.returncode)
```


**进程间通信**

Process之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python的multiprocessing模块包装了底层的机制，提供了Queue，Pipes等多种方式来交换数据。

以queue为例，创建两个线程，一个向queue队列写入数据，一个从queue队列读取数据。

```python
from multiprocessing import Process,Queue
import os,time,random

def write(q):
	print ("process to write: %s " % os.getpid())
	for value in ['A','B','C']:
		print("put %s to queue..." % value)
		q.put(value)
		time.sleep(random.random())

def read(p):
	print("process to read: %s" % os.getpid())
	while True:
		value = q.get(True)
		print("get %s from queue" % value)

if __name == '__main__':
	
	# 父进程创建Queue，并传给各个子进程
	q = Queue()
	qw = Process(target=write,args=(q,))
	qr = Process(target=read,args=(q,))
	
	# 启动子进程pw，写入
	pw.start()

	# 启动子进程pr，读取
	pr.start()

	# 等待pw结束
	pw.join()

	# pr进程里的死循环，无法等待结束，只能强制终止
	pr.terminate()

```

![fork-06.png](https://aaron-13.github.io/images/fork-06.png)

在Unix/Linux下，multiprocessing模块封装了fork()调用，使我们不需要关注fork()的细节。由于windows没有fork调用，因此，multiprocessing需要模拟出fork的效果，父进程所有Python对象必须通过pickle序列化再传入到子进程去，如果multiprocessing在windows下调用失败，要先考虑是不是pickle失败


**进程小结**

在Unix/Linux下，可以使用fork()调用实现多进程。
要实现跨平台的多进程，可以使用multiprocessing模块。
进程间通信是通过Queue，Pipes实现的。



**多线程**

多任务可以由多进程完成的，也可以在一个进程内的多线程完成。

由于线程是操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python的线程是真正的Posix Thread，而不是模拟出来的线程。

Python的标准库提供了两个模块: _threads和threading，_thread是低级模块，threading是高级模块，对_thread进行了封装，绝大多数情况下，只需要使用threading这个高级模块。

启动一个线程就是把一个函数传入并创建Thread实例，然后调用start()开始执行:

```python
#!/usr/bin/python

import os, threading

def thr():
    print ("thread %s is running" % threading.current_thread().name)
    n = 0
    while n < 5:
        n += 1
        print("thread %s >>> %s" % (threading.currentThread().name,n))
    print("thread %s ended" % threading.currentThread().name)

print ("thread %s is running" % threading.currentThread().name)
t =threading.Thread(target = thr,name = "thread-01")
t.start()
t.join()
print("thread %s ended" % threading.currentThread().name)
```

threading.current_thread() = threading.currentThread()

![fork-07.png](https://aaron-13.github.io/images/fork-07.png)


由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的threading模块有个current_thread()函数，它永远返回当前的线程的实例。主线程实例的名字叫MainThread，子线程的名字创建时指定，用thread-01；命名子线程，名字仅仅是在打印时用来显示，完全没其他意义，如果不起名字，Python就自动给线程命名为Thread-1，Thread-2...


**Lock**

多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互补影响，而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时修改一个变量，把内容给改乱了。

```python
#!/usr/bin/python

import time, threading

balance = 0

def change_it(n):
    global balance
    balance = balance + n
    balance = balance - n
#   print balance

def run_thread(n):
    for i in range(1000000):
#       print ("this is %s time" % i)
        change_it(n)

t1 = threading.Thread(target=run_thread,args=(5,))
t2 = threading.Thread(target=run_thread,args=(8,))
t1.start()
t2.start()
t1.join()
t2.join()
print(balance)

```

![fork-08.png](https://aaron-13.github.io/images/fork-08.png)

定义一个共享变量balance，初始值为0，并启动两个线程，先存后取，理论上结果应为0，但是，由于线程的调度是由系统决定的，当t1，t2交替执行时，只要循环次数够多，balance的结果就不一定是0了。

究其原因，是因为修改balance需要多条语句，而执行这几条语句时，线程可能中断，从而导致多个线程把同一个对象的内容改乱了。

两个线程同时一存一取u，就可能导致余额不对，所以，就必须确保一个线程在修改balance的时候，别的线程就不能改。

如果要确保balance计算正确，就要给change_it()上一把锁，当某个线程开始执行change_it()时，该线程就获得锁，因此其他线程不能同时执行change_it(),只能等待，直到锁释放，获得该锁以后才能改，由于锁只有一个，无论多少线程，同一时刻只有一个线程持有该锁，所以，不会造成修改的冲突。创建一个锁就是通过threading.Lock()来实现：

```python
#!/usr/bin/python

import time, threading

balance = 0
lock = threading.Lock()

def change_it(n):
    global balance
    balance = balance + n
    balance = balance - n
#   print balance

def run_thread(n):
    for i in range(1000000):
		lock.acquire()
		try:
	        change_it(n)
		finally:
			lock.release()

t1 = threading.Thread(target=run_thread,args=(5,))
t2 = threading.Thread(target=run_thread,args=(8,))
t1.start()
t2.start()
t1.join()
t2.join()
print(balance)
```

当多个线程同时执行lock.acquire()时，只有一个线程能成功获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。

获得锁的线程用完后一定要释放锁，否则等待锁的线程将永远等待，成为死线程，所以，使用try...finally来确保锁一定会被释放。

锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整的执行，坏处也很多，首先是阻止了多线程并发执行，包含锁的某段代码只能以单线程模式执行，效率就大大降低了。其次，由于，可以存在多个锁，不同的 线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。



**多核CPU**

用Python写个死循环：

```python
#!/usr/bin/python

import threading.multiprocessing

def loop():
	x = 0
	wile True:
		x = x ^ 1

for i in range(multiprocessing.cpu_count()):
	t = threading.Thread(target=loop)
	t.start()

```

上面的程序，启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率100%+

但是用C,C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，为什么Python不行呢？

因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁: Global Interpreter Lock,任何Python线程执行前，必须先获得GIL锁。然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。

GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。

所以，在Python中，可以使用多线程，但不要指望能有效利用多核如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。

不过，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。

多线程编程，模型复杂，容易发生冲突，必须用锁加以隔离，同时，又要小心死锁的发生。

Python解释器由于设计时有GIL全局锁，导致了多线程无法利用多核。


**ThreadLocal**
在多线程环境下，每个线程都有自己的数据，一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。

但是局部变量也有问题，就是在函数调用的时候，传递起来麻烦：

```python

def process_studnet(name):
	std = student(name)
	# std是局部变量，但是每个函数都要使用到它，因此必须传进去
	do_task_1(std)
	do_task_2(std)

def do_task_1(std):
	do_subtask_1(std)
	do_subtask_2(std)

def do_task_2(std):
	do_subtask_2(std)
	do_subtask_2(std)

```

每个函数一层一层调用都这么传参数效率很低下。用全局变量，不行，因为每个线程处理的不同Student对象，不能共享。

如果用一个dict存放所有的Student对象，然后以thread自身作为key获得线程对应的Student对象如何?


```python
global_dist = {}

def std_thread(name):
	std = Student(name)
	# 把std放到全局变量global_dict中
	global_dict[thread.current_thread()] = std
	do_task_1()
	do_task_2()

def do_task_1():
	# 不传入std，而是根据当前线程进行查找
	std = global_dict[thread.current_thread()]
	...

def do_task_2():
	# 不传入std，而是根据当前线程进行查找
	...

```

这个方式理论上是可行的，它最大的优点是消除了std对象在每层函数中的传递问题，但是，每个函数获取std的代码有点丑。

有没有更简单的方式？

ThreadLocal应运而生，不用查找dict，ThreadLocal帮你自动做这件事

```python

import threading

# 创建全局threadLocal对象
local_school = threading.local()

def process_student():

	# 获取当前线程关联的students
	std = local_school.student
	print("%s in %s" % (std,threading.current_thread().name))

def process_thread(name):

	# 绑定threadLocal的student
	local_school.student = name
	process_student()

t1 = threading.Thread(target=process_thread,args=('aaron',),name="thread-a")
t2 = threading.Thread(target=process_thread,args=('ss',),name="thread-b")
t1.start()
t2.start()
t1.join()
t2.join()

```

全局变量local_school就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但相互不影响。可以把local_school看成是全局变量，但每个属性例如local_school.student都是 线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。

可以理解为local_school是一个dict，不但可以用local_school.student，还可以绑定其他变量，local_school.teacher等。

ThreadLocal最常用的地方就是每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便的访问这些资源。

一个ThreadLocal变量虽是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间传递的问题。



**进程 vs 线程**

要实现多任务，通常会设计Master-Worker模式，Master负责分配任务，Worker负责执行任务，因此，多任务环境下，通常是一个Master，多个Worker。

如果用多进程实现Master-Worker，主进程就是Master，其他进程是Worker。

如果用多线程实现Master-Worker，主线程就是Master，其他线程是Worker。

多进程模式最大的有点就是稳定性高，因为一个子进程崩溃，不会影响主进程和其他子进程。(当然主进程挂了所有进程就全挂了，但是Master进程只负责分配任务，挂掉的概率低)，Apache最早就是采用多进程模式。

多进程模式的缺点是创建进程的代价大，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU限制下，如果有几千个进程同时运行，操作系统调度都会有问题。

多线程模式通常比进程快一点，但区别不大，而且，多线程模式的缺点是任何一个线程挂掉，都可能导致整个进程崩溃，因为所有线程共享进程的内存。

在Windows下，多线程的效率比多进程要高，IIS服务器默认采用多线程模式。Apache和IIS现在都有多进程+多线程的混合模式。


**线程切换**

切换成多任务模型，是有代价的。操作系统在切换进程或者线程时，需要先保存当前执行的现场环境CPU寄存器状态，内存页等，然后，把新任务的执行环境准备好(恢复上次的寄存器状态，切换内存页等)，才能开始执行。如果任务过多，操作系统可能就主要忙于切换任务。
所以，多任务一旦多到一定程度，就会消耗掉系统所有的资源，结果效率急剧下降。


**计算密集型 vs IO密集型**

是否采用多任务的第二个考虑因素是任务的类型。

计算密集型任务的特点是进行大量的计算，消耗CPU资源。计算密集型任务虽然可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要高效利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。

计算密集型任务主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务，对于计算密集型任务，最好用C语言编写。

IO密集型，对CPU的消耗很少，任务的大部分时间都是在等待IO操作完成，对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度，常见的大部分任务都是IO密集型任务，比如web应用。
对于IO密集型任务，最适合的语言就是开发效率最高(代码量最少)的语言，脚本语言是首选，C语言最差。


**异步IO**

充分利用操作系统对异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型成为事件驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效的支持多任务。在多核CPU上，可以运行多个进程，充分利用多核CPU。由于系统总的进城数量有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。

对应到Python，单进程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。


**分布式进程**

在Thread和Process中，应当优先选Process，因为Process更稳定，而且，Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU上。

Python的multiprocessing模块不但可以支持多进程，其中managers子模块还支持把多进程分布到各台机器上，一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信，由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。

如:有一个通过queue通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上。

原有的queue可以继续使用，但是通过managers模块把Queue通过网络暴露出去，就可以让其他机器的进程访问Queue了。


```
import time,random，queue
from multiprocessing.managers import BaseManager

# 发送任务队列
task_queue = queue.Queue()

# 接收任务队列
receive_queue = queue.Queue()

# 从BaseManager继承的QueueManager
class QueueManager(BaseManager):
	pass

# 把两个队列都注册到网络上去，callable参数关联queue对象

QueueManager.register('get_task_queue',callable=lamba: send_queue)
QueueManager.register('get_result_queue',callable=lamba: receive_queue)

# 绑定端口，并设置验证码abc
manager = QueueManager(address=('',5000),authkey=b'abc')

# 启动manager
manager.start()

# 获得通过网络访问的Queue对象

task = manager.get_task_queue()
result = manager.get_result_queue()

# 放几个任务进去

for i in range(10):
	n = random.randint(0,10000)
	print("put task %d" % n)
	task.put(n)

# 从result队列中读取结果

for i in range(10):
	r = result.get(timeout=10)
	print("result: %s" % r)

# 关闭
manager.shutdown()
print("master exit")

```

注意：当我们在一台机器上写多进程程序时，创建的 Queue可以直接拿来用，但是在分布式多进程环境下，添加任务到Queue不可以直接对原始的task_queue进行操作，那样就绕过了QueueManager的封装，必须通过manager.get_task_queue()获得Queue接口添加。


在另一台机器上启动任务进程:

```python

import random,time,Queue
from multiprocessing.Manager import BaseManager

# 创建类似的QueueManager
class QueueManager(BaseManager):
	pass


# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字
QueueManager.register('get_task_queue')
QueueManager.register('get_result_queue')


# 连接到服务器，即运行task_master.py的机器
server = '127.0.0.1'
print("connect to server %s" % server)

# 端口和验证码注意保持与task_master.py设置的完全一致
q = QueueManager(address=(server,5000),authkey=b'abc')

# 从网络连接
q.connect()

# 获取queue对象
task = q.get_task_queue()
result = q.get_result_queue()

# 从task队列获取任务u，并把结果写入result队列
for i  in range(10):
	try:
		n = task.get(timeout=1)
		print ("task run %d * %d" % (n,n))
		r = '%d * %d = %d' % (n,n,n*n)
		time.sleep(1)
		result.put(r)
	except:
		print("task queue is empty")
# 处理结束
print("worker exit")

```

任务进程要通过网络连接到服务进程，所以要指定服务进程的IP

![fork-10.png](https://aaron-13.github.io/images/fork-10.png)

![fork-11.png](https://aaron-13.github.io/images/fork-11.png)


这个简单的Master/Worker模型有什么用？其实就是一个简单但真正的分布式计算，把代码稍加改造，启动多个Worker，就可以把任务分布到几台甚至几十台机器上，比如把计算n*n的代码换成发送邮件，就实现了邮件队列的异步发送。

Queue对象存储在哪？注意到task_worker.py中根本没有创建Queue的代码，所以，Queue对象存储在task_master.py进程中。

而Queue之所以能通过网络访问，就是通过QueueManager实现的。由于QueueManager管理的不止一个Queue，所以，要给每个Queue的网络调用接口起个名字，比如：get_task_queue。

authkey有什么用?这是为了保证两台机器正常通信，不被其他机器恶意干扰，如果task_worker.py的authkey和task_master.py中的authkey不一致，则连接不上。

Python的分布式进程接口简单，封装良好，适合需要把繁重任务分布到多台机器的环境下。

注意Queue的作用是用来传递任务和接收结果，每个任务的描述数据量要尽量小，比如发送一个处理日志文件的任务，就不要发送几百兆的日志本身，而是发送日志文件存放的完整路径，由Worker进程再去共享的 磁盘上读取文件。


