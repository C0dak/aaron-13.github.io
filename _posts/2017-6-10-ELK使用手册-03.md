# 过滤器插件(Filter)

------

## 时间处理(Date)

filter/date插件可以用来转换日志记录中的时间字符串，变成LogStash::Timestamp对象，然后转存到@timestamp字段里

**注意: 因为在outputs/elasticsearch中常用的%{+YYYY.MM.dd}这种写法必须读取@timestamp数据，所以不要直接删除这个字段保留自己的字段，而是应该用filters/date转换后删除自己的字段。**

**建议打开nginx的access_log配置项的buffer参数，对极限响应性能有很大提升**

### 配置示例

filters/date插件支持五中时间格式

**ISO8601**

类似`2017-06-10T10:11:12.135Z`这样的格式。常用场景里，Nginx的log_format配置里就可以使用`$time_iso8601`变量来记录请求时间成这种格式。

**UNIX**
UNIX时间戳格式，记录从1970年开始至今的总秒数。Squid的默认日志格式中就是用了这种格式

**UNIX_MS**
这个时间戳则是从1970年至今的总毫秒数。

**TAI64N**
该格式比较少见`@4000000052f88ea32489532c`。qmail会用这种格式

**Joda-Time库**
Logstash内部使用了Java的Joda时间库来作时间处理。

**时间格式**

|**Symbol** | **Meaning** | **Presentation** | **Example**  |
| :----:    |  :----:     |   :-----:        |  :-----:     |
| G    		|	era		  | 	text		 |  AD		    |
| C			| century of ear(>=0) | number   | 20		    |
| Y			| year of era(>=0) | year 		 | 1996		    |
| x         | weekyear	  |   year 			 | 1996 		|
| w			| week of weekyear | number		 | 27 			|
| e		    | day of week |	number			 | 2			|
| E			| day of week | text			 | Tuesday; Tue |
| y			| year		  | year			 | 1996			|
| D 		| day of year | number 			 | 189			|
| M 		| month of year    | month		 | July; Jul; o7|
| d			| day of month| number			 | 10			|
| a 		| halfday of day   | text		 | PM			|
| K 		| hour of halfday(0-11) | number | 0			|
| h 		| clockhour of halfday(1-12)| number | 12		|
| H			| hour of day(0-23)| number 	 | 0			|
| k			| clockhour of day(1-24) | number| 24			|
| m			| minute of hour   | number		 | 30			|
| s 		| second of minute | number 	 | 55			|
| S 		| fraction of second | number    | 978 			|
| z 		| time zone 	   | text		 | Pacific Standard Time; PST |
| Z 		| time zone offset/id | zone	 | -0800; -08:00; America/Los_ANgeles |
| ' 		| escape for text  | delimiter   | 				|
| "			| single quote	   | literal	 | '			|



```
filter {
	grok { 
	match => ["message","%{HTTPDATE:logdate}"]
	}
	date {
		match => ["logdate","dd/MMM/yyyy:HH:mm:ss Z"]
	}
}
```

**注意: 时区偏移量只需要一个字母 Z 即可**

在Elasticsearch内部，对时间类型字段，是统一采用UTC时间，存成long长整型数据。对于页面查看，ELK的解决方案是在kibana上，读取浏览器当前时区，然后在页面上转换时间内容的显示。



## Grok正则捕获
Grok是Logstash最重要的插件。

**正则表达式语法**
`\s+(?<request_time>\d+(?:\.\d+)?)\s+`

```
input {stdin{}}
filter {
    grok {
        match => {
            "message" => "\s+(?<request_time>\d+(?:\.\d+)?)\s+"
        }
    }
}
output {stdout{codec => rubydebug}}
```

![grok-01](https://aaron-13.github.io/images/grok-01.png)

可以用LogStash::Filters::Mutate来转换字段值类型，不过在grok里，有自己的方式来实现这个功能


### Grok表达式语法

Grok支持把预定义的grok表达式写入到文件中，官方提供的grok表达式[https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns](https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns)

**注意: 在新版本的logstash里，pattern目录已经为空，最后一个commit提示core patterns将会由logstash-patterns-core gem来提供，该目录可供用户存放自定义patterns**

```
USERNAME [a-zA-Z0-9._-]+
USER %{USERNAME}
```

第一行，用普通的正则表达式来定义一个grok表达式;第二行，通过打印赋值格式(printf format),用前面定义好的grok表达式来定义另一个grok表达式。

grok表达式的打印赋值格式的完整语法:
```
%{PATTERN_NAME:capture_name:data_type}
```

data_type目前只支持两个值: `int` 和 `float`

```Java
filter {
	match => {
		"message" => "%{WORD} %{NUMBER:request_time:float} %{WORD}"
	}
}

![grok-02.png](https://aaron-13.github.io/images/grok-02.png)


**最佳实践**
建议将所有的grok表达式统一写到一个地方，然后用filter/grok的`patterns_dir`选项来指明

如果把`message`里所有的信息都grok到不同的字段了，数据实质上就相当于是重复存储了两份。所以可以用`remove_field`参数来删除掉message字段，或者用`overwrite`参数来重写默认的message字段，只保留重要的部分。

```
filter {
	grok {
		patterns_dir => ["/path/to/your/own/patterns"]
		match => {
			"message" => "%{SYSLOGBASE} %{DATA:message}"
		}
		overwrite => ["message"]
	}
}
```

更多grok正则性能的最佳实践(timeout_millis等)，[https://www.elastic.co/blog/do-you-grok-grok](https://www.elastic.co/blog/do-you-grok-grok)


**多行匹配**

在和codec/multiline搭配使用的时候，要注意一个问题，grok正则和普通的正则一样。默认是不支持匹配回车换行的。具体写法，在表达式开始位置(?m)标记:

```
match => {
	"message" => "(?m)\s+(?<request_time>\d+(?:\.\d+)?)\s+"
}
```

**多项选择**

文档中，都说明logstash/filter/grok插件的`match`参数应该接受的是一个Hash值。但是因为早期的logstash语法中Hash值也是用[]这种方式书写的，所以其实现在传递Array值给match参数也完全没问题。

```
match => [
	"message","(?<request_time>\d+(?:\.\d+)?)",
	"message","%{SYSLOGBASE} %{DATA:message}",
	"message","(?m)%{WORD}"
]
```

logstash会按照这个定义次序依次尝试匹配，到匹配成功为止。和使用`|`分割效果一样。

**可以使用[Grok Debugger](http://grokdebug.herokuapp.com/)来调试自己的grok表达式**


**dissect**
grok作为Logstash最广为人知的插件，在性能和资源耗损方面同样也广为诟病。为了应对这个情况，同时也考虑到多大数时候，日志格式并没有那么复杂，Logstash开发团队在5.0版新添加了另一个解析字段的插件: dissect。

当日志格式有比较简明的分割标志位，而且重复性大的时候，可以使用dissect插件更快的完成解析工作。

```
filter {
	dissect {
		mapping => {
			"message" => "%{ts} %{+ts} %{+ts} %{src} %{} %{prog}[%{pid}]: %{msg}"
		}
		convert_datatype => {
			pid => "int"
		}
	}
}
```


**语法解释**
上面使用了和Grok很类似的%{}语法来表示字段，这显然是基于习惯延续的考虑。dissect除了字段外面的字符串定位功能以外，还通过几个特殊符号来处理字段提取的规则:
+ `%{+key}` 这个+表示,前面已经捕获到一个key字段了，而这次捕获的内容，自动添补到之前key字段内容的后面。

+ `%{+key/2}` 这个 /2 表示，在有多次捕获内容都填到key字段里的时候，拼接字符串的顺序,/2表示排第2位

+ `%{?string}` 这个?表示，这块只是一个占位，并不实际生成捕获字段存到Event里面

+ `%(?string) %{&string}` 当同样捕获名称都是string，但是一个?一个&的时候，表示这是一个键值对

比如对 `http://rizhiyi.com/index.do?id=123` 写这么一段配置:

```
http://%{domain}//%{?url}?%{?arg1}=%{&arg1}
```

则最终生成的Event内容是这样的:

```
{
	domain => "rizhiyi.com"
	id => "123"
}



### GeoIP地址查询归类

GeoIP是最常见的免费IP地址归类查询库，也有收费版可以采购。GeoIP库可以根据IP地址提供对应的地域信息，包括国别，省市，经纬度等，对于可视化地图和区域统计非常有用。

**配置示例**

```
filter {
	geoip {
		source => "message"
	}
}
```

运行结果

![geoip-01](https://aaron-13.github.io/images/geoip-01.png)


配置说明:

GeoIP库数据较多，如果不需要这么多数据，可以通过fields选项指定自己所需要的

```
filter {
	geoip {
		fields => ["city_name","continue_code","country_code2","country_code3","country_name","dma_code","ip","latitude","longitude","postal_code","region_name","timezone"]
	}
}
```

需要注意的是: `geoip.location`是logstash通过`latitude`和`longitude`额外生成的数据。所以，想要经纬度又不想重复的数据，应该这么做:


```
filter {
	geoip {
		fields => ["city_name","country_code2","country_code3","country_name","latitude","longitue","region_name"]
		remove_field => ["[geoip][latitude]","[geoip][longitude]"]
	}
}
```


geoip插件的"source"字段可以是任意处理后的字段，比附"client_ip"，但是字段内容却需要小心，geoip库内只存有公共网络上的IP信息，查询不到结果的，会直接返回null，而logstash的geoip插件对null结果的处理是: 不生成对应的geoip字段



### JSON编解码











































