# 过滤器插件(Filter)

------

## 时间处理(Date)

filter/date插件可以用来转换日志记录中的时间字符串，变成LogStash::Timestamp对象，然后转存到@timestamp字段里

**注意: 因为在outputs/elasticsearch中常用的%{+YYYY.MM.dd}这种写法必须读取@timestamp数据，所以不要直接删除这个字段保留自己的字段，而是应该用filters/date转换后删除自己的字段。**

**建议打开nginx的access_log配置项的buffer参数，对极限响应性能有很大提升**

### 配置示例

filters/date插件支持五中时间格式

**ISO8601**

类似`2017-06-10T10:11:12.135Z`这样的格式。常用场景里，Nginx的log_format配置里就可以使用`$time_iso8601`变量来记录请求时间成这种格式。

**UNIX**
UNIX时间戳格式，记录从1970年开始至今的总秒数。Squid的默认日志格式中就是用了这种格式

**UNIX_MS**
这个时间戳则是从1970年至今的总毫秒数。

**TAI64N**
该格式比较少见`@4000000052f88ea32489532c`。qmail会用这种格式

**Joda-Time库**
Logstash内部使用了Java的Joda时间库来作时间处理。

**时间格式**

|**Symbol** | **Meaning** | **Presentation** | **Example**  |
| :----:    |  :----:     |   :-----:        |  :-----:     |
| G    		|	era		  | 	text		 |  AD		    |
| C			| century of ear(>=0) | number   | 20		    |
| Y			| year of era(>=0) | year 		 | 1996		    |
| x         | weekyear	  |   year 			 | 1996 		|
| w			| week of weekyear | number		 | 27 			|
| e		    | day of week |	number			 | 2			|
| E			| day of week | text			 | Tuesday; Tue |
| y			| year		  | year			 | 1996			|
| D 		| day of year | number 			 | 189			|
| M 		| month of year    | month		 | July; Jul; o7|
| d			| day of month| number			 | 10			|
| a 		| halfday of day   | text		 | PM			|
| K 		| hour of halfday(0-11) | number | 0			|
| h 		| clockhour of halfday(1-12)| number | 12		|
| H			| hour of day(0-23)| number 	 | 0			|
| k			| clockhour of day(1-24) | number| 24			|
| m			| minute of hour   | number		 | 30			|
| s 		| second of minute | number 	 | 55			|
| S 		| fraction of second | number    | 978 			|
| z 		| time zone 	   | text		 | Pacific Standard Time; PST |
| Z 		| time zone offset/id | zone	 | -0800; -08:00; America/Los_ANgeles |
| ' 		| escape for text  | delimiter   | 				|
| "			| single quote	   | literal	 | '			|



```
filter {
	grok { 
	match => ["message","%{HTTPDATE:logdate}"]
	}
	date {
		match => ["logdate","dd/MMM/yyyy:HH:mm:ss Z"]
	}
}
```

**注意: 时区偏移量只需要一个字母 Z 即可**

在Elasticsearch内部，对时间类型字段，是统一采用UTC时间，存成long长整型数据。对于页面查看，ELK的解决方案是在kibana上，读取浏览器当前时区，然后在页面上转换时间内容的显示。



## Grok正则捕获
Grok是Logstash最重要的插件。

**正则表达式语法**
`\s+(?<request_time>\d+(?:\.\d+)?)\s+`

```
input {stdin{}}
filter {
    grok {
        match => {
            "message" => "\s+(?<request_time>\d+(?:\.\d+)?)\s+"
        }
    }
}
output {stdout{codec => rubydebug}}
```

![grok-01](https://aaron-13.github.io/images/grok-01.png)

可以用LogStash::Filters::Mutate来转换字段值类型，不过在grok里，有自己的方式来实现这个功能


### Grok表达式语法

Grok支持把预定义的grok表达式写入到文件中，官方提供的grok表达式[https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns](https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns)

**注意: 在新版本的logstash里，pattern目录已经为空，最后一个commit提示core patterns将会由logstash-patterns-core gem来提供，该目录可供用户存放自定义patterns**

```
USERNAME [a-zA-Z0-9._-]+
USER %{USERNAME}
```

第一行，用普通的正则表达式来定义一个grok表达式;第二行，通过打印赋值格式(printf format),用前面定义好的grok表达式来定义另一个grok表达式。

grok表达式的打印赋值格式的完整语法:
```
%{PATTERN_NAME:capture_name:data_type}
```

data_type目前只支持两个值: `int` 和 `float`

```Java
filter {
	match => {
		"message" => "%{WORD} %{NUMBER:request_time:float} %{WORD}"
	}
}

![grok-02.png](https://aaron-13.github.io/images/grok-02.png)


**最佳实践**
建议将所有的grok表达式统一写到一个地方，然后用filter/grok的`patterns_dir`选项来指明

如果把`message`里所有的信息都grok到不同的字段了，数据实质上就相当于是重复存储了两份。所以可以用`remove_field`参数来删除掉message字段，或者用`overwrite`参数来重写默认的message字段，只保留重要的部分。

```
filter {
	grok {
		patterns_dir => ["/path/to/your/own/patterns"]
		match => {
			"message" => "%{SYSLOGBASE} %{DATA:message}"
		}
		overwrite => ["message"]
	}
}
```

更多grok正则性能的最佳实践(timeout_millis等)，[https://www.elastic.co/blog/do-you-grok-grok](https://www.elastic.co/blog/do-you-grok-grok)


**多行匹配**

在和codec/multiline搭配使用的时候，要注意一个问题，grok正则和普通的正则一样。默认是不支持匹配回车换行的。具体写法，在表达式开始位置(?m)标记:

```
match => {
	"message" => "(?m)\s+(?<request_time>\d+(?:\.\d+)?)\s+"
}
```

**多项选择**

文档中，都说明logstash/filter/grok插件的`match`参数应该接受的是一个Hash值。但是因为早期的logstash语法中Hash值也是用[]这种方式书写的，所以其实现在传递Array值给match参数也完全没问题。

```
match => [
	"message","(?<request_time>\d+(?:\.\d+)?)",
	"message","%{SYSLOGBASE} %{DATA:message}",
	"message","(?m)%{WORD}"
]
```

logstash会按照这个定义次序依次尝试匹配，到匹配成功为止。和使用`|`分割效果一样。

**可以使用[Grok Debugger](http://grokdebug.herokuapp.com/)来调试自己的grok表达式**


**dissect**
grok作为Logstash最广为人知的插件，在性能和资源耗损方面同样也广为诟病。为了应对这个情况，同时也考虑到多大数时候，日志格式并没有那么复杂，Logstash开发团队在5.0版新添加了另一个解析字段的插件: dissect。

当日志格式有比较简明的分割标志位，而且重复性大的时候，可以使用dissect插件更快的完成解析工作。

```
filter {
	dissect {
		mapping => {
			"message" => "%{ts} %{+ts} %{+ts} %{src} %{} %{prog}[%{pid}]: %{msg}"
		}
		convert_datatype => {
			pid => "int"
		}
	}
}

```


**语法解释**
上面使用了和Grok很类似的%{}语法来表示字段，这显然是基于习惯延续的考虑。dissect除了字段外面的字符串定位功能以外，还通过几个特殊符号来处理字段提取的规则:
+ `%{+key}` 这个+表示,前面已经捕获到一个key字段了，而这次捕获的内容，自动添补到之前key字段内容的后面。

+ `%{+key/2}` 这个 /2 表示，在有多次捕获内容都填到key字段里的时候，拼接字符串的顺序,/2表示排第2位

+ `%{?string}` 这个?表示，这块只是一个占位，并不实际生成捕获字段存到Event里面

+ `%(?string) %{&string}` 当同样捕获名称都是string，但是一个?一个&的时候，表示这是一个键值对

比如对 `http://rizhiyi.com/index.do?id=123` 写这么一段配置:

```
http://%{domain}//%{?url}?%{?arg1}=%{&arg1}
```

则最终生成的Event内容是这样的:

```
{
	domain => "rizhiyi.com"
	id => "123"
}

```

### GeoIP地址查询归类

GeoIP是最常见的免费IP地址归类查询库，也有收费版可以采购。GeoIP库可以根据IP地址提供对应的地域信息，包括国别，省市，经纬度等，对于可视化地图和区域统计非常有用。

**配置示例**

```
filter {
	geoip {
		source => "message"
	}
}
```

运行结果

![geoip-01](https://aaron-13.github.io/images/geoip-01.png)


配置说明:

GeoIP库数据较多，如果不需要这么多数据，可以通过fields选项指定自己所需要的

```
filter {
	geoip {
		fields => ["city_name","continue_code","country_code2","country_code3","country_name","dma_code","ip","latitude","longitude","postal_code","region_name","timezone"]
	}
}
```

需要注意的是: `geoip.location`是logstash通过`latitude`和`longitude`额外生成的数据。所以，想要经纬度又不想重复的数据，应该这么做:


```
filter {
	geoip {
		fields => ["city_name","country_code2","country_code3","country_name","latitude","longitue","region_name"]
		remove_field => ["[geoip][latitude]","[geoip][longitude]"]
	}
}
```


geoip插件的"source"字段可以是任意处理后的字段，比附"client_ip"，但是字段内容却需要小心，geoip库内只存有公共网络上的IP信息，查询不到结果的，会直接返回null，而logstash的geoip插件对null结果的处理是: 不生成对应的geoip字段



### JSON编解码

示例
···
filter {
	json {
		source => "message"
		target => "jsoncontent"
	}
}
```

![json-03.png](https://aaron-13.github.io/images/json-03.png)


如果不打算使用多层结构的话，删掉target配置即可



### key-Value切分

很多情况下，日志内容本身都是一个类似于key-value的格式，但是格式具体的样式却是多种多样的。Logstash提供`filters/kv`插件，帮助处理不同样式的key-value日志，变成实际的LogStash::Event数据

**配置示例**

```
filter {
	ruby {
		init => "@kname" = ['method','uri','verb']
		code => "
			new_event = LogStash::Event.new(Hash[@kname.zip(event.get('request').split('|'))])
			new_event.remove('@timestamp')
			event.append(new_event)
		"
	}
	if [uri] {
		ruby {
			init => "@kname = ['url_path','url_args']"
			code => "
				new_event = LogStash::Event.new(Hash[@kname.zip(event.get('uri').split('?'))])
				new_event.remove('@timestamp')
				event.append(new_event)
			"
		}
		kv {
			prefix => "uri"
			source => "url_args"
			field_split => "&"
			remove_field => ["url_args","uri","request"]
		}
	}		
}
```


**解释**

Nginx访问日志中的`$request`,通过这段配置，可以详细切分成`method`,'url_path','verb','url_a','url_b'...

进一步的，如果url_args中有过多字段，可能导致Elasticsearch集群因为频繁update mapping或者消耗太多内存再cluster state上而宕机。所以，更优的选择，是指保留明确有用的url_args内容，其他部分舍去。

```
kv {
	prefix => "url_"
	source => "url_args"
	field_split => "&"
	include_keys => ["uid","cip"]
	remove_field => ["url_args","uri","request"]
}
```


### 数值统计(Metrics)

filters/metrics插件是使用Ruby的Metriks模块来实现在内存里实时的计数和采样分析。该模块支持两个类型的数值分析: `meter`和`timer`


**Meter示例(速率阈值检测)**

web访问日志的异常状态频率是一个关心的数据。通常的做法，是通过logstash或者其他日志分析脚本，把计数发送到rrdtool或者graphite里面。然后在通过check-graphite脚本之类的东西来检查异常并报警。


事实上，这个事情可以直接在logstash内部完成

```
filter {
	metric {
		meter => "error_%{status}"
		add_tag => "metric"
		ignore_older_than => 10
	}
	if "metric" in [tags] {
		ruby {
			code => "event.cancel if (event.get('[error_504][rate_1m]')*60 > 100)"
		}
	}
}

output {
	if "metric" in [tags] {
		exec {
			command => "echo \"Out of threshold: %{[error][rate_1m]}\""
		}
	}
}

```

metriks模块生成的rate_1m/5m/15m意思是: 最近1,5,15分钟的每秒速率


#### Timer示例(box and whisher异常检测)

官版的filters/metrics只适用于metric事件的检查。由插件生成的新事件内部不存有来自input区段的实际数据信息。要完成我们的百分比分布箱体检测，西药首先对代码稍微做几行变动。即在metric的timer事件里加一个属性，存储最近一个实际事件的数值
有了这个last值，就可以用如下配置探测异常数据了

```
filter {
	metrics {
		timer => {"rt" => "%{requet_time}"}
		percentiles => [25,75]
		add_tag => "percentile"
	}
	if "percentile" in [tags] {
		ruby {
			code => "1=event.get('[rt][p75]')-event.get('[rt][p25]');event.set('[rt][low]',event.get('[rt][p25]')-1);event.set('[rt][high]',event.get('[rt][p75]')+1)"
		}
	}
}

output {
	if "pencentile" in [tags] and ([rt][last] > [rt][high] or [rt][last] < [rt][low]) {
		exec {
			command => "echo \"Anomaly:%{[rt][last]}\""
		}
	}
}

```

**有关box and shisker plot内容和重要性，参见 数据之魅 **


### 数据修改

filters/mutate插件是logstash另一个插件，提供了丰富的基础类型数据处理能力。包括类型转换，字符串处理和字段处理等

**类型转换**

```
filter {
	mutate {
		comvert => ["request_time","float"]
	}
}

```

**字符串处理**

+ gsub
仅对字符串类型字段有效

`gsub => ["urlparams","[\\?#]","_"]`


+ split

```
filter {
	mutate {
		split => ["message","|"]
	}
}
```

+ join

仅对数组类型字段有效

```
filter {
	mutate {
		split => ["message","|"]
	}
	mutate {
		join => ["message",","]
	}
}

```

+ merge

合并两个数组或者哈希字段

```
filter {
	mutate {
		split => ["message","|"]
	}
	mutate {
		merge => ["message","message"]
	}
}

```

如果src字段是字符串，会自动转换成一个单元素的数组再合并。

![mutate-01.png](https://aaron-13.github.io/images/mutate-01.png)


+ strip

+ lowercase

+ uppercase


**字段处理**

+ rename

重命名某个字段，如果目的字段已经存在你，会被覆盖

```
filter {
	mutate {
		rename => ["syslog_host","host"]
	}
}

```

+ update

更新某个字段的内容，如果字段不存在，不会新建


+ replace

作用和update类似，当字段不存在的时候，有add_field参数的效果，自动添加新的字段


**执行次序**

filter/mutate内部是有执行次序的。

```Java

rename(event) if @rename
update(event) if @update
replace(event) if @replace
convert(event) if @convert
gsub(event) if @gsub
uppercase(event) if @uppercase
lowercase(event) if @lowercase
strip(event) if @strip
remove(event) if @remove
split(evetn) if @split
join(event) if @join
merge(event) if @merge
filter_matched(event)

```

`filter_matched` 这个filters/base.rb里继承的方法也是有次序的

```Java
@add_field.each do | field, value|
end
@remove_field.each do |field|
end
@add_tag.each do |tag|
end
@remove_tag.each do |tag|
end

```

### Ruby处理

**配置示例**

```
filter {
	ruby {
		init => "@kname = ['client','servername','url','status','time','size','upstream','upstreamstatus','upstreamtime','referer','xff','useragent']"
		code => "
			new_event = LogStash::Event.new(Hash[@kname.zip(event.get('message').split('|'))])
			new_event.remove('@timestamp')
			event.append(new_event)"
	}
}

```

通常使用filter/grok插件来捕获字段，但是正则消耗大量CPU资源，很容易成为Logstash进程的瓶颈
，很多流经Logstash的数据都有自己定义的特殊分隔符，可以直接切分成多个字段。
filters/mutate插件里的"split"选项只能切分数组，后续不方便使用和识别。而在filters/ruby里，可以通过"init"参数预定义好由每个新字段的名字组成的数组，然后在"code"参数指定的Ruby语句里通过两个数组的zip操作生成一个哈希并添加进数组里。可提升50%以上的CPU使用率。

+ 从logstash-2.3开始，LogStash::Event.append不再直接接受Hash对象，而是必须是LogStash::Event对象。要先初始化一个新event，再把无用的@timestamp移除，在append进去。否则会把@timestamp变成两个时间的数组了。

+ 从LogStash-5.0开始，LogStash::Event 改为Java实现，直接使用event["parent"]["child"]形式获取的不是原事件的引用而是复制品。需要改用event.get('[parent][child]')和event.set('[parent][child]','value')的方法


```
filter {
	date {
		match => ["datetime","UNIX"]
	}
	ruby {
		code => "event.cancel if 5 * 24 * 3600 < (event['@timestamp']-::Time.now).abs"
	}
}

```

上面配置，就是用于过滤掉时间范围与当前时间差距太大的非法数据的。



### split差分事件

```
filter {
	split {
		field => "message"
		terminator => "#"
	}
}
```

![split.png](https://aaron-13.github.io/images/split.png)

split插件中使用的是yield功能，其结果是split出来的新事件，会直接结束其在filter阶段的历程，split后面的其他filter插件都不起作用，进入到output阶段。所以，要保证**split配置写在全部filter配置的最后**



### elapsed

该插件用来计算两个时间之间经历的时间。在一对start/end事件中用时间戳来计算两者之间经历的时间

```
filter {
	grok {
		match => ["message","%{TIMESTAMP_ISO8601} START id: (?<task_id>.*)"]
	}
	grok {
		match => ["message","%{TIMESTAMP_ISO8601} END id: (?<task_id>).*"]
	}
	elapsed {
		start_tag => "taskStarted"
		end_tag => "taskTerminated"
		unique_id_field => "task_id"		
	}
}


