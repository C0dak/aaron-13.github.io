# Ceph存储集群

------

## 配置
Ceph作为集群时可以包含数千个对象存储设备(OSD)。最简系统至少需要2个OSD做数据复制。要配置OSD集群，要把配置写入到配置文件。Ceph对很多选项提供了默认值，可以在配置文件里覆盖；也可用命令行工具修改运行时配置

Ceph启动时要激活三类守护进程:
+ ceph-mon(必备)
+ ceph-osd(必备)
+ ceph-mds(cephfs必备)

各进程，守护进程或工具都会读取配置文件。一个进程可能需要不止一个守护进程例程的信息(即多个上下文)；一个守护进程或工具只有关于单个守护进程例程的信息(单上下文)

Ceph注重数据安全，Ceph客户端收到数据已写入存储器的通知时，数据确实已写入硬盘。使用较老的内核(<2.6.33),如果日志在原始硬盘上，就要禁用写缓存，较新内核没问题
`sudo hdparm -W 0 /dev/hda 0`

文件系统推荐xfs和ext4

## 配置CEPH
启动Ceph服务时，初始化进程会把一系列守护进程放到后台运行。Ceph存储集群运行两种守护进程:
+ Ceph监视器 (ceph-mon)
+ Ceph OSD守护进程 (ceph-osd)
要支持Ceph文件系统功能，他还需要至少运行一个Ceph元数据服务器(Ceph-mds);支持Ceph对象存储功能的集群还需要运行网关守护进程(radosgw)。为了方便起见，各类守护进程都有一系列默认值(很多由ceph/src/common/config_opts.h配置)，可用ceph配置文件覆盖这些默认值

### 配置文件
启动Ceph存储集群时，各守护进程都从同一个配置文件(ceph.conf)里查找它的配置。手动部署，需创建此文件；用部署工具(ceph-deploy,chef等)创建配置文件时，可以参考下面信息。配置文件定义了:
+ 集群身份
+ 认证配置
+ 集群成员
+ 主机名
+ 主机IP地址
+ 密钥环路径
+ 日志路径
+ 数据路径
+ 其他运行时选项

默认Ceph配置文件位置相继排列如下:
+ $CEPH_CONF($CEPH_CONF环境变量所指示的路径)
+ -c path/path (-c命令行参数)
+ /etc/ceph/ceph.conf
+ ~/.ceph/config
+ ./ceph.conf

Ceph配置文件是ini风格的语法，以分号(;)和井号(#)开始的行时注释

### 配置段落
Ceph配置文件可用于配置存储集群内的所有守护进程，或者某一类型的所有守护进程。
```
[global]
    描述： [global]下的配置影响Ceph集群里的所有守护进程
    实例： auth supported = cephx

[osd]
    描述： [osd]下的配置影响存储集群里的所有ceph-osd进程，并且会覆盖global下的同一选项
    实例： osd journal size = 1000

[mon]
    描述： mon下的配置影响集群里所有ceph-mon进程，并覆盖global下同一选项
    实例： mon addr = 10.0.0.1:6789

[mds]
    描述： mds下的配置影响集群里所有ceph-mds进程，并覆盖global下同一选项
    实例： host = myserver01

[client]
    描述： client下的配置影响所有客户端(如挂载的Ceph文件系统，挂载的块设备)
    实例: log file = /var/log/ceph/radosgw.log
```

全局设置影响集群内所有守护进程的例程，所以[global]可用于设置适用所有守护进程的选项，但可以用这些覆盖[global]设置：
1. 在[osd],[mon],[mds]下更改某一类进程的配置
2. 更改特定进程的设置，如[osd.1]
```
[global]
#Enable authentication between hosts within the cluster.
#v 0.54 and earlier
auth supported = cephx

#v 0.55 and after
auth cluster required = cephx
auth service required = cephx
auth client required = cephx

[osd]
osd journal size = 1000

[osd.1]
# settings affect osd.1 only.

[mon.a]
# settings affect mon.a only.

[mds.b]
# settings affect mds.b only.

[client.radosgw.instance-name]
# settings affect client.radosgw.instance-name only.
```

### 元变量
元变量大大简化了集群配置。Ceph会把配置的元变量展开为具体值；元变量功能强大，可以用在配置文件的[global],[osd],[mon],[mds]段里，类似于BASH的shell扩展
Ceph支持下列变量:
```
$cluster
    描述: 展开为存储集群名字，在同一套硬件上运行多个集群时有用。
    实例: /etc/ceph/$cluster.keyring
    默认值: ceph

$type
    描述: 可展开为 mds 、 osd 、 mon 中的一个，有赖于当前守护进程的类型。
    实例: /var/lib/ceph/$type

$id
    描述: 展开为守护进程标识符； osd.0 应为 0 ， mds.a 是 a 。
    实例: /var/lib/ceph/$type/$cluster-$id

$host
    描述: 展开为当前守护进程的主机名。

$name
    描述: 展开为 $type.$id 。
    实例: /var/run/ceph/$cluster-$name.asok
```

### ceph.conf实例
```
[global]
fsid = {cluster-id}
mon initial members = {hostname}[, {hostname}]
mon host = {ip-address}[, {ip-address}]

#All clusters have a front-side public network.
#If you have two NICs, you can configure a back side cluster 
#network for OSD object replication, heart beats, backfilling,
#recovery, etc.
public network = {network}[, {network}]
#cluster network = {network}[, {network}] 

#Clusters require authentication by default.
auth cluster required = cephx
auth service required = cephx
auth client required = cephx

#Choose reasonable numbers for your journals, number of replicas
#and placement groups.
osd journal size = {n}
osd pool default size = {n}  # Write an object n times.
osd pool default min size = {n} # Allow writing n copy in a degraded state.
osd pool default pg num = {n}
osd pool default pgp num = {n}

#Choose a reasonable crush leaf type.
#0 for a 1-node cluster.
#1 for a multi node cluster in a single rack
#2 for a multi node, multi chassis cluster with multiple hosts in a chassis
#3 for a multi node cluster with hosts across racks, etc.
osd crush chooseleaf type = {n}
```

### 运行时更改
Ceph可以在运行时更改ceph-osd，ceph-mon，ceph-mds守护进程的配置，此功能在增加/降低日志的输出，启用/禁用调试设置，设置时运行时优化的时候很有用：
`ceph tell {daemon-type}.{id or *} injectargs -- {name} {value} [--{name} {value}]`
如: 
`ceph tell osd.0 ingectargs --debug-osd 20 --debug-ms 1`

### 查看运行时配置
如果Ceph存储集群在运行，查看一个运行进程的配置:
`ceph daemon {daemon-type}.{id} config show | less`
如：
`ceph daemon osd.0 config show |less`

### 运行多个集群
用Ceph可以使用在同一套硬件上运行多个集群，运行多个集群与同一集群内用CRUSH规则控制多个存储池相比提供了更高水平的隔离。
运行多个集群时，要为集群命名并用这个名称保存配置文件，如，名为openstack的集群其配置文件将是/etc/ceph/openstack.conf。
**集群名字里只能包含字母a-z和数字0-9**

独立的集群意味着独立数据盘和日志，它们不能在集群中共享。根据前面的元变量，$cluster元变量对应集群名字(之前的openstack)。多处设置都用到$cluster元变量，包括：
+ keyring
+ admin socket
+ log file
+ pid file
+ mon data
+ mon cluster log file
+ osd data
+ osd journal
+ mds data
+ rgw data

创建默认目录和文件时，路径里该用集群名的地方要注意，如:
sudo mkdir /var/lib/ceph/osd/openstack-0
sudo mkdir /var/lib/ceph/mon/openstack-1

要调动一个名字不是ceph集群，要给ceph命令加-c {filename}.conf选项
ceph -c {cluster-name}.conf health
ceph -c openstack.conf health

### 网络配置参考
[建议使用两个网络运营Ceph存储集群](http://docs.ceph.org.cn/rados/configuration/network-config-ref/)

### CEPHX配置参考
认证配置
cephx协议已默认开启。加密认证要耗费一定计算资源，但通常很低。

#### 启用和禁用CEPH
启动cephx后，Ceph将在默认搜索路径(包括/etc/ceph/ceph.$name.keyring)查找密钥环。可在配置文件中[global]添加keyring选项来修改，但不推荐
在禁用了cephx的集群上执行以下步骤来启用，如果(部署工具)已经生成了密钥，可跳过相关步骤：
```
1.创建client.admin密钥，并为客户端保存此密钥的副本
    ceph auth get-or-create client.admin mon 'allow *' mds 'allow *' osd 'allow *' -o /etc/ceph/ceph.client.admin.keyring
注：此命令会覆盖任何存在的/etc/ceph/ceph.client.admin.keyring文件

2.创建监视器集群所需的密钥环，并给它们生成密钥
    ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'

3.把监视器密钥环复制到ceph.mon.keyring文件，再把此文件复制到各监视器的mon data目录下，比如要把它复制给名为ceph集群的mon.a,用此命令:
    cd /tmp/ceph.mon.keyring /var/lib/ceph/mon/ceph-a/keyring

4.为每个OSD生成密钥，{$id}是OSD编号
    ceph auth get-or-create osd.{$id} mon 'allow rwx' osd 'allow *' -o /var/lib/ceph/osd/ceph-{$id}/keyring

5.为每个MDS生成密钥，{$id}是MDS的标识字母
    ceph auth get-or-create mds.{$id} mon 'allow rwx' osd 'allow *' -o /var/lib/ceph/mds/ceph-{$id}/keyring

6.把以下配置加入Ceph配置文件的global段以启用cephx认证
    auth cluster required = cephx
    auth service required = cephx
    auth client required = cephx

7.启动或重启ceph集群
```


