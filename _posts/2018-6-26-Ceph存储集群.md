# Ceph存储集群

------

## 配置
Ceph作为集群时可以包含数千个对象存储设备(OSD)。最简系统至少需要2个OSD做数据复制。要配置OSD集群，要把配置写入到配置文件。Ceph对很多选项提供了默认值，可以在配置文件里覆盖；也可用命令行工具修改运行时配置

Ceph启动时要激活三类守护进程:
+ ceph-mon(必备)
+ ceph-osd(必备)
+ ceph-mds(cephfs必备)

各进程，守护进程或工具都会读取配置文件。一个进程可能需要不止一个守护进程例程的信息(即多个上下文)；一个守护进程或工具只有关于单个守护进程例程的信息(单上下文)

Ceph注重数据安全，Ceph客户端收到数据已写入存储器的通知时，数据确实已写入硬盘。使用较老的内核(<2.6.33),如果日志在原始硬盘上，就要禁用写缓存，较新内核没问题
`sudo hdparm -W 0 /dev/hda 0`

文件系统推荐xfs和ext4

## 配置CEPH
启动Ceph服务时，初始化进程会把一系列守护进程放到后台运行。Ceph存储集群运行两种守护进程:
+ Ceph监视器 (ceph-mon)
+ Ceph OSD守护进程 (ceph-osd)
要支持Ceph文件系统功能，他还需要至少运行一个Ceph元数据服务器(Ceph-mds);支持Ceph对象存储功能的集群还需要运行网关守护进程(radosgw)。为了方便起见，各类守护进程都有一系列默认值(很多由ceph/src/common/config_opts.h配置)，可用ceph配置文件覆盖这些默认值

### 配置文件
启动Ceph存储集群时，各守护进程都从同一个配置文件(ceph.conf)里查找它的配置。手动部署，需创建此文件；用部署工具(ceph-deploy,chef等)创建配置文件时，可以参考下面信息。配置文件定义了:
+ 集群身份
+ 认证配置
+ 集群成员
+ 主机名
+ 主机IP地址
+ 密钥环路径
+ 日志路径
+ 数据路径
+ 其他运行时选项

默认Ceph配置文件位置相继排列如下:
+ $CEPH_CONF($CEPH_CONF环境变量所指示的路径)
+ -c path/path (-c命令行参数)
+ /etc/ceph/ceph.conf
+ ~/.ceph/config
+ ./ceph.conf

Ceph配置文件是ini风格的语法，以分号(;)和井号(#)开始的行时注释

### 配置段落
Ceph配置文件可用于配置存储集群内的所有守护进程，或者某一类型的所有守护进程。
```
[global]
    描述： [global]下的配置影响Ceph集群里的所有守护进程
    实例： auth supported = cephx

[osd]
    描述： [osd]下的配置影响存储集群里的所有ceph-osd进程，并且会覆盖global下的同一选项
    实例： osd journal size = 1000

[mon]
    描述： mon下的配置影响集群里所有ceph-mon进程，并覆盖global下同一选项
    实例： mon addr = 10.0.0.1:6789

[mds]
    描述： mds下的配置影响集群里所有ceph-mds进程，并覆盖global下同一选项
    实例： host = myserver01

[client]
    描述： client下的配置影响所有客户端(如挂载的Ceph文件系统，挂载的块设备)
    实例: log file = /var/log/ceph/radosgw.log
```

全局设置影响集群内所有守护进程的例程，所以[global]可用于设置适用所有守护进程的选项，但可以用这些覆盖[global]设置：
1. 在[osd],[mon],[mds]下更改某一类进程的配置
2. 更改特定进程的设置，如[osd.1]
```
[global]
#Enable authentication between hosts within the cluster.
#v 0.54 and earlier
auth supported = cephx

#v 0.55 and after
auth cluster required = cephx
auth service required = cephx
auth client required = cephx

[osd]
osd journal size = 1000

[osd.1]
# settings affect osd.1 only.

[mon.a]
# settings affect mon.a only.

[mds.b]
# settings affect mds.b only.

[client.radosgw.instance-name]
# settings affect client.radosgw.instance-name only.
```

### 元变量
元变量大大简化了集群配置。Ceph会把配置的元变量展开为具体值；元变量功能强大，可以用在配置文件的[global],[osd],[mon],[mds]段里，类似于BASH的shell扩展
Ceph支持下列变量:
```
$cluster
    描述: 展开为存储集群名字，在同一套硬件上运行多个集群时有用。
    实例: /etc/ceph/$cluster.keyring
    默认值: ceph

$type
    描述: 可展开为 mds 、 osd 、 mon 中的一个，有赖于当前守护进程的类型。
    实例: /var/lib/ceph/$type

$id
    描述: 展开为守护进程标识符； osd.0 应为 0 ， mds.a 是 a 。
    实例: /var/lib/ceph/$type/$cluster-$id

$host
    描述: 展开为当前守护进程的主机名。

$name
    描述: 展开为 $type.$id 。
    实例: /var/run/ceph/$cluster-$name.asok
```

### ceph.conf实例
```
[global]
fsid = {cluster-id}
mon initial members = {hostname}[, {hostname}]
mon host = {ip-address}[, {ip-address}]

#All clusters have a front-side public network.
#If you have two NICs, you can configure a back side cluster 
#network for OSD object replication, heart beats, backfilling,
#recovery, etc.
public network = {network}[, {network}]
#cluster network = {network}[, {network}] 

#Clusters require authentication by default.
auth cluster required = cephx
auth service required = cephx
auth client required = cephx

#Choose reasonable numbers for your journals, number of replicas
#and placement groups.
osd journal size = {n}
osd pool default size = {n}  # Write an object n times.
osd pool default min size = {n} # Allow writing n copy in a degraded state.
osd pool default pg num = {n}
osd pool default pgp num = {n}

#Choose a reasonable crush leaf type.
#0 for a 1-node cluster.
#1 for a multi node cluster in a single rack
#2 for a multi node, multi chassis cluster with multiple hosts in a chassis
#3 for a multi node cluster with hosts across racks, etc.
osd crush chooseleaf type = {n}
```

### 运行时更改
Ceph可以在运行时更改ceph-osd，ceph-mon，ceph-mds守护进程的配置，此功能在增加/降低日志的输出，启用/禁用调试设置，设置时运行时优化的时候很有用：
`ceph tell {daemon-type}.{id or *} injectargs -- {name} {value} [--{name} {value}]`
如: 
`ceph tell osd.0 ingectargs --debug-osd 20 --debug-ms 1`

### 查看运行时配置
如果Ceph存储集群在运行，查看一个运行进程的配置:
`ceph daemon {daemon-type}.{id} config show | less`
如：
`ceph daemon osd.0 config show |less`

### 运行多个集群
用Ceph可以使用在同一套硬件上运行多个集群，运行多个集群与同一集群内用CRUSH规则控制多个存储池相比提供了更高水平的隔离。
运行多个集群时，要为集群命名并用这个名称保存配置文件，如，名为openstack的集群其配置文件将是/etc/ceph/openstack.conf。
**集群名字里只能包含字母a-z和数字0-9**

独立的集群意味着独立数据盘和日志，它们不能在集群中共享。根据前面的元变量，$cluster元变量对应集群名字(之前的openstack)。多处设置都用到$cluster元变量，包括：
+ keyring
+ admin socket
+ log file
+ pid file
+ mon data
+ mon cluster log file
+ osd data
+ osd journal
+ mds data
+ rgw data

创建默认目录和文件时，路径里该用集群名的地方要注意，如:
sudo mkdir /var/lib/ceph/osd/openstack-0
sudo mkdir /var/lib/ceph/mon/openstack-1

要调动一个名字不是ceph集群，要给ceph命令加-c {filename}.conf选项
ceph -c {cluster-name}.conf health
ceph -c openstack.conf health

### 网络配置参考
[建议使用两个网络运营Ceph存储集群](http://docs.ceph.org.cn/rados/configuration/network-config-ref/)

### CEPHX配置参考
认证配置
cephx协议已默认开启。加密认证要耗费一定计算资源，但通常很低。

#### 启用和禁用CEPH
启动cephx后，Ceph将在默认搜索路径(包括/etc/ceph/ceph.$name.keyring)查找密钥环。可在配置文件中[global]添加keyring选项来修改，但不推荐
在禁用了cephx的集群上执行以下步骤来启用，如果(部署工具)已经生成了密钥，可跳过相关步骤：
```
1.创建client.admin密钥，并为客户端保存此密钥的副本
    ceph auth get-or-create client.admin mon 'allow *' mds 'allow *' osd 'allow *' -o /etc/ceph/ceph.client.admin.keyring
注：此命令会覆盖任何存在的/etc/ceph/ceph.client.admin.keyring文件

2.创建监视器集群所需的密钥环，并给它们生成密钥
    ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'

3.把监视器密钥环复制到ceph.mon.keyring文件，再把此文件复制到各监视器的mon data目录下，比如要把它复制给名为ceph集群的mon.a,用此命令:
    cd /tmp/ceph.mon.keyring /var/lib/ceph/mon/ceph-a/keyring

4.为每个OSD生成密钥，{$id}是OSD编号
    ceph auth get-or-create osd.{$id} mon 'allow rwx' osd 'allow *' -o /var/lib/ceph/osd/ceph-{$id}/keyring

5.为每个MDS生成密钥，{$id}是MDS的标识字母
    ceph auth get-or-create mds.{$id} mon 'allow rwx' osd 'allow *' -o /var/lib/ceph/mds/ceph-{$id}/keyring

6.把以下配置加入Ceph配置文件的global段以启用cephx认证
    auth cluster required = cephx
    auth service required = cephx
    auth client required = cephx

    禁用
    auth cluster required = none
    ...

7.启动或重启ceph集群
```

### 配置选项
启用事项
```
auth cluster required
    描述： 如果启用了，集群守护进程(如ceph-mon，ceph-mds，ceph-osd)间必须相互认证，可用选项cephx或none
    类型： String
    是否必须： No
    默认值： cephx

auth service required
    描述: 如果启用，客户端要访问 Ceph 服务的话，集群守护进程会要求它和集群认证。可用选项为 cephx 或 none 。
    类型: String
    是否必需:   No
    默认值:    cephx.

auth client required
    描述: 如果启用，客户端会要求 Ceph 集群和它认证。可用选项为 cephx 或 none 。
    类型: String
    是否必需:   No
    默认值:    cephx.
```

### 密钥
如果集群启用了认证，ceph管理命令和客户端得有密钥才能访问集群
给ceph管理命令和客户端提供密钥的最常用方法就是把密钥环放到/etc/ceph，通过ceph-deploy不熟的Cuttlefish及更高版本，其文件名通常是ceph.client.admin.keyring，如果密钥位于/etc/ceph下，就不需要在Ceph配置文件里指定keyring选项了。

建议把集群的密钥环复制到执行管理命令的节点，包含client.admin密钥

可以用ceph-deploy admin命令做此事
```
keyring
    描述: 密钥环文件的路径。
    类型: String
    是否必需:   No
    默认值:    /etc/ceph/$cluster.$name.keyring,/etc/ceph/$cluster.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin

keyfile
    描述: 到密钥文件的路径，如一个只包含密钥的文件。
    类型: String
    是否必需:   No
    默认值:    None

key
    描述: 密钥（密钥文本），最好别这样做。
    类型: String
    是否必需:   No
    默认值:    None
```

### 守护进程密钥环
管理员用户或部署工具(如ceph-deploy)生成守护进程密钥环或生成用户密钥环的方法一样。默认情况下，守护进程把密钥环保存在各自的数据目录下，默认密钥环位置和守护进程发挥作用必需的能力展示如下:
```
ceph-mon
    位置： $mon_data/keyring
    能力： mon 'allow *'

ceph-osd
    位置： $osd_data/keyring
    能力： mon 'allow profile osd' osd 'allow *'

ceph-mds
    位置： $mds_data/keyring
    能力： mds 'allow' mon 'allow profile mds' osd 'allow rwx'

radosgw
    位置： $rgw_data/keyring
    能力: mon 'allow rwx' osd 'allow rwx'
```

注: 监视器密钥环(mon.)包含一个密钥环，但没有能力，且不是集群auth数据库的一部分

守护进程数据目录位置默认格式如下:
`/var/lib/ceph/$type/$cluster-id`

### 签名
在Bobtail及后续版本中，Ceph会用开始认证生成的会话密钥认证所有在线实体。然而Argonaut及之前版本不知道如何认证在线消息，为保持向后兼容，消息签名默认是关闭的。
```
cephx require signatures
    描述: 若设置为 true ， Ceph 集群会要求客户端签名所有消息，包括集群内其他守护进程间的。
    类型: Boolean
    是否必需:   No
    默认值:    false

cephx cluster require signatures
    描述: 若设置为 true ， Ceph 要求集群内所有守护进程签名相互之间的消息。
    类型: Boolean
    是否必需:   No
    默认值:    false

cephx service require signatures
    描述: 若设置为 true ， Ceph 要求签名所有客户端和集群间的消息。
    类型: Boolean
    是否必需:   No
    默认值:    false

cephx sign messages
    描述: 如果 Ceph 版本支持消息签名， Ceph 会签名所有消息以防欺骗。
    类型: Boolean
    默认值:    true
```

### 生存期
```
auth service ticket ttl
    描述: Ceph 存储集群发给客户端一个用于认证的票据时分配给这个票据的生存期。
    类型: Double
    默认值:    60*60
```

## 监视器配置参考
任何Ceph集群都需要至少一个监视器。
监视器们维护着集群运行图的"主副本"，就是说客户端连到一个监视器并获取当前运行图就能确定所有监视器，OSD和元数据服务器的位置。Ceph客户端读写OSD或元数据服务器前，必须先连到一个监视器，靠当前集群运行图的副本和CRUSH算法，客户端能计算出任何对象的位置，故此客户端有能力直接连到OSD，这对Ceph的高伸缩性，高性能来说很重要。

监视器的主要角色是维护集群运行图的主副本，也提供认证和日志记录服务。Ceph监视器们把监视器服务的所有更改写入一个单独的Paxos例程，然后Paxos以键值方式存储所有变更以实现高度一致性。同步期间，Ceph监视器能查询集群运行图的近期版本，他们通过操作键值存储快照和迭代器(用leveldb)来进行存储级同步


### 集群运行图
集群运行图是多个图的组合，包括监视器图，OSD图，归置组图和元数据服务器图。集群运行图追踪几个重要事件: 哪些进程在集群里(in);哪些进程在集群里(in)是UP且在运行，或down；归置组状态是active或inactive，clean或其他状态；和其他反应其他当前集群状态的信息，像总存储容量，和使用量

当集群状态有明显变更时，如一OSD挂了，一归置组降级了等，集群运行图会被更新以反映集群当前状态。另外，监视器也维护着集群的主要状态历史。监视器图，OSD图，归置组图和元数据服务器图各自维护着它们的运行图版本。我们把各图的版本称为一个epoch。

### 监视器法定人数
单监视器是一个单故障点，生产集群要实现高可用性的话得配置多个监视器，这样单个监视器的失效才不会影响整个集群

集群用多个监视器实现高可用性时，多个监视器用Paxos算法对主机群运行图达成一致，这里的一致要求监视器都在运行且构成法定人数(如1个，2/3在运行，3/5运行等)

### 一致性
把监视器加进Ceph配置文件是，要注意一些架构问题，Ceph发现集群内的其他监视器时对其有着严格的一致性要求。尽管如此，Ceph客户端和其他Ceph守护进程用配置文件发现监视器，监视器却用监视器图(monmap)相互发现而非配置文件

一个监视器发现集群内的其他监视器时总是参考monmap的本地副本，用monmap而非Ceph配置文件避免了可能损坏集群的错误(如ceph。conf中执行地址或端口的拼写错误)。这是因为监视器把monmap用于发现，并共享于客户端和其他Ceph守护进程间，**monmap可严格地保证监视器的一致性是可靠地。**

严格的一致性也适用于monmap的更新，因为关于监视器的任何更新，关于monmap的变更都是通过称为Paxos的分布式一致性算法传递的。监视器们必须就monmap的每次更新达成一致，以确保法定人数里的每个监视器monmap版本相同。

初始化监视器：
一个监视器需要4个选项
+ 文件系统标识符： fsid是对象存储的唯一标识符。因为你可以在一套硬件上运行多个集群，所以在初始化监视器时必须指定对象存储的唯一标识符。部署工具通常可替你完成(ceph-deploy会调用类似uuidgen的程序)，但也可以手动指定fsid
+ 监视器标识符： 监视器标识符是分配给集群内个监视器的唯一ID，它是一个字母数字组合，为方便起见，标识符通常以字母顺序结尾，可设置于Ceph配置文件，部署工具，或ceph命令工具等 
+ 密钥：监视器必须有密钥。像ceph-deploy这样的部署工具通常会自动生成，也可手动生成

```
[global]
[mon]
    mon host = hostname1,hostname2,hostname3
    mon addr = 10.0.0.1:6789,10.0.0.2.6789,10.0.0.3:6789
[mon.a]
    host = hostname1
    mon addr = 10.0.0.1:6789
...
```

[http://docs.ceph.org.cn/rados/configuration/mon-config-ref/](http://docs.ceph.org.cn/rados/configuration/mon-config-ref/)

## 监视器与OSD交互的配置
执行ceph health或ceph -s命令时，监视器会报告Ceph存储集群当前状态。监视器通过让各OSD自己报告，并接收OSD关于邻居状态的报告来掌握集群动态，如果监视器没有收到报告，或者它只收到集群的变更报告，那它就要更新集群运行图。

### OSD验证心跳
各OSD每6秒都会与其他OSD进行心跳检查，用[osd]下的osd heartbeat internal可更改此间隔，或运行时更改。如果一个OSD在20秒内没有更新，集群就认为它down了，用[osd]的osd heartbeat grace可更改宽限期，或运行时更改

### OSD报告死亡OSD
默认情况下，一个OSD必须向监视器报告3次另一个OSD down的消息，监视器才会认为那个被报告的OSD down了；配置文件里[mon]段下的mon osd min down reports可更改这个最少osd down消息次数，或运行时更改。

### OSD报告互联失败

### OSD报告自己的状态
如果一OSD在mon osd report timeout时间内没向监视器报告过，监视器就认为它down了。设置[osd]下的osd mon report interval min来更改最小报告间隔，osd mon report interval max最大间隔

[配置选项](http://docs.ceph.org.cn/rados/configuration/mon-osd-interaction/)


## OSD配置参考
可通过配置文件调整OSD，但靠默认值和极少的配置OSD守护进程就能运行。最简OSD配置需设置osd journal size和host，其他几乎都能用默认值
Ceph的OSD守护进程用递增的数字做标识，按惯例从0开始：
```
osd.0
osd.1
osd.2
```

在配置文件里，[osd]段下的配置适用于所有OSD；要添加针对特定OSD的选项(如host)，把它放到那个OSD段下即可：
```
[osd]
    osd journal size = 1024
[osd.0]
    host = osd-host-a
[osd.1]
    host = osd-host-b
```

### 常规配置
日志尺寸应该大于期望的驱动速度和filestore max sync interval之乘积的两倍；最常见的方法是为日志驱动器分区并挂载号，这样Ceph就可以用整个分区做日志

[http://docs.ceph.org.cn/rados/configuration/osd-config-ref/](http://docs.ceph.org.cn/rados/configuration/osd-config-ref/)


## FILESTORE配置参考
```
filestore debug omap check
    描述： 打开对同步检查过程的调试，代价很高，仅用于调试
    类型： Boolean
    是否必须： no
    默认值： 0
```

[http://docs.ceph.org.cn/rados/configuration/filestore-config-ref/](http://docs.ceph.org.cn/rados/configuration/filestore-config-ref/)

## KEYVALUESTORE配置参考
```
keyvaluestore backend
    描述： KeyValueStore存储使用的后端
    类型： Type
    是否必须： No
    默认值： leveldb
```
[http://docs.ceph.org.cn/rados/configuration/keyvaluestore-config-ref/](http://docs.ceph.org.cn/rados/configuration/keyvaluestore-config-ref/)

## 日志配置参考
Ceph的OSD使用日志的原因有二： 速度和一致性
+ 速度：日志使得OSD可以快速地提交小块数据的写入，Ceph把小片，随机IO依次写入日志，这样，后端文件系统就有可能归并写入动作，并最终提升并发承载力。因此，使用OSD日志能展现出优秀的突发写性能，实际上数据还没有写入OSD，因为文件系统把它们捕捉到了日志
+ 一致性： ceph的OSD守护进程需要一个能保证原子操作的文件系统接口。OSD把一个操作的描述写入日志，然后把操作应用到文件系统，这需要原子更新一个对象(例如归置组元数据)。每隔一段filestore max sync interval和filestore min sync interval之间的时间，OSD停止写入，把日志同步到文件系统，这样运行OSD修整日志里的操作并重用空间，若失败，OSD从上个同步点重放日志
+ 
[http://docs.ceph.org.cn/rados/configuration/journal-ref/](http://docs.ceph.org.cn/rados/configuration/journal-ref/)

## 存储池，归置组和CRUSH配置参考
当创建存储池并给它设置归置组数量时，如果没有指定Ceph就使用默认值。建议更改某些默认值，特别是存储池的副本数和默认归置组数量，可以在pool命令的时候设置这些值，也可以把配置写入到Ceph配置文件[global]段覆盖默认值
```
[global]
    osd pool default size = 4
    osd pool default min size = 1
    osd pool default pg num = 250
    osd poll default pg p num = 250
```
[http://docs.ceph.org.cn/rados/configuration/pool-pg-config-ref/](http://docs.ceph.org.cn/rados/configuration/pool-pg-config-ref/)

## 消息传递
[http://docs.ceph.org.cn/rados/configuration/ms-ref/](http://docs.ceph.org.cn/rados/configuration/ms-ref/)

## 常规配置参考
[http://docs.ceph.org.cn/rados/configuration/general-config-ref/](http://docs.ceph.org.cn/rados/configuration/general-config-ref/)

