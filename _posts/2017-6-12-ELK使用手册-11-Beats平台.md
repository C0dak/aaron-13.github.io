# Beats平台

------

Beats平台是Elastic.co从packetbeat发展出来的数据收集器系统。beat收集器可以直接写入Elasticsearch，也可以传输给Logstash。其中抽象出来的libbeat，提供了统一的数据发送方法，输入配置解析，日志记录框架等功能。

所有的beat工具，在配置上，除了input以外，在output，filter，shipper，logging，run-options上的配置规则都是一致的。


**filter**

5.0版本后，beats新增了简单的filter功能，用来完成事件过滤和字段删减:

```
filters:
    - drop_event:
        regexp:
            message: "^DBG:"
    - drop_fields:
        contains:
            source: "test"
    - include_fields:
        fields: ["http.code","http.host"]
        equals:
            http.code: 200
        range:
            gte:
                cpu.user_p: 0.5
            lt:
                cpu.user_p: 0.8
```


可用的条件判断包括:

+ equals

+ contains

+ regexp

+ range

+ or

+ and

+ not


**output**

目前beat可以发送数据给Elasticsearch，Logstash，File，Kafka，Redis和Console六种目的地


**Elasticsearch**

beats发送到Elasticsearch也是走HTTP接口，示例配置如下:

```
output:
    elasticsearch:
        hosts: ["http://localhost:9200","https://onesslip:9200","anotherip"]
        parameters: {pipeline: my_pipeline_id}  #仅用于Elasticsearch5.0以后的ingest方式
        username: "user"
        password: "pwd"
        index: "topbeat"
        bulk_max_size: 20000
        flush_interval: 5
        tls:
            certificate_authorities: ["/etc/pki/root/ca.pem"]
            certificate: "/etc/pki/client/cert.pem"
            certificatekey: "/etc/pki/clinet/cert.key"
```

+ `hosts`中可以通过URL的不同形式，来表示HTTP还是HTTPS，是否有添加代理层的URL路径等情况

+ `index`表示写入Elasticsearch时索引的前缀，比如示例表示索引名未`topbeat-yyyy.MM.dd`


**Logstash**

beat写入Logstash时，会配置Logstash-1.5后新增的metadata特性。将beat名和type名记录在metadata里，所以对应的Logstash配置应该是这样:

```
input {
  beats {
    prot =>5044
  }
}

output {
  elasticsearch {
    hosts: ["http://localhost:9200]
    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
    document_type => "%{[@metadata][type]}"
  }
}
```

beat示例配置段如下:

```
output:
    logstash:
        hosts: ["localhost:5044","localhost:5045"]
        workers: 2
        loadbalance: true
        index: topbeat
```


**File**

```
output:
    file:
        path: "/tmp/topbeat"
        filename: topbeat
        rotate_every_kb: 1000
        number_of_files: 7
```


**Kafka**


```
output:
    kafka:
        hosts: ["kafka1:9092", "kafka2:9092", "kafka3:9092"]
        topic: '%{[type]}'
        topics:
            - key: "info_list"
              when:
                  contains:
                      message: "INFO"
            - key: "debug_list"
              when:
                  contains:
                      message: "DEBUG"
            - key: "%{[type]}"
              mapping:
                  "http": "frontend_list"
                  "nginx": "frontend_list"
                  "mysql": "backend_list"
        partition:
            round_robin:
                reachable_only: true
        required_acks: 1
        compression: gzip
        max_message_bytes: 1000000
```

+ 大于`max_message_bytes`长度事件(不只是原日志长度)会被直接丢弃

+ partition策略默认为hash。可选项还有random和round_robin

+ compression可选项还有none和snappy

+ required_acks可选项有-1,0和1.分别代表: 等待全部副本完成，不等待，等待本地完成

+ topics用来配置基于匹配规则的选择器，支持when和mapping，when条件下可使用上小节列出的各种filter，如果都匹配不上，则采用topic配置



**Redis**

```
output:
    redis:
        hosts: ["localhost"]
        password: "my_password"
        key: "filebeat"
        db: 0
        timeout: 5
```


**Console**

```
output:
    console:
        pretty: true
```


**shipper**

shipper部分是一些和网络拓扑相关的配置，就目前来说，大多数是packetbeat独有的。

```
shipper:
    name: "my-shipper"
    tags: ["my_service","hardware","test"]
    ignore_outgoing: true
    refresh_topology_freq: 10
    topology_expire: 15
    geoip:
        paths:
            - "/usr/share/GeoIP/GeoLiteCity.dat"
```


**logging**

```
logging:  
    level: warning
    to_files: true
    to_syslog: false
    files:
        path: /var/log/mybeat
        name: mybeat.log
        keepfiles: 7
```

**run options**

```
runoptions:
    uid=501
    gid=501
```




