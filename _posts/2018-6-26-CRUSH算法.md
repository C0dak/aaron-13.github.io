# CRUSH算法

------

## 简介
随着大规模分布式存储系统(PB级的数据和成千上百台存储设备)的出现。这些系统必须平衡的分布数据和负载，最大化系统的性能，并要处理系统的扩展和硬件失效。Ceph设计了CRUSH(一种可扩展的伪随机数据分布算法)，用在分布式对象存储系统上，可以有效映射数据对象到存储设备上(不需要中心设备)。因为大型系统的结构是动态变化的，CRUSH能够处理存储设备的添加和移除，并最小化由于存储设备的添加和移动而导致的数据迁移。
为了保证负载均衡，保证新旧数据混合在一起。但是简单HASH分布式不能有效处理设备数量的变化，导致大量数据迁移。ceph开发了CRUSH(Controlled Replication Under Scalable Hashing)，一种伪随机数据分布算法，能够在层级结构的存储集群中有效的分布对象的副本。CRUSH实现了一种伪随机(确定性)的函数，它的参数是object id或object group id，并返回一组存储设备(用于保存object副本OSD)。CRUSH需要cluster map(描述存储集群的层级结构)，和副本分布策略(rule)

CRUSH有两个关键点：
    + 任何组件都可以独立计算出每个obejct所在的位置(去中心化)
    + 只需要很少的元数据(cluster map)，只要当删除添加设备时，这些元数据才需要改变

CRUSH的目的是利用可用资源优化分配数据，当存储设备添加或删除时高效的重组数据，以及灵活地约束对象副本放置，当数据同步或相关硬件故障的时候最大化保证数据安全。支持各种各样的数据安全机制，包括多方复制(镜像)，RAID奇偶校验方案或其他形式的校验码，以及混合方法(如RAID-10)。这些特性使得CRUSH适合管理对象分布非常大的(PB级别)，要求可伸缩性，性能和可靠性非常高的存储系统。简而言之就是PG归置组到OSD的映射过程。

## 映射过程
### 概念
ceph中的Pool的属性有: 1.obejct的副本数 2.Placement Groups的数量 3.所使用的CRUSH Ruleset
数据映射(Data Placement)的方式决定了存储系统的性能和扩展性。(Pool, PG) --> OSD set的映射由四个因素决定：
+ CRUSH算法
+ OSD WAP: 包括当前所有pool的状态和OSD的状态。OSDMap管理当前ceph中所有的OSD，OSDMap规定了crush算法的一个范围，在这个范围中选择OSD结合。OSDMap其实就是一个树形的结构，叶子节点是device(即OSD)，其他的节点称为bucket节点，这些bucket节点是虚构的节点，可以根据物理结构进行抽象，当然树形结构只有一个最终的根节点称之为root节点，中间虚拟的bucket节点可以是数据中心抽象，机房抽象，机架抽象，主机抽象等
![crush-01.png](https://aaron-13.github.io/images/crush-01.png)
+ CRUSH MAP： 包含当前磁盘，服务器，机架的层级结构
+ CRUSH Rules： 数据映射的策略。这些策略可以灵活的设置object存放的区域。比如可以指定pool1中所有的objects放置的机架1上，所有objects的第1个副本放置在机架1上的服务器A上，第2个副本分布在机架上1上的服务器B上。pool2中所有的object分布在机架2,3,4上，所有object的第1个副本分布在机架2的服务器上，第2个副本分布在机架3的服务器上，第4个副本在机架4的服务器上

### 流程
Ceph架构中，Ceph客户端是直接读或者写存放在OSD上的RADOS对象存储中的对象(data object)的，因此，Ceph需要走完(Pool,Object) --> (Pool,PG) --> OSD set --> OSD/Disk完整的链路，才能让ceph client知道目标数据object的具体位置在哪里

数据写入时，文件被切分成object，object先映射到PG，再由PG映射到OSD set。每个pool有多个PG，每个object通过计算hash值并取模得到它所对应的PG。PG再映射到一组OSD(OSD个数由pool的副本数决定)，第一个OSD是primary，剩下的都是Replicas。

Ceph分布数据的过程：首先计算数据x的Hash值并将结果和PG数目取余，得到数据X对应的PG编号。然后，通过CRUSH算法将PG映射到一组OSD中。最后把数据X存放到PG对应的OSD中。这个过程中包含了两次映射，第一次是数据X到PG的映射。PG是抽象的存储节点，它不会随着物理节点的加入或离开而增加或减少，因此数据到PG的映射是稳定的。

+ 创建Pool和它的PG。根据上述的计算过程，PG在Pool被创建后就会被MON根据CRUSH算法计算出来的PG应该所在若干的OSD上被创建出来。即，在客户端写入对象的时候，PG已经被创建好了，PG和OSD的映射关系是已经确定了的
+ Ceph客户端通过哈希算法计算出存放obejct的PG的ID：
    * 客户端输入Pool ID和Object ID(如 pool="liverpool" and object-id="john")
    * ceph对obejct ID做哈希
    * ceph对该hash值取PG总数的模，得到PG的编号
    * ceph对pool ID取hash
    * ceph将pool ID和PG ID组合在一起得到完整的ID
**PG-id=hash(pool-id).hash(object-id) % PG-number**

+ 客户端通过CRUSH算法计算出Object应该会被保存到PG中哪个OSD上。(这里说应该，而不是将会，因为PG和OSD之间的关系是已经确定了的，那客户端需要做的就是需要知道它所选中的这个PG到底将会在哪些OSD上创建对象)。这步骤也叫CRUSH查找。

对Ceph客户端来说，只要它获得了cluster map，就可以使用CRUSH算法计算出某个object将要所在的OSD的ID，然后直接与它通信。
- Ceph client从MON中获取最新的cluster map
- Ceph client根据上面的第2步计算出该Object将要在的PG的ID
- Ceph client再根据CRUSH算法计算出PG中目标主和次OSD的ID
**OSD-ids = CRUSH(PG-id,cluster-map,crush-rules)**

### CRUSH算法
CRUSH算法根据每个设备的权重尽可能概率平均分配数据。分布算法是由集群可用存储资源以及其逻辑单元的map控制的。这个map的描述类似于一个大型服务器的描述： 服务器由一系列的机柜组成，机柜装满服务器，服务器装满磁盘。数据分配的策略是由定位规则来定义的，定位规则指定了集群中将保存多少个副本，以及数据副本放置有什么限制。

给定一个输入X，CRUSH算法将输出一个确定的有序的存储目标向量R，当输入X，CRUSH利用强大的多重整数hash函数根据集群map，定位规则，以及x计算出独立的完全确定可靠地映射关系。CRUSH分配算法是伪随机算法，并且输入的内容和输出的存储位置之间没有显示相关的。可以说CRUSH算法在集群设备中生成了"伪集群"的数据副本。集群的设备对一个数据项目共享数据副本，对其他数据项目又是独立的。

CRUSH算法通过每个设备的权重来计算数据对象的分布。对象分布是由cluster map和data distribution policy决定的。**cluster map描述了可用存储资源和层级结构(如多少个机架，机架上多少台服务器，每个服务器有多少个磁盘)。data distribution policy由palcement rules组成。rule决定了每个数据对象有多少个副本，这些副本存储的限制条件(如3个副本放在不同机架上)**

CRUSH算出X到一组OSD集合(OSD是对象存储设备)
(osd0,osd1,osd2...) = CRUSH(X)

CRUSH利用多参数HASH哈函数，HASH函数中的参数包括X,使得从X到OSD集合是确定性和独立的。CRUSH只使用了Cluster map，palcement rules，X。CRUSH是伪随机算法，相似输入的结果没有相关性

Cluster map由devices和bucket组成，它们都有id和权重值。Bucket可以包含任意数量item。item可以都是devices或者都是buckets。管理员控制存储设备的权重。权重和存储设备的容量有关。Bucket的权重被定义为它所包含所有item的权重之和。CRUSH基于4种不同的bucket type，每种有不同的选择算法：
+ 分层集群映射(cluster map)
    * 集群映射由设备和桶(buckets)组成，设备和桶都有数值的描述和权重值。桶可以包含任意多的设备或者其他的桶，使他们形成内部节点的存储层次结构，设备总是在叶节点。存储设备的权重由管理员设置以控制相应设备负责存储的相对数据量。尽管大型系统的设备含不同的容量大小和性能特性刁难，随机数据分布算法可以根据设备利用率和负载来分布数据。这样设备平均负载和存储的数据量成正比。这导致一维位置指标，权重，应来源于设备的能力。桶的权重是它所包含的元素的权重的总和。

+ 副本放置(Replica Placement)
    * CRUSH算法的设置目的是使数据能够根据设备的存储能力和看待资源加权平均的分布，并保持一个相对的概率平衡。副本放在在具有层次结构的存储设备中，这对数据安全也有重要影响。通过反射系统的物理安装组织，CRUSH算法可以将系统模块化，从而定位潜在的设备故障。这些潜在的故障的资源包括物理的，比如共用电源，网络。通过向集群映射编码信息，CRUSH副本放置策略可以将数据对象独立在不同故障域，同时仍然保持所需的分布。CRUSH算法是为了使用千篇一律的脚本，向数据复制策略和底层的硬件配置，CRUSH对于没分数据的赋值策略或者分布式策略的部署方式，允许存储系统或者管理员精确指定对象副本如何放置。

+ Map的变化和数据移动
    * 在大型文件系统中一个比较典型的二部分就是数据存储资源中的增加和移动。为了避免非对称造成的系统压力和资源的不充分利用，CRUSH主张均衡的数据分布和系统负载。当存储系统中个别设备宕机后，CRUSH会对这些宕机设备做好标记，并且会将其从存储架构中移除，这样这些设备就不会参与后面的存储，同时也会将其上面的数据复制一份到其他机器进行存储

+ Bucket类型
    * 一般而言，CRUSH的开发是为了协调两个计算目标： map计算的高效性和可伸缩性，以及当添加或者移除存储设备后的数据均衡。在最后，CRUSH定义了4中类型的buckets来代表集群架构中的叶子节点： 一般的buckets，列表式buckets，树结构buckets以及稻草类型buckets。对于在数据副本存储的进程中的伪随机选择嵌套项，每个类型的bucket都是建立在不同的内部数据结构和充分利用不同c(r,x)函数的基础上，这些buckets在计算和重构效率上发挥着不同的权衡性。一般的bucket会被所具有相同权重的项限制，然后其他类型的bucket可以在任何组合权重中包含混合项。这些bucket的差异总结如下：
|Action|Uniform|List|Tree|Straw|
|:-:|:-:|:-:|:-:|:-:|
|Speed|O(1)|O(n)|O(log n)|O(n)|
|Addtions|poor|optimal|good|optimal|
|Removals|poor|poor|good|optimal|

    - 一般的Bucket
    这些存储设备纯粹按个体添加进一个大型存储系统，取而代之的是，新型存储系统上存储的都是文件块，就像将机器添加进机架或者整个机柜一样。这些设备在退役后会被拆分成各个零件。在这样的环境下CRUSH中的一般类型Bucket会被当成一个设备集合一样进行使用。这样做的好处在于，CRUSH可以一直map复制品到一般的bucket中。在这种情况下，正常使用的Bucket就可以和不能正常使用的Bucket直接互不影响。

    - List类型的buckets
    List类型的buckets组织其内部的内容会像list方式一样，并且里面的项都有随机的权重。为了放置一个数据副本，CRUSH在list的头部开始添加项并且和除这些项外其他项的权重进行比较。根据hash(x,r,item)函数的值，每个当前项会根据适合的概率被选择，或者出现继续递归查找该list。这种方法重申了数据存储所存在的问题"是大部分新加项还是旧项"，这对于一个扩展中的集群是一个根本且直观的选择： 一方面每个数据对象会根据相应的概率重新分配到新的存储设备上，或者依然像一样被存储在旧的存储设备上，这样当新的项添加进bucket中时这些项会获得最优的移动方式。当这些项从list的中间或者末尾开始移动时，list类型的bucket将比较适合这种环境

    - 树状Buckets
    像任何链表结构一样，列表bucket对于少量的数据还是高效的，而遇到大量的数据就不合适了，其时间复杂度太大。树状Buckets由CRUSH发展而来，它通过将这些大量的数据项存储到一个二叉树来解决这个问题。它将定位的时间复杂度由O(n)降低到O(logn)，这使其适用于管理大得多设备数量或嵌套buckets。CRUSH等价于一个由单一树状bucket组成的二级CRUSH结构，该树状bucket包含许多一般buckets。
    树状buckets是一种加权二叉排序树，数据项位于树的叶子节点，每个递归节点有其左子树和右子树的总权重，并根据一种固定算法进行标记。为了从bucket中选择一个数据项，CRUSH由树的根节点开始，计算输入主键X，副本数量r，bucket标识以及当前节点标志的哈希值，计算的结果会跟当前节点左子树和右子树的权重进行比较，以确定下次访问的节点。重复这一过程，直到到达相应数据项的叶子节点。定位该数据项最多只需要进行O(logn)次哈希值计算和比较。
    该bucket二叉树节点使用一种简单固定的策略来得到二进制数进行标记，以避免当树增长或收缩时标记更改。该树最左侧的叶子节点通常标记为"1",每次树扩展时，原来的根节点称为新根节点的左子树，新根节点的标记由原根节点的标记左移一位得到(如1变成10,10变成100等)。右子树的标记在左子树标记的基础上增加了"1"。这一策略保证了当bucket增加(或删除)新数据项并且树结构增长(或收缩)时，二叉树中现有项的路径通过在根节点增加(或删除)额外节点即可实现，决策树的初始位置随之发生变化。一旦某个对象放入特定的子树中，其最终的mapping将仅由盖子树中的权重和节点标记来决定，只要该子树中的数据项不发生变化mapping就不会发生变化。虽然层次化的决策树在嵌套数据项之间会增加额外的数据迁移，但是这一标记策略可以保证移动在可接受范围内，同时还能为非常巨大的bucket提供有效的mapping。

    - Straw类型Buckets
    列表buckets和树状buckets的结构决定了只有有限的哈希值需要计算并与权重进行比较以确定bucket中的项。这样做的话，它们采用了分而治之的方式，要么给特定项以优先权，要么消除完全考虑整个子树的必要。尽管这样提高了副本定位过程的效率，但当向buckets中增加项，删除项或者重新计算某一项的权重已改变其内容时，其重组的过程是次最优的。
    Straw类型bucket允许所有项通过类似抽签的方式来与其他项公平竞争。定位副本时，bucket中的每一项都对应一个随机长度的straw，且又有最长长度的straw会被选中。每个straw的长度都是有固定区间内基于CRUSH输入x，副本数目r，以及bucket项i的哈希值计算得到的一个值，每个straw长度都乘以根据该项权重的立方获得的一个系数f(wi),这样拥有最大权重的项更容器被选中，如c(r,x)=maxi(f(wi)hash(x,r,i))。尽管straw类型bucket定位过程要比列表bucket平均慢一倍，甚至比树状bucket都要慢，但是straw类型的bucket在修改时最近邻近之间的数据移动是最优的。
    Bucket类型的选择是基于预期的集群增长类型，一权衡映射方法的运算量和数据移动之间的效率，这样的权衡是非常值得的。当buckets是固定时，一般类型的buckets是最快的。当一个bucket预计将会不断增长，则列表类型的buckets在其列表开头插入新项时将提供最优的数据移动。这允许CRUSH准确恰当地转移足够的数据到新添加的设备中，而不影响其他bucket项。其缺点是映射速度的时间复杂度为O(n)且当旧项移除或重新计算权重时会增加额外的数据移动。当删除和重新计算权重的效率特别重要时(比如存储结构的根节点附近(项))，straw类型的buckets可以为子树之间的数据移动提供最优的解决方案，树状buckets是一种适用于任何情况的buckets，兼具高性能与出色的重组效率。

[CRUSH](https://www.cnblogs.com/chenxianpao/p/5568207.html)
