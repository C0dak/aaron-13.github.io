# CRUSH算法

------

## 简介
随着大规模分布式存储系统(PB级的数据和成千上百台存储设备)的出现。这些系统必须平衡的分布数据和负载，最大化系统的性能，并要处理系统的扩展和硬件失效。Ceph设计了CRUSH(一种可扩展的伪随机数据分布算法)，用在分布式对象存储系统上，可以有效映射数据对象到存储设备上(不需要中心设备)。因为大型系统的结构是动态变化的，CRUSH能够处理存储设备的添加和移除，并最小化由于存储设备的添加和移动而导致的数据迁移。
为了保证负载均衡，保证新旧数据混合在一起。但是简单HASH分布式不能有效处理设备数量的变化，导致大量数据迁移。ceph开发了CRUSH(Controlled Replication Under Scalable Hashing)，一种伪随机数据分布算法，能够在层级结构的存储集群中有效的分布对象的副本。CRUSH实现了一种伪随机(确定性)的函数，它的参数是object id或object group id，并返回一组存储设备(用于保存object副本OSD)。CRUSH需要cluster map(描述存储集群的层级结构)，和副本分布策略(rule)

CRUSH有两个关键点：
    + 任何组件都可以独立计算出每个obejct所在的位置(去中心化)
    + 只需要很少的元数据(cluster map)，只要当删除添加设备时，这些元数据才需要改变

CRUSH的目的是利用可用资源优化分配数据，当存储设备添加或删除时高效的重组数据，以及灵活地约束对象副本放置，当数据同步或相关硬件故障的时候最大化保证数据安全。支持各种各样的数据安全机制，包括多方复制(镜像)，RAID奇偶校验方案或其他形式的校验码，以及混合方法(如RAID-10)。这些特性使得CRUSH适合管理对象分布非常大的(PB级别)，要求可伸缩性，性能和可靠性非常高的存储系统。简而言之就是PG归置组到OSD的映射过程。

## 映射过程
### 概念
ceph中的Pool的属性有: 1.obejct的副本数 2.Placement Groups的数量 3.所使用的CRUSH Ruleset
数据映射(Data Placement)的方式决定了存储系统的性能和扩展性。(Pool, PG) --> OSD set的映射由四个因素决定：
+ CRUSH算法
+ OSD WAP: 包括当前所有pool的状态和OSD的状态。OSDMap管理当前ceph中所有的OSD，OSDMap规定了crush算法的一个范围，在这个范围中选择OSD结合。OSDMap其实就是一个树形的结构，叶子节点是device(即OSD)，其他的节点称为bucket节点，这些bucket节点是虚构的节点，可以根据物理结构进行抽象，当然树形结构只有一个最终的根节点称之为root节点，中间虚拟的bucket节点可以是数据中心抽象，机房抽象，机架抽象，主机抽象等
![crush-01.png](https://aaron-13.github.io/images/crush-01.png)
