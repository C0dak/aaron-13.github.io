# CEPH部署

------

## 预检
### 选择一个操作系统

### 安装SSH服务器

### 创建用户
```
ssh user@ceph-server
sudo useradd -d /home/ceph -m ceph
sudo passwd ceph

echo "ceph ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph
sudo chmod 0440 /etc/sudoers.d/ceph
```

### 配置SSH
```
ssh-keygen -t rsa -P''

ssh-copy-id -i ~/.ssh/id_rsa.pub user@server

修改管理节点上的~/.ssh/config，以使未指定用户名时默认使用刚刚创建的用户名
Host ceph-server
    HostName ceph-server.fqdn-or-ip-address.com
    User Ceph
```

### 安装CEPH-DEPLOY
```
wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -
echo deb http://ceph.com/debian-dumpling/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
sudo apt-get update
sudo apt-get install ceph-deploy
```

## 软件包管理
### 安装
要在集群主机上安装Ceph软件包，在管理主机上打开命令行并执行以下命令:
`ceph-deploy install {hostname [hostname]...}`

没提供额外选项的话，ceph-deploy默认会把最新稳定版安装到集群主机，要指定某个软件包可用以下参数:
+ --release <code-name>
+ --testing
+ --dev <branch-or-tag>
```
ceph-deploy install --release cuttlefish hostname1
ceph-deploy install --testing hostname2
ceph-deploy install --dev wip-some-branch hostname{1,2,3,4,5}
```

### 卸载
```
ceph-deploy uninstall {hostname [hostname]...}

在Debian或Ubuntu上：
ceph-deploy purge {hostname [hostname]...}
```

## 创建集群
使用ceph-deploy的第一步是创建一个集群，新集群具备:
+ 一个Ceph配置文件
+ 一个监视器密钥环

Ceph配置文件至少包含：
+ 它自己的文件系统ID(fsid)
+ 最初的监视器机器主机名
+ 最初的监视器及其IP地址

ceph-deploy工具也创建了一个监视器密钥环并置于[mon.]

### 用法
用ceph-deploy创建集群，用new命令，并指定几个主机作为初始监视器法定人数
```
ceph-deploy new {host [host]...}

如：
ceph-deploy new mon1.foo.com
ceph-deploy new mon{1,2,3}
```
ceph-deploy工具会用DNS把主机名解析为IP地址。监视器将被命名为域名的第一段(如mon1)，它会指定主机名加入Ceph配置文件

### 命名集群
Ceph集群的默认名称为ceph，如果想在同一套硬件上运行多套集群可以指定其他名字。如,想优化一个集群用于块设备，另一个用作网关，可以在同一套硬件上运行两个不同的集群，但它们要配置不同的fsid和集群名：
```
ceph-deploy --cluster {cluster-name} new {host [host]...}

如：
ceph-deploy --cluster rbdcluster new ceph-mon{1,2,3}
```

## 增加/删除监视器
用ceph-deploy增加和删除监视器很简单，只要一个命令就可以增加或删除一个或多个监视器。用ceph-deploy有一个局限性，只能在一主机上安装一个监视器

**不建议把监视器和OSD置于同一主机上**

考虑到高可用性，生产集群应该至少3个监视器。Ceph用Paxos算法，要求法定人数大多数达成一致

### 增加/删除一监视器
```
增加
ceph-deploy mon create {hostname [hostname]...}

删除
ceph-deploy mon destory {hostname [hostname]...}
```

## 密钥管理
### 收集/销毁 密钥
```
ceph-deploy gatherkeys {monitor-host}

ceph-deploy forgetkeys
```

## 增加/删除OSD
新增和拆除Ceph的OSD进程相比其他两种要多几步。OSD守护进程把数据写入磁盘和日志，所以要相应提供一OSD数据盘和日志分区路径(这是最常见的配置，但可以按需调整)
从Ceph v0.60开始，Ceph支持dm-crypt加密的硬盘，在准备OSD时可以用--dm-crypt参数告诉ceph-deploy想要使用加密功能。也可以用--dmcrypt-key-dir参数指定dm-crypt加密密钥位置。

### 列举/擦净 磁盘
```
ceph-deploy disk list {node-name [node-name]...}

ceph-deploy disk zap {osd-server-name}:{disk-name}
ceph-deploy disk zap osdserver1:sdb
```

### 准备OSD
创建集群，安装Ceph软件包，收集密钥完成后就可以准备OSD并把它们部署到OSD节点了。
```
ceph-deploy osd prepare {node-name}:{data-disk}[:{journal-disk}]
ceph-deploy osd prepare osdserver1:sdb:/dev/ssd
ceph-deploy osd prepare osdserver1.sdc:/dev/ssd
```

prepare命令只准备OSD。在大多数操作系统中国，硬盘分区创建后，不用activate命令也会自动执行activate命令也会自动执行activate阶段(通过ceph的udev规则)。

建议把日志存储于另外的驱动器以最优化性能；也可以指定一单独的驱动器用于日志，或者把日志放到OSD数据盘

### 激活OSD
```
ceph-deploy osd activate {node-name}:{data-disk-partition}[:{journal-disk-partition}]
ceph-deploy osd activate osdserver1:/dev/sdb1:/dev/ssd1
```

### 创建OSD
```
ceph-deploy osd create {node-name}:{disk}[:{path/to/journal}]
ceph-deploy osd create osdserver1:sdb:/dev/ssd1
```

## 增加/拆除元数据服务器
用ceph-deploy增加和拆除元数据服务器很简单
**必须部署至少一个元数据服务器才能使用CephFS文件系统，多个元数据服务器并行运行仍处于试验阶段，不要在生产环境运行多个元数据服务器**

### 增加一元数据服务器
```
ceph-deploy mds create {host-name}[:{daemon-name}][{host-name}[:{daemon-name}]...]
```

## 清除一主机

### PURGEDATA
如果只想清除/var/lib/ceph下的数据，并保留Ceph安装包，可以用purgedata命令：
```
ceph-deploy purgedata {hostname [hostname] ...}
```

### PURGE
清理/var/lib/ceph下的所有数据，并卸载ceph软件包：
```
ceph-deploy purge {hostname [hostname]...}
```


## 管理任务
用ceph-deploy建立一个集群后，可以把客户端管理密钥和Ceph配置文件发给其他管理员，以便让他用ceph命令管理集群
### 创建一管理主机
```
ceph-deploy admin {host-name [hostname]...}
```

### 分发配置文件
```
ceph-deploy config push {hostname [hostname]...}
```

### 检索配置文件
```
ceph-deploy config pull {hostname [hostname]...}
```






