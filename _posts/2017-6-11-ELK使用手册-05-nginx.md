# Nginx访问日志

------

**grok**
Logstash默认自带了apache标准日志的grok正则
查看logstash内置的grok
`logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns/grok`

```
COMMONAPACHELOG %{IPORHOST:clientip} %{USER:ident} %{NOTSPACE:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-)

COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}
```

对于nginx标准日志，可以发现最后多了个`$http_x_forwarded_for`变量，所以nginx标准日志的grok正则定义是

```
MAINNGINXLOG %{COMBINEDAPACHELOG} %{QS:x_forwarded_for}
```

![nginx-01.png](https://aaron-13.github.io/images/nginx-02.png)


**split**

nginx日志因为部分变量中含有空格，所以很多时候只能使用`%{QS}`正则来做分割，性能和细度都不太好。如果能自定义一个比较少见的字符作为分隔符，那么处理起来就简单多了，假设自定义的日志格式如下:

```
log_format main "$http_x_forwarded_for | $time_local | $request | $status | $body_bytes_sent | " 
				"$request_body | $content_length | $http_referer | $http_user_agent | $nuid | " 
				"$http_cookie | $remote_ddr | $hostname | $upstream_addr | $upstream_addr | $upstream_reponse_time | $request_time";
```

![nginx-02.png](https://aaron-13.github.io/images/nginx-02.png)


然后还可以针对request做更细致的切分，比如URL参数部分，很明显，URL参数中的字段，顺序是乱的，第一行，问好之后的第一个字段是sign，第二行问号之后的第一个字段是appv，所以需要将字段进行切分，取出每个字段对应的值

```
filter {
	ruby {
		init => "@kname = ['http_x_forwarded_for','time_local','request','status','body_bytes_sent','request_body','content_length','http_referer','http_user_agent','nuid','http_cookie','remote_addr','hostname','upstream_addr','upstream_response_time','request_time']"
		code => "
			new_event = LogStash::Event.new(Hash[@kname.zip(event.get('request').split(' '))])
			new_event.remove('@timestamp')
			event.append(new_event)
		"
	}
	if [request] {
		ruby {
			init => "@kname = ['method','uri','verb']"
			code => "
				new_event = LogStash::Event.new(Hash[@kname.zip(event.get('request').split(' '))])
				new_event.remove('@timestamp')
				event.append(new_event)
			"
		}
	if [uri] {
		ruby {
			init => "@kname = ['url_path','url_args']"
			code => "
				new_event = LogStash::Event.new(Hash[@kname.zip(event.get('request').split(' '))])
				new_event.remove('@timestamp')
				event.append(new_event)
			"
			}
		kv {
			prefix => "url_"
			source => "url_args"
			field_split => "& "
			remove_field => ["url_args","uri","request"]
			}
		}
	}
	mutate {
		convert => [
			"body_bytes_sent","ignore",
			"content_length","integer"
			"upstream_response_time","float",
			"request_time","float"
		]
	}
	date {
		match => ["time_local","dd/MMM/yyyy:hh:mm:ss Z"]
		locale => "en"
	}
}

```

如果url参数过多，可以不使用kv切割，或者预先定义成nested object后，改成数组形式:

```
if [uri] {
	ruby {
		init => "@kname = ['url_path','url_args']"
		code => "
			new_event = LogStash::Event.new(Hash[@kname.zip(event.get('request').split('?'))])
			new_event.remove('@timestamp')
			event.append(new_event)
		"
	}
	if [url_args] {
		ruby {
			init => "@kname = ['key','value']"
			code => "event.set('nested_args',event.get('url_args').split('&').collect{|i| Hask[@kname.zip(i.split('='))]})"
			remove_field => ["url_args","uri","request"]
		}
	}
}

```


**json format**

自定义分隔符配置写起来较为复杂，对于logstash来说，nginx日志还有另一种更简单的处理方式，就是自定义日志格式，通过手工拼写，直接输出成JSON格式

![nginx-03.png](https://aaron-13.github.io/images/nginx-03.png)

然后可采用下面的logstash配置:

```
input {
	file {
		path => "/var/log/nginx/access.log"
		codec => json
	}
}

filter {
	mutate {
		split => ["upstreamtime",","]
	}
	mutate {
		convert => ["upstreamtime","float"]
	}
}

```

采用多个mutate插件，因为upstreamtime可能有多个数值，所以先切割成数组以后，再分别转换成浮点型数值。而在mutate中，convert函数执行优先于split函数，所以分开写。


**syslog**
Ningx从1.7开始，加入了syslog支持，可以直接通过syslog发送日志。

```
access_log syslog:server=unix:/data0/rsyslog/nginx.sock locallog;
```

或者直接发送给远程logstash机器:

```
access_log syslog:server=192.168.0.2:5140,facility=local6,tag=nginx_access,severity=info logstashlog;
```

默认情况下，nginx将使用local7.info等级，nginx为标签，发送数据。采用syslog发送日志的时候，无法配置`buffer=16k`选项。



### Nginx错误日志

nginx错误日志没有统一明确的分隔符，也没有特别方便的正则模式，但通过logstash不同的插件的组合，可以轻松做到数据处理。
Nginx错误日志中，有一类数据是接收过大请求时报错，默认信息会把请求体的具体字节数记录下来。每次请求的字节数基本都是在变化的，这意味着常用的topN等聚合函数对该字段都没有明显的效果，所以，对此需要特殊处理

```
filter {
	grok {
		match => {"message" => "(?<datetime>\d\d\d\d/\d\d/\d\d \d\d:\d\d:\d\d) \[(?<errtype>\w+)\] \S+: \*\d+ (?<errmsg>[^,]+),(?<errinfo>.*)$"}
	}
	mutate {
		rename => ["host","fromhost"]
		gsub => ["errmsg","too large body: \d+ bytes","too large body"]
	}
	if [errinfo] {
		ruby {
			code => "
				new_event = LogStash::Event.new(Hash[event.get('errinfo').split(', ').map{|1| 1.split(': ')}])
				new_event.remove('@timestamp')
				event.append(new_event)
			"
		}
	}
	grok {
		match => {"request" => '"%{WORD:verb} %{URIPATH:urlpath}(?:\?%{NGX_URIPARAM:urlparam})?(?: HTTP/%{NUMBER:httpversion})"'}
		pattern_dir => ["/etc/logstash/patterns"]
		remove_field => ["message","errinfo","request"]
	}
}
```




