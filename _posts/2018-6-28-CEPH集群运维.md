# Ceph集群运维

------

[TOC]

## 操纵集群
### 用UPSTART控制CEPH
用ceph-deploy把Ceph Cuttlefish即更高版本部署到Ubuntu之后，可用基于事件的Upstart来启动，关闭Ceph节点上的守护进程。
列出Ceph作业和例程
`sudo initctl list | grep ceph`

启动所有守护进程
`sudo start ceph-all`

停止所有守护进程
`sudo stop ceph-all`

按类型启动/停止所有守护进程
```
sudo start/stop ceph-osd-all
sudo start/stop ceph-mon-all
sudo start/stop ceph-mds-all
```

启动/停止单个进程
```
sudo start/stop ceph-osd id={id}
sudo start/stop ceph-mon id={hostname}
sudo start/stop ceph-mds id={hostname}
```

### 运行CEPH
`{commandline} [options] [commands] [daemons]`

|选项|简写|描述|
|:-:|:-:|:-:|
|--verbose|-v|详细日志|
|--valgrind|N/A|用Valgrind调试|
|--allhosts|-a|在ceph.conf里配置的所有主机上执行，否则只在本机执行|
|--restart|N/A|核心转储后自动重启|
|--norestart|N/A|核心转储后不自动重启|
|--conf|-c|使用指定配置文件|

|命令|描述|
|:-:|:-:|
|start|启动守护进程|
|stop|停止守护进程|
|forcestop|暴力停止|
|killall|杀死某一类守护进程|
|cleanlogs|清理日志目录|
|cleanalllogs|清理日志目录内所有内容|

+ 通过SYSVINT机制运行CEPH
在CentOS，Redhat，Fedora和SLES发行版上可以通过sysvinit运行ceph

- 启动/停止所有守护进程
```
sudo /etc/init.d/ceph [iptions] [start|restart] [daemonType|daemonID]
sudo /etc/init.d/ceph -a start

sudo /etc/init.d/ceph [options] stop [daemonType|daemonID]
sudo /etc/init.d/ceph -a stop
```

- 启动/停止一类守护进程
```
启动本节点上某一类的所有Ceph守护进程
sudo /etc/init.d/ceph start/stop {daemon-type}
sudo /etc/init.d/ceph start/stop osd

启动费本机节点某一类的所有Ceph守护进程
sudo /etc/init.d/ceph -a start/stop {daemon-type}
sudo /etc/init.d/ceph -a start/stop osd
```

- 启动/停止单个守护进程
```
启动本节点上某个的Ceph守护进程
sudo /etc/init.d/ceph start/stop {daemon-type}.{instance}
sudo /etc/init.d/ceph start/stop osd.0

启动费本机节点某个的Ceph守护进程
sudo /etc/init.d/ceph -a start/stop {daemon-type}.{instance}
sudo /etc/init.d/ceph -a start/stop osd.0
```


## 监控集群
### 交互模式
```
ceph
ceph> health
ceph> status
ceph> quorum_status
ceph> mon_status
```

### 检查集群健康状态
```
ceph health

如果配置文件或密钥环不在默认路径下，需要指定
ceph -c /path/to/conf -k /path/to/keyring health
```

### 观察集群
```
ceph -w
```
输出信息包含：
+ 集群唯一标识符
+ 集群健康状况
+ 监视器图元版本，和监视器法定人数状态
+ OSD图元版本，和OSD状态摘要
+ 归置组图版本
+ 归置组和存储池数量
+ 器内存储的数量和对象数量的粗略统计
+ 数据总量

### 检查集群的使用情况
```
ceph df 
```

### 检查OSD状态
```
ceph osd stat
ceph osd dump
ceph osd tree
```

### 检查监视器状态
```
ceph mon stat
ceph mon dump
ceph quorum_status
```

### 检查MSD状态
```
ceph mds stat
ceph mds dump
```

### 使用管理套接字
```
ceph daemon {daemon-name}
ceph daemon {path-to-socket-file}

ceph daemon osd.0 foo
ceph daemon /var/run/ceph/ceph-osd.0.asok foo

ceph daemon {daemon-name} help
```


## 监控OSD和归置组

高性能和高可靠性要求容错方法来管理软硬件。Ceph没有单点故障，并且能在"降级"模式下继续提供服务。其数据归置引进了一个简介曾，可保证数据不会直接绑死到某个特定OSD地址，这也意味着追踪系统座位的根源要摄入归置组及底层OSD

**集群某一部分失效可能导致不能访问某个对象，但不会牵连其他对象。碰到这种问题时无需恐慌，只需要按步骤检查OSD和归置组，然后排除故障**

### 监控OSD
某OSD的状态可以是在集群内(in)或集群外(out),也可以是活着且在运行(up)或挂了且不在运行(down).
在一些情况下，集群不会返回HEALTH_OK：
+ 还没启动集群
+ 刚启动完，还没准备好，因为归置组正在被创建，OSD们正在相互创建连接
+ 刚增加或删除一个OSD
+ 刚修改完集群运行图

若一个OSD状态为down，启动它：
```
ceph -a start osd.1
```

### 归置组集
CRUSH要把归置组分配到OSD时，先查询这个存储池的副本数设置，再把归置组分配到OSD，这样就把个副本分配到了不同OSD。比如，如果存储池要求归置组有3个副本，CRUSH可能把它们分别分配到osd.1,osd.2,osd.3。考虑到设置于CRUSH运行图中的失败域，实际上CRUSH找出的伪随机位置，所以在大型集群中，很少能看到归置组被分配到了相邻的OSD。把涉及某个特定归置组副本的额一组OSD称为acting set。在一些情况下，位于acting set中的一个OSD down额或者不能为归置组内的对象提供服务，常见原因如下：
+ 增加或拆除一个OSD。然后CRUSH把那个归置组分配到了其他OSD，因此改变了Acting set的构成，并且用backfill进程启动了数据迁移
+ 一OSD down了，重启了，而现在正在恢复(recovering)
+ acting set中的一个OSD挂了，不能提供服务，另一个OSD临时接替其工作

```
获取归置组列表
ceph pg dump

根据指定归置组号查看哪些OSD位于Acting Set或Up Set里，执行：
ceph pg map {pg-num}

其结果会显示osdmap版本(eNNN)，归置组号({pg-num}), Up Set内的OSD(up[])，和ActingSet内的OSD(acting[])
osdmap eNNN pg {pg-num} -> up [0,1,2] acting [0,1,2]
```

### 节点互联
写入数据前，归置组必须处于active，而且应该是clean状态。假设一存储池的归置组有3个副本，为让Ceph确定归置组当前状态，一归置组的主OSD(即acting set内的第一个OSD)会与第二和第三OSD建立连接，并并就归置组的当前状态达成一致

### 监控归置组状态
```
ceph pg stat

ceph pg dump

ceph pg dump -o {filename} --format=json

ceph pg {poolnum}.{pg-id} query
```

#### 存储池在建中
创建存储池时，它会创建指定数量的归置组。Ceph在创建一个或多个归置组时显示creating；创建完成后，在其归置组的Acting Set里的OSD将建立互联；一旦互联完成，归置组状态应该变为active+clean，意思是Ceph客户端可以向归置组写入数据了

#### 互联建立中
Ceph为归置组建立互联时，会让存储归置组副本的OSD之间就其中的对象和元数据状态达成一致。Ceph完成互联，就意味着存储着归置组的OSD酒气当前状态达成一致。然而，互联过程的完成并不能表达各副本都有了数据的最新副本

#### 活跃
Ceph完成互联后，一归置组状态就会变成active。active表示数据以完好的保存到主归置组和副本归置组

#### 整洁
某一归置组处于clean状态时，主OSD和副本OSD已成功互联，并且没有偏离的归置组。Ceph已把归置组中的对象赋值了规定的次数

#### 已降级
当客户端向主OSD写入数据时，由主OSD负责把数据副本写入其余副本。主OSD把对象写入存储器后，在副本OSD创建完对象副本并报告给主OSD之前，主OSD会一直停留在degraded状态

归置组状态处于active+degraded状态，原因在于一OSD即使尚未持有所有对象也可以处于active状态。如果一OSD挂了，Ceph会把分配到此OSD的归置组标记为degraded；那个OSD重生后，它们必须重新互联。然而，客户端仍可以向处于degrade状态的归置组写入对象，只要它还在active状态

归置组也会被降级(degraded)，因为Ceph找不到本应存在于此归置组中的一个或多个对象，这是，不能读写找不到的对象那个，但仍能访问处于降级归置组中的其他对象

#### 恢复中
Ceph被设计为可容错，可抵御一定规模的软硬件问题。当某OSD挂了，其内的归置组会落后与别的归置组副本；此OSD重生时，归置组内容必须更新到当前状态，在此期间，OSD处于recovering状态。

Ceph提供了几个选项来均衡资源竞争，如新服务请求，恢复数据对象和恢复归置组到当前状态。
osd recovery delay start
    允许一OSD在开始恢复进程前，先重启，重建互联，甚至处理一些重放请求；
osd recovery threads
    限制恢复进程的线程数，默认为1线程
osd recovery thread timeout
    设置线程超时
osd recovery max active
    限制一OSD最多同时接受多少请求，以防它压力过大而不能正常服务
osd recovery max chunk
    限制恢复数据块尺寸，以防网路堵塞

#### 回填中
有新的OSD加入集群时，CRUSH会把现有集群内的部分归置组重新分配给它。配置型OSD立即接受重分配的归置组会使之过载，用归置组回填可使这个过程在后台开始。只要回填顺利完成，新OSD就可以对外服务了。

回填期间的几种状态：
backfill_wait
    一回填操作在等待时机，尚未开始
backfill    
    一回填操作正在进行
backfill_too_full
    需要进行回填，但因存储空间不足而不能够完成

某归置组不能回填时，其状态是incomplete

Ceph提供了多个选项来解决重分配归置组给一OSD时相关的负载问题
osd_max_backfills
    把双向的回填并发量都设置为10
osd backfill full \ ratio
    可让一OSD在接近沾满率(85%)时拒绝回填
osd backfill retry interval
    间隔重试时间
osd backfill scan min/max
    扫描间隔(64/512)

#### 被重映射

#### 发蔫

### 找出故障归置组
如果一个归置组状态不是active+clean时未必有问题，一般来说，归置组卡住时Ceph的自修复功能往往无能为力，卡转的状态细分为：
+ Unclean： 归置组有些对象的副本数未达到期望次数，应该在恢复中
+ Inactive： 归置组不能处理读写请求，因为它们在等着一个持有最新数据OSD回到up状态
+ Stale： 归置组处于一种未知状态

`ceph pg dump_stuck [unclean|inactive|stale|undersized|degraded]`

### 定位对象
要把对行啊改数据存入Ceph对象储存，一Ceph客户端必须：
+ 设置对象名
+ 指定一存储池

`ceph osd map {poolname} {objectname}`

### 练习定位一个对象
```
创建一个对象，给rados put命令指定一个对象名，一个包含数据的测试文件路径和一个存储池的名字：
rados put {obejct-name} {file-path} --pool=data
rados put test-object-1 textfile.txt --pool=data

用下列命令确认Ceph对象存储已经包含此对象
rados -p data ls

定位对象
ceph osd map {pool-name} {object-name}
ceph osd map data test-object-1

删除测试对象，用rados rm即可
rados rm test-object-1 --pool=data
```

## 用户管理

### 授权
Ceph用能力(capabilities,caps)这个属于来描述给认证用户的授权，这样才能使用监视器，OSD，和元数据服务器的功能。能力也用于限制对一存储池内的数据或某个名字空间的访问。

```
{daemon-type} 'allow {capability}' [{daemon-type} 'allow {capability}']

监视器能力： 监视器能力包括r,w,x和allow profile {cap}
    mon 'allow rwx'
    mon 'allow profile osd'

OSD能力：OSD能力包括r,w,x,class-read,class-write和profile osd。另外，OSD能力还支持存储池和命名空间的配置
    osd 'allow {capability}' [pool={poolname}] [namespace={namespace-name}]

元数据服务器能力
    mds 'allow'
```

**Ceph对象网关守护进程radosgw是Ceph存储集群的一种客户端，所以它没被表示成一种独立的Ceph存储集群守护进程类型**

### 管理用户
#### 罗列用户
```
ceph auth list
```

#### 获取用户
```
ceph auth get {TYPE.ID}
ceph auth get client.admin
ceph auth export {TYPE.ID}
```

#### 新增用户
```
ceph auth add client.john mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth get-or-create client.paul mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth get-or-create client.george mon 'allow r' osd 'allow rw pool=liverpool' -o george.keyring
ceph auth get-or-create client.ringo mon 'allow r' osd 'allow rw pool=liverpool' -o ringo.key
```

#### 修改用户能力
```
ceph auth caps USERTYPE.USERID {daemon} 'allow [r|w|x|*|...]' [pool={pool-name}] [namespace={namespace-name}]' [{daemon} 'allow [r|w|x|*|...] [pool={pool-name}] [namespace={namespace-name}]']

ceph auth get client.john
ceph auth caps client.john mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth caps client.paul mon 'allow rw' osd 'allow rwx pool=liverpool'
ceph auth caps client.brian-manager mon 'allow *' osd 'allow *'
```

#### 移除用户能力
```
ceph auth caps client.ringo mon '' osd ''
```

#### 删除用户
```
ceph auth del {TYPE}.{ID}
其中{TYPE}是client，osd，mon或mds之一，{ID}是用户名或守护进程ID
```

#### 查看用户密钥
```
将用户的身份验证密钥输出到标准输出
ceph auth primary-key {TYPE}.{ID}

{TYPE}是client，osd，mon，mds中的一种，{ID}是用户或守护进程的ID
当使用用户密钥填充客户端软件时，打印用户密钥很有用
mount -t ceph serverhost:/ mountpoint -o name=client.user,secret=`ceph auth print-key client.user`

```

#### 导入用户
```
ceph auth import -i /path/to/keyring
```

### 密钥环管理
默认路径：/etc/ceph/{$cluster.$name.keyring,$cluster.keyring,keyring,keyring.bin}

#### 创建密钥环
```
ceph-authtool --create-keyring /path/to/keyring
```

#### 将用户加入密钥环
```
ceph auth get client.admin -o /etc/ceph/ceph.client.admin.keyring

将用户导入密钥环时，可使用ceph-authtool指定目标密钥环和源密钥环
ceph-authtool /etc/ceph/ceph.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring
```

#### 创建用户
```
ceph-authtool -n client.ringo --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.keyring
```

### 命令行用法
Ceph支持用户性和密钥的下列用法：
--id | --user
    描述： Ceph用一个类型和ID(如TYPE.ID或client.admin，client.user)来标识用户，id，name和-n选项可用于指定用户名(如admin，user1,foo等)的ID部分，可以用--id指定用户并忽略类型
    ceph --id foo --keyring /path/to/keyring health
    ceph --user foo --keyring /path/to/keyring health

--name | -n
    描述： Ceph用一个类型和ID来标识用户，--name和-n选项可用于指定完整的额用户名，但必须指定用户类型(一般是client)和用户ID
    ceph --name client.foo --keyring /path/to/keyring health
    ceph -n client.foo --keyring /path/to/keyring health

--keyring
    描述： 包含一或多个用户名，密钥的密钥环路径。--secret选项提供了相同的功能。但它不能用于RADOS网关，其--secret另有用途，可以用ceph auth get-or-create获取密钥环并保存在本地，然后就可以改用其他用户而无需指定密钥环路径了
    rbd map --id foo --keyring /path/to/keyring mypool/myimage

## 数据归置概览
Ceph通过RADOS集群动态地存储，复制和重新均衡数据对象，很多不同用户因不同目的把对象存储在不同的存储池里，而他们都坐落于无数的OSD之上，所以Ceph的运营需要些数据归置计划。Ceph的数据归置计划概念主要有：
+ 存储池(Pool): Ceph在存储池内存储数据，它是对象存储的逻辑组；存储池管理者归置组数量，副本数量和存储池规则集。要往存储池里存储数据，用户必须通过认证，且权限合适，存储池可做快照
+ 归置组(Placement Group): Ceph把对象映射到归置组(PG),归置组是一逻辑对象池的片段，这些对象组团后再存储到OSD。归置组减少了各对象存入对应OSD时元数据数量，更多的归置组(如每OSD 100个)使得均衡更好
+ CRUSH图(CRUSH Map): CRUSH是重要组件，它使Ceph能伸缩自如而没有性能瓶颈，没有扩展限制，没有单点故障，它为CRUSH算法提供集群的物理拓扑，以此确定一个对象的数据及它的副本应该在哪里，怎样跨故障域存储，以提升数据安全


## 存储池
当部署时没有创建存储池，Ceph会用默认存储池存数据。存储池提供的功能：
+ 自恢复力：可以设置在不丢数据的前提下允许多少OSD失效，对多副本存储池来说，此值是一对象应达到的副本数。典型配置存储一个对象和它的一个副本(即size=2)，但可以更改副本数；对纠删编码的存储池来说，此值是编码块数(即纠删配置里的m=2)
+ 归置组： 可设置一个存储池的归置组数量。典型配置给每个OSD分配大约100个归置组，这样，不通过多计算资源就能得到较优的均衡。配置多个存储池时，药老绿道这些存储池和整个集群的归置组数量要合理
+ CRUSH规则： 当存储池里存数据的时候，与此存储池相关联的CRUSH规则集可控制CRUSH算法，并以此操纵集群内对象及其副本的赋值，可自定义存储池的CRUSH规则
+ 快照： 用ceph osd pool mksnap创建快照的时候，实际上创建了某一特定存储池的快照
+ 设置所有者： 可设置一个用户ID为一个存储池的所有者

### 列出存储池
```
ceph osd lspools
```

### 创建存储池
```
创建存储池前先看看存储池，归置组和CRUSH配置参考。最好在配置文件里重置默认归置组数量，因为默认值不理想
osd pool default pg num = 100
osd pool default pgp num = 100

要创建一个存储池，执行：
ceph osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated] [crush-ruleset-name] [expected-num-objects]
ceph osd pool create {pool-name} {pg-num} {pgp-num} erasure [erasure-code-profile] [crush-ruleset-name] [expected_num_objects]

{pool-name}
描述： 存储池名称，必须唯一
类型： string
是否必须： 必须

{pg-num}
描述： 存储池拥有的归置组总数
类型： 整数
是否必须： YES
默认值： 8

{pgp-num}
描述： 用于归置的归置组总数，此值应该与归置组总数相等，归置组分隔的情况下除外
类型： 整数
是否必须： 没指定的话读取默认值，或Ceph配置文件里的值
默认值： 8

{replicated|erasure}
描述： 存储池类型，可以是副本(保存多份对象那个副本，以便从丢失的OSD恢复)或纠删(获取类似RAID5的功能)。多副本存储池需要更多原始存储空间，但已实现所有Ceph操作，纠删存储池所需原始存储空间较少，但目前仅实现部分ceph操作
类型： string
是否必须： NO
默认值： replicated

[crush-ruleset-name]
描述：此存储池所用的CRUSH规则集没名字。指定的规则集必须存在
类型：string
是否必须： No
默认值： 对于多副本(replicated)存储池来说，器默认规则集由osd pool default crush replicated ruleset配置决定，此规则集必须存在。对于erasure-code编码的纠删码erasure存储池来说，不同的{pool-name}所使用的默认纠删码是不同的，如果它不存在的话，会显式地创建

[erasure-code-profile=profile]
描述： 仅用于纠删存储池，指定纠删码配置框架，此配置必须已由osd erasure-code-profile set定义
类型： String
是否必须： No

创建存储池时，要设置一个合理的归置组数量（如 100 ）。也要考虑到每 OSD 的归置组总数，因为归置组很耗计算资源，所以很多存储池和很多归置组（如 50 个存储池，各包含 100 归置组）会导致性能下降。收益递减点取决于 OSD 主机的强大

[expected-num-objects]
描述：为这个存储池预估的对象数，设置此值(要同时把filestore merge threshold设置为负数)后，在创建存储池时就会拆分PG文件夹，以免运行时拆分文件夹导致延时增大
类型：integer
是否必须：no
默认值： 0，创建存储池时不拆分目录
```

### 设置存储池配额
```
存储池配额可设置最大字节数，和/或每个存储池最大对象数
ceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}]
ceph osd pool set-quota data max_objects 10000
```

### 删除存储池
```
ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]
```

### 重命名存储池
```
ceph osd pool rename {current-pool-name} {new-pool-name}
如果重命名了一个存储池，且认证用户有每存储池能力，必须用新存储池名字更新用户的能力(caps)
```

### 查看存储池统计信息
```
rados df
```

### 拍下存储池快照
```
ceph osd pool mksnap {pool-name} {snap-name}
```

### 调整存储池选项值
```
ceph osd pool set {pool-name} {key} {value}
```

### 获取存储池选项值
```
ceph osd pool get {pool-name} {key}
```

### 设置对象副本数
```
ceph osd pool set {poolname} size {num-replicas}
ceph osd pool set data size/min_size 3
```

### 获取对象副本数
```
ceph osd dump |grep 'replicated size'
```

## 纠删码
### 创建样板纠删码存储池
```
ceph osd pool create ecpool 12 12 erasure
echo ABCDEFGHI | rados - pool ecpool put NYAN - 
rados --pool ecpool get NYAN -
```

### 纠删码配置
默认的纠删码配置文件可以支持丢失单个OSD。它相当于一个大小为2的复制池，但需要1.5TB而不是2TB才能存储1TB的数据。默认配置文件显示为：
```
ceph osd erasure-code-profile get default
directory = .libs 
k = 2 
m = 1 
plugin = jerasure 
ruleset-failure-domain = host 
technique = reed_sol_van
```

配置文件中最重要的参数是K,M和ruleset-failure-domain，因为它们定义了存储开销和数据持久性。例如，如果需要的体系接口必须承受两个机架的丢失，并且存储开销为40%，则可以定义以下配置文件：
```
ceph osd erasure-code-profile set myprofile \
    k = 3 \
    m = 2 \
    ruleset-failure-domain = rack
ceph osd pool crete ecpool 12 12 ersure myprofile
echo ABCDEFGHI | rados --pool ecpool put NYAN -
rados --pool ecpool get NYAN - 
```

### 纠删码存储池和缓存分级
擦除编码比复制池需要更多的资源，并且缺少某些功能，如部分写入。为克服这些限制，建议在纠删编码之前设置缓存层
```
如果池的热存储由快速存储构成
ceph osd tier add ecpool host-storage
ceph osd tier cache-mode host-storage writeback
ceph osd tier set-overlay ecpool host-storage
```

[http://docs.ceph.org.cn/rados/operations/erasure-code/](http://docs.ceph.org.cn/rados/operations/erasure-code/)

### 内容列表

#### 纠删码配置
纠删码由配置定义，在创建纠删码存储池及其相关的CRUSH规则集时用到
创建Ceph集群时初始化的，名为default的纠删码配置可提供与量副本相同的冗余水平，却能节省25%的磁盘空间。在此配置中k=2和m=1，其含义为数据分布于3个OSD(k+m==3)且允许一个失效

为了在不增加原始存储空间需求的前提下提升冗余性，可新建配置。如，一个k=10且m=4的配置可容忍4个OSD失效，他会把以对象分布到14个OSD上。此对象先被分割成10块，并计算出4个用于恢复的编码块；这样，原始空间仅多占用10%就可容忍4个OSD同时失效，且不丢失数据

##### OSD ERASURE-CODE-PROFILE SET
```
新建纠删码配置
ceph osd erasure-code-profile set {name} \
    [{directory=directory}] \
    [{plugin=plugin}] \
    [{key=value} ... ] \
    [--force]
```

###### OSD ERASURE-CODE-PROFILE RM
```
删除纠删码配置
ceph osd erasure-code-profile rm {name}
```

##### OSD ERASURE-CODE-PROFILE GET
```
查看纠删码
ceph osd erasure-code-profile get {name}
```

##### OSD ERASURE-CODE-PROFILE LS
```
列出所有纠删码配置
ceph osd erasure-code-profile ls
```

#### JERASURE纠删码插件
jerasure插件是最通用，最灵活的插件，也是Ceph纠删码存储池的默认插件
jerasure插件封装了Jerasure库

##### 创建JERASURE配置
```
ceph osd erasure-code-profile set {name} \
    plugin=jerasure \
    k={data-chunks} \
    m={coding-chunks} \
    technique={reed_sol_van|reed_sol_r6_op|cauchy_orig|cauchy_good|liberation|blaum_roth|liber8tion} \
    [ruleset-failure-domain={bucket-type}] \
    [directory={directory}] \
    [--force]
```

[http://docs.ceph.org.cn/rados/operations/erasure-code-jerasure/](http://docs.ceph.org.cn/rados/operations/erasure-code-jerasure/)

#### ISA纠删码插件
isa插件封装了ISA库，只能运行在Intel处理器上
##### 创建ISA配置
```
ceph osd erasure-code-profile set {name} \
    plugin=isa \
    technique={reed_sol_van|cauchy} \
    [k={data-chunks}] \
    [m={coding-chunks}] \
    [ruleset-root={root}] \
    [ruleset-failure-domain={bucket-type}] \
    [directory={directory}] \
    [--force]
```
[http://docs.ceph.org.cn/rados/operations/erasure-code-isa/](http://docs.ceph.org.cn/rados/operations/erasure-code-isa/)

#### 局部自修复纠删码插件
用jerasure插件时，纠删码编码的对象存储在多个OSD上，丢失一个OSD的恢复过程虚度去所有其他的OSD。lrc纠删码插件创建的是局部校验码，这样只需较少的OSD即可恢复。比如lrc的配置为k=8，m=4且l=4，它将为每四个OSD创建额外的校验块，当一个OSD丢失时，它只需4个OSD即可恢复，而不需要11个

##### 纠删码配置实例
降低主机间的恢复带宽
虽然所有主机都接入同一交换机时，这不会是诱人的用法，但是带宽利用率确实降低了
```
ceph osd erasure-code-profile set LRCprofile \
    plugin=lrc \
    k=4 m=2 l=3 \
    ruleset-failure-domaim=host
ceph osd pool create lrcpool 12 12 erasure LRCprofile
```

降低机架间的恢复带宽
在Firefly版中，只有主OSD与丢失块位于统一机架时所需带宽才能降低
```
ceph osd erasure-code-profile set LRCprofile \
    plugin=lrc \
    k=4 m=2 l=3 \
    ruleset-locality=rack \
    ruleset-failure-domain=host
ceph osd pool create lrcpool 12 12 erasure LRCprofile
```

[http://docs.ceph.org.cn/rados/operations/erasure-code-lrc/](http://docs.ceph.org.cn/rados/operations/erasure-code-lrc/)

#### SHEC纠删码插件
SHEC插件封装了multiple SHEC库，与Reed Solomon编码相比，它能使Ceph更高效地恢复数据
##### 创建SHEC配置
```
ceph osd erasure-code-profile set {name} \
    plugin=shec \
    [k={data-chunks}] \
    [m={coding-chunks}] \
    [c={durability-estimator}] \
    [ruleset-root={root}] \
    [ruleset-failure-domain={bucket-type}] \
    [directory={directory}] \
    [--force]
```
[http://docs.ceph.org.cn/rados/operations/erasure-code-shec/](http://docs.ceph.org.cn/rados/operations/erasure-code-shec/)

## 分级缓存
分级存储可提升后端存储内某些(热点)数据的I/O性能。分级缓存需创建一个由告诉而昂贵存储设备组成的存储池，作为缓存层，以及一个相对低速、廉价设备组成的后端存储池，作为经济存储层。Ceph的对象处理器决定往哪里存储对象，分级代理决定何时把缓存内的对象刷回后端存储层；所以缓存层和后端存储层对Ceph客户端来说是完全透明的。

缓存层代理自动处理缓存层和后端存储之间的数据迁移。然而，管理员仍可干预此迁移规则，主要有两种场景：
+ 回写模式：管理员把缓存层配置为writeback模式时，Ceph客户端会把数据写入到缓存层，并收到缓存层发来的ACK；写入缓存层的数据会被迁移到存储层，然后从缓存层刷掉。直观地看，缓存层位于后端存储层的前面，当Ceph客户端要读取的数据位于存储层时，缓存层代理会把这些数据迁移到缓存层，然后再发往Ceph客户端。从此，Ceph客户端与缓存层进行I/O操作，直到数据不再被读写。此模式对于一边数据来说较理想(如照片，视频编辑，事务数据等)
+ 只读模式：管理员把缓存层配置为readonly模式，Ceph直接把数据写入后端。读取时，Ceph把相应对象从后端复制到缓存层，根据已定义策略，脏对象会被缓存层剔除。此模式适合不变数据(如图片，视频，DNA数据，X-Ray照片等)，因为从缓存层读取的数据肯能包含过期数据，即一致性差。对易变数据不要用readonly模式

正因为所有Ceph客户端都能用缓存层，所以才有提升块设备，Ceph对象存储，Ceph文件系统和原生绑定的I/O性能的潜力

### 配置存储池
要设置缓存层，需要两个存储池，一个作为后端存储，一个作为缓存

配置后端存储池
设置后端存储池通常会遇到两种场景：
+ 标准存储： 此时，Ceph存储集群内的存储池保存了一对象的多个副本
+ 纠删存储池： 此时，存储池用纠删码高效地存储数据，性能稍有损失

在标准存储场景中，可以用CRUSH规则集来标识失败域(如OSD，主机，机箱，机架，排等)。当规则集所涉及的所有驱动器规格，速度(转速和吞吐量)和类型相同时，OSD守护进程运行得最优。

在纠删码编码场景中，创建存储池时制定好参数就会自动生成合适的规则集

配置缓存吃
缓存存储池的设置步骤大致与标准存储情景相同，但仍有不同：缓存层所用的驱动器通常都是高性能的，且安装在专用服务器上，有自己的规则集。指定规则集时，要考虑到装有高性能驱动器的主机，并忽略没有的主机

### 创建缓存层

```
设置一缓存层许吧缓存存储层挂着到后端存储池上
ceph osd tier add {storagepool} {cachepool}
如：ceph osd tier add cold-storage host-storage

设置缓存模式
ceph osd tier cache-mode {cachepool} {cachepool}
如： ceph osd tier cache-mode host-storage writeback

缓存层盖在后端存储曾之上，所以要多一步：必须把所有客户端流量从存储池迁移到缓存存储池
ceph osd tier set-overlay {storagepool} {cachepool}
如： ceph osd tier set-overlay cold-storage hot-storage
```

### 配置缓存层

```
缓存层支持几个配置选项，可按下列语法配置：
ceph osd pool set {cachepool} {key} {value}

目标尺寸和类型
生产环境下，缓存层的hit_set_type还只能用Bloom过滤器：
ceph osd pool set {cachepool} hit_set_type bloom
如： ceph osd pool set hot-storage hit_set_type bloom

hit_set_count和hit_set_period选项分别定义了HitSet覆盖的时间区间，以及保留多少个这样的HitSet
ceph osd pool set {cachepool} hit_set_count 1
ceph osd pool set {cachepool} hit_set_period 3600
ceph osd pool set {cachepool} target_max_bytes 1000000000000

保留一段时间以来的访问记录，这样Ceph就能判断一客户端在一段时间内访问了某对象一次，还是多次(存活期与热度)
min_read_recency_for_promote定义了在处理一个对象的读操作时检查多少个HitSet，检查结果将用于决定是否异步地提升对象。它的取值应该在0和hit_set_count之间，如果设置为0，对象会一直被提升，如果设置为1，就只检查当前HitSet，如果此对象在当前HitSet里就提升它，否则就不提升；设置其他值，就要挨个检查此数量的历史HitSet。如果此对象出现在min_read_recency_for_promote个HitSet里的任意一个，那就提升它。

还有一个相似的参数用于配置写操作，它是min_write_recency_for_promote
ceph osd pool set {cachepool} min_read_recency_for_promote 1
ceph osd pool set {cachepool} min_write_recency_for_promete 1

统计时间越长，min_read_recncy_for_promote或min_write_recency_for_promote取值越高，ceph-osd进程消耗的内存越多，特别是代理正忙着刷回或赶出对象时，此时，所有hit_set_count个HitSet都要载入内存

缓存空间消长
缓存分层代理有两个主要功能：
+ 刷回： 代理找出修改过(或脏)的对象，并把它们转发给存储池做长期存储
+ 赶出： 代理找出未修改(或干净)的对象，并把最近为用过的赶出缓存

相对空间消长
缓存分层代理可根据缓存存储池相对大小刷回或赶出对象。当缓存池包含的已修改(或脏)对象达到一定比例时，缓存封层代理就把它们刷回到存储池。
ceph osd pool set {cachepool} cache_target_dirty_ratio {0. 0.. 1.0}

当脏对象达到其容量的一定比例时，要更快地刷回脏对象。用下列命令设置cache_target_dirty_high_ratio:
ceph osd pool set {cachepool} cache_target_dirty_high_ratio {0. 0.. 1.0}

当缓存池利用率达到总容量的一定比例时，缓存分层代理会赶出部分对象以维持空闲空间。执行此命令设置cache_target_full_ratio:
ceph osd pool set {cachepool} cache_target_full_ratio {0. 0.. 1.0}

绝对空间消长
缓存分层代理可根据总字节数或对象数量来刷回或赶出对象，用下列命令可指定指定最大字节数
ceph osd pool set {cachepool} target_max_bytes {#bytes}

指定缓存对象的最大数量
ceph osd pool set hot-storage target_max_objects {#objects}

缓存时长
ceph osd pool set {cachepool} cache_min_flush_age {#seconds}

指定某对象在缓存层至少放置多长时间才能被赶出
ceph osd pool {cache-tier} cache_min_evict_age {#seconds}
```

### 拆除缓存层
```
拆除只读缓存
1.把缓存模式改为none即可禁用
ceph osd tier cache-mode {cachepool} none

2.去除后端存储池的缓存池
ceph osd tier remove {storagepool} {cachepool}

拆除回写缓存
1.把缓存模式改为forward，这样新的和更新过的对象直接刷回到后端存储池
ceph osd tier cache-mode {cachepool} forward

2.确保缓存池已刷回，可能要等数分钟
rados -p {cachepool} ls

3.去除此盖子，这样客户端就不会被指定缓存
ceph osd tier remove-overlay {storagetier}

4.最后端存储池剥离缓存层存储池
ceph osd pool remove {storagepool} {cachepool}
```

## 归置组

### 预定义PG_NUM
用此命令创建存储池时：
`ceph osdd pool create {pool-name} pg_num`

确定pg_num取值是强制的，因为不能自动计算，下面是几个常用的值：
+ 少于5个OSD是可把PG_NUM设为128
+ OSD数量在5到10个时，可把pg_num设置为512
+ OSD数量在10-50个时，可设置为4096个
+ OSD大于50个时，要理解权衡方法，以及如何自己计算pg_num的值
+ 自己计算pg_num取值可借助pgcalc工具

### 归置组如何使用
存储池内的归置组(PG)把对象汇聚子安一起，因为跟踪每一个对象的位置及其元数据需要大量计算--即一个拥有数百万的系统，不可能在对象这一级追踪位置

Ceph客户端会计算某一对象应该位于哪个归置组它是这样实现的，先给对象ID做哈希操作，然后再根据指定存储池里的PG数量，存储池ID做一个运算。

### 归置组权衡

### 获取归置组数量
`ceph osd pool get {pool-name} pg_num`

### 获取归置组统计信息
`ceph pg dump [--format {pg_num}]`

### 获取卡住的归置组统计信息
```
ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format <format>] [-t|--threshold <seconds>]
    inactive(不活跃)归置组不能处理读写，因为它们在等待一个有最新数据的OSD复活且进入集群
    Unclean(不干净)归置组含有复制数未达到期望数量的对象，它们应该在恢复中
    Stale(不新鲜)归置组处于未知状态，存储它们的OSD有段时间没向监视器报告了(由mon_osd_report_timeout配置)
```

### 获取一归置组运行图
```
ceph pg map {pg-id}
```

### 获取一PG的统计信息
```
ceph pg scrub {pg-id}
```

### 恢复丢失的
如果集群丢了一或多个对象，而且必须放弃搜索这些数据，就要把未找到的对象标记为丢失(lost)
当前只支持revert选项，它使得回滚到对象的前一个版本或完全忽略它。要把unfound对象标记为lost，执行命令
`ceph pg {pg-id} mark_unfound_lost revert|delete`


### 归置组状态
集群状态检查(ceph -s/-w),Ceph会报告归置组状态。一个归置组有一个到多个状态，其最优状态为active+clean

+ Creating： Ceph仍在创建归置组
+ Active：Ceph可处理到归置组的请求
+ Clean：Ceph把归置组内的对象复制了规定次数
+ Down：包含必备数据的副本挂了，所以归置组离线
+ Replay：某OSD崩溃后，归置组在等待客户端重放操作
+ Splitting：Ceph正在把一个归置组分割为多个
+ Scrubbing：Ceph正在检查归置组的一致性
+ Degraded：归置组内的对象还没复制到规定次数
+ Inconsistent：Ceph检测到归置组内一或多个副本件不一致
+ Peering：归置组正在互联
+ Repair：Ceph正在检查归置组，并试图修复发现的不一致
+ Recovering：Ceph正在迁移/同步对象及其副本
+ Backfill：Ceph正在扫描并同步整个归置组的内容，而不是根据日志推算哪些最新操作需要同步
+ Wait-backfill：归置组正在排队，等候回填
+ Backfill-toofull：一回填操作在等待，因为目标OSD使用率超过沾满率
+ Incomplete：Ceph探测到某一归置组可能丢失了写入信息，或者没有健康的副本
+ Stale：归置组处于一种未知状态---从归置组运行图并更起就没再收到它的更新
+ Remapped：归置组被临时映射到了另一组OSD，它们不是CRUSH算法指定的
+ Undersized：此归置组的副本数小于配置的存储池副本水平
+ Peered：此归置组已互联，但是不能向客户端提供服务，因为其副本数没达到本存储池的配置值(min_size参数)。在此状态可进行恢复，最终达到min_size

### 归置组术语解释
[http://docs.ceph.org.cn/rados/operations/pg-concepts/](http://docs.ceph.org.cn/rados/operations/pg-concepts/)

## CRUSH图
[http://docs.ceph.org.cn/rados/operations/crush-map/](http://docs.ceph.org.cn/rados/operations/crush-map/)

CRUSH算法通过计算数据存储位置来确定如何存储和检索。CRUSH授权Ceph客户端直接连接OSD，而非通过一个中央服务器或经纪人。数据存储，检索算法的使用，使Ceph避免了单点故障，性能瓶颈，和伸缩的物理限制。
CRUSH需要一张集群的地图，且使用CRUSH把数据伪随机的存储，检索于真个集群的OSD里。
CRUSH图包含源头，OSD列表，把设备汇聚为物理位置的"桶"的列表，和只是CRUSH如何复制存储池里的数据的规则列表。由于对所有安装底层物理组织的表达，CRUSH能模型化，并因此定位到潜在的相关失败设备，典型的源头有物理距离，共享电源和共享网络，把这些信息编码到集群运行图里，CRUSH归置策略可把对象副本分离到不同的失败域，却仍能保持期望的分布。

当写好配置文件，用ceph-deploy部署ceph后，它生成了一个默认的CRUSH图。


### CRUSH位置
用CRUSH图层次结构所表示的OSD位置被称为"crush位置"，用键值对列表来表示。例如，一OSD位于某特定行，机柜，机架和主机，且是CRUSH图里名为default树的一部分，那么其crush位置可表示为：
```
root=default row=a rack=a2 chassis=a2a host=a2a1

注：
1.注意键(关键词)与顺序无关
2.键名( = 左边)必须是CRUSH内的合法type，默认情况下，它包含root，datacenter，room，row，pod，pdu，rack，chassis和host，但这些类型可修改CRUSH图任意定义
3.并非所有键都需指定，例如，默认情况下Ceph会自动把ceph-osd守护进程的位置设为root=default host=HOSTNAME(即hostname -s)

```

CEPH-CRUSH-LOCATION挂钩
ceph-crush-location工具可为某守护进程生成默认CRUSH位置字符串，此位置依次基于:
1.ceph.conf里的TYPE crush location，例如这是OSD守护进程的：osd crush location
2.ceph.conf里的crush location；
3.默认的root=default host=HOSTNAME,其中主机名由hostname -s获取

完全手动管理CRUSH图也是可能的额，在配置中把挂钩关掉即可
`osd crush update on start = false`

定制位置挂钩
定制化位置挂钩可代替通用挂钩，用于控制OSD在分级结构中的位置(启动时，各OSD都确认它们的位置正确无误)
`osd crush location hook = /path/to/script`

此挂钩应该接受几个参数并向标准输出打印一行CRUSH位置描述
`ceph-crush-location --cluster CLUSTER --id ID --type TYPE`

### 编辑CRUSH图
要编辑现有的CRUSH图：
1.获取CRUSH图
2.反编译CRUSH图
3.至少编辑一个设备，桶，规则
4.重编译CRUSH图
5.注入CRUSH图

获取CRUSH图
`ceph osd getcrushmap -o {compiled-crushmap-filename}`

反编译CRUSH图
`crushtool -d {compiled-crushmap-filename} -o {decompiled-crushmap-filename}`

编译CRUSH图
`crushtool -c {decompiled-crush-map-filename} -o {compiled-crush-map-filename}`

编译CRUSH图
`crushtool -c {decompiled-crush-map-filename} -o {compiled-crush-map-filename}`

注入CRUSH图
`ceph osd setcrushmap -i {compiled-crushmap-filename}`

### CRUSH图参数
CRUSH图主要有4个主要段落：
1.设备： 由任意对象存储设备组成，即对应一个ceph-osd进程的存储器。Ceph配置文件里的每一个OSD都应该有一个设备
2.桶类型：定义了CRUSH分级结构里要用的桶类型(types),桶由逐级汇聚的存储位置及其权重组成
3.桶例程：定义了桶类型后，还必须声明主机的桶类型，以及规划的其他故障域
4.规则：由选择桶的方法组成

Ceph部署工具生成了默认CRUSH运行图，它列出了你定义在Ceph配置以文件中的OSD设备，并把配置文件[osd]端定义的各OSD主机声明为桶。为了保证数据安全和可用，应该创建自己的CRUSH图，以反映出自己集群的故障域

**生成的CRUSH图没有考虑大粒度故障域，所以修改CRUSH图时要考虑上，像机柜，行，数据中心**

CRUSH图之设备
为了把归置组映射到OSD，CRUSH图需要OSD列表(即配置文件所定义的OSD守护进程名称)，所以它们首先出现在CRUSH图里。要在CRUSH图里声明一个设备，在设备列表后面新建一行，输入device，之后是唯一的数字ID，之后是相应的ceph-osd守护进程例程名字
`device {num} {osd.name}`
一般来说，一个OSD映射到一个单独的硬盘或RAID

CRUSH图之桶类型
CRUSH图里的第二个列表定义了bucket(桶)类型，桶简化了节点和叶子层次。节点(或非叶子)桶在分级结构里一般表示物理位置，节点汇聚了其他节点或叶子，叶桶表示ceph-osd守护进程及其对应的存储媒体
**CRUSH中用到的bucket意思是分级结构中的一个节点，也就是一个位置或一部分硬件。但是在RADOS网关接口的术语中，它又是不同的概念**
要往CRUSH图中增加一种bucket类型，在现有桶类型列表下方新增一行，输入type，之后是唯一数字ID和一个桶名。按惯例，会有一个叶子桶为type 0，然而你可以指定任何名字(如osd，disk，drive，storage等)
`type {num} {bucket-name}`

CRUSH图之桶层次
CRUSH算法根据各设备的权重，大致统一的概率把数据分布到存储设备中。CRUSH根据定义的集群运行图分布对象及其副本，CRUSH图表达了可用存储设备以及包含它们的逻辑单元。
要把归置组映射到跨故障域的OSD，一个CRUSH图需定义一系列分级桶类型(即现有CRUSH图的#type下)。创建桶分级结构的目的是按故障域隔离叶节点，向主机，几项，机柜，电力分配单元，集群，行，房间，和数据中心。除了表示叶节点的OSD，其他分级结构都是任意的，可按需定义。
建议CRUSH图内的命名符合公司的硬件命名规则，并且采用反应物理硬件的例程名。良好的命名可简化集群管理和故障排除。
**编号较高的rack桶类型汇聚编号较低的host桶类型**

声明一个桶例程时，必须指定其类型，唯一名称(字符串)，唯一负整数ID(可选)，指定和各条目总容量/能力相关的权重，指定桶算法(通常是straw)，和哈希(通常是0，表示哈希算法rjenkins1)。一个桶可以包含一到多条，这些条目可以由节点桶或叶子组成，它们可以有个权重用来反应条目的相对权重
```
[bucket-type] [bucket-name] {
    id [a unique negative numeric ID]
    weight [the relative capacity/capability of the items]
    alg [the bucket type: uniform|list|tree|straw]
    hash [the hash type: 0 by default]
    item [item-name] weight [weight]
}

host node1 {
    id -1
    alg straw
    hash 0
    item osd.0 weight 1.00
    item osd.1 weight 1.00
}
```

### 主亲和性
```
主亲和性默认为1(此OSD可作为主OSD)，此值合法范围为0-1，其中0意为此OSD不能用作主的，1意为可作用主的；此权重小于1时，CRUSH选择主OSD时选中它的可能性低

ceph osd primary-affinityy <osd-id> <weight>
```

### 给存储池指定OSD
```
ceph osd pool set <poolname> crush_ruleset 4
```

### 增加/移动OSD
```
ceph osd crush set {id} {name} {weight} pool={pool-name} [{bucket-type}={bucket-name} ...]

v0.56版本
ceph osd crush set {id-orname} {weight} root={pool-name} [{bucket-type}={bucket-name} ...]
```

### 删除OSD
`ceph osd crush remove {name}`

### 增加桶
`ceph osd crush add-bucket {bucket-name} {bucket-type}`

### 移动桶
`ceph osd crush move {bucket-name} {bucket-type}={bucket-name}, [...]`

### 删除桶
`ceph osd crush remove {bucket-name}`

### 可调选项
...


## 增加/删除OSD
### 增加OSD
Ceph允许在运行时增加OSD。在Ceph里，一个OSD一般是一个ceph-osd守护进程，它运行在硬盘之上，如果有多个硬盘，可以给每个硬盘启动一个ceph-osd守护进程。
通常，应该监控集群容量，看是否达到了容量上限，因为达到了它的near full比率后，要增加一或多个OSD来扩容

**不要等空间满了再增加OSD，空间利用率达到near full比率后，OSD失败可能导致集群空间沾满**

部署硬件
安装必要软件
增加OSD(手动)
    此过程要设置一个ceph-osd守护进程，让它使用一个硬盘，且让集群把数据发布到OSD。如果一台主机有多个硬盘，可重复此过程，把每个硬盘配置为一个OSD。
    要添加OSD，要依次创建数据目录，把硬盘挂载到目录，把OSD加入集群，然后把它加入CRUSH图。
    往CRUSH图里添加OSD时建议设置权重，硬盘容量每年增长40%，所以较新的OSD主机拥有更大的空间(即它们可以有更大的权重)
**Ceph喜欢统一的硬件，与存储池无关。如果要新增容量不一的驱动器，还需调整它们的鄂全忠。但为实现最佳性能，CRUSH的分级结构最好按类型，容量定义**

1.创建OSD。如果未指定UUID，OSD启动时会自动生成一个。下列命令会输出OSD号，后续步骤会用到
`ceph osd create [{uuid} [{id}]]`
如果指定了可选参数{id}，那么它将作为OSD id。要注意，如果此数字已使用，此命令会出错
**一般来说，不建议指定{id}。因为ID是按照数组分配的，跳过一些依然会浪费内存。若未指定{id},将用最小可用数字**

2.在新OSD主机上创建默认目录
```
ssh {new-osd-host}
sudo mkdir /var/lib/ceph/osd/ceph-{osd-number}
```

3.如果准备用于OSD的是单独的而非系统盘，先把它挂载到刚创建的目录下：
```
ssh {new-osd-host}
sudo mkfs -t {fstype} /dev/{drive}
sudo mount -o user_xattr /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number}
```

4.初始化OSD数据目录
```
ssh {new-osd-host}
ceph-osd -i {osd-num} --mkfs --mkkey

运行ceph-osd时，目录必须是空的
```

5.注册OSD认证密钥，ceph-{osd-num}路径里的ceph值应该 是$cluster-$id，如果你的集群名字不是ceph，那就用改过的名字
```
ceph auth add osd.{osd-num} osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-{osd-num}
```

6.把OSDJ加入CRUSH图，这样它才开始收数据。用ceph osd crush add命令把OSD加入CRUSH分级结构的合适位置。如果指定了不止一个桶，此命令会把它加入你所指定桶中最具体的一个，并且把此桶挪到指定的其他桶之内。
```
ceph osd crush add {id-or-name} {weight} [{bucket-type}={bucket-name} ...]
```

启动OSD
把OSD加入到Ceph后，OSD就在配置里了。然而它还没运行，现在的状态为down且out，必须先启动osd才能收集数据。
`sudo /etc/init.d/ceph start osd.{osd-num}`

### 删除OSD(手动)
在Ceph里，一个OSD通常是一台主机上的一个ceph-osd守护进程，它运行在一个硬盘之上。如果一台主机上有多个数据盘，要挨个删除其对应的ceph-osd。通常，操作前应该检查集群容量，看是否快到达上限，确保删除OSD后不会使集群打到near full比率

把OSD踢出集群
```
ceph osd out {osd-num}
```

观察数据迁移
```
ceph -w
```
归置组状态从active+clean变为active，some degraded objects，迁移完成后最终回到active+clean状态

停止OSD
```
ssh {osd-host}
sudo /etc/init.d/ceph stop osd.{osd-num}
```

删除OSD
此步骤依次把一个OSD移除集群CRUSH图，删除认证密钥，删除OSD图条目，删除ceph.conf条目。如果主机有多个硬盘，每个硬盘对应的OSD都要重复此步骤
1.删除CRUSH图的对应OSD条目
```
ceph osd crush remove {name}
```

2.删除OSD认证密钥
```
ceph auth del osd.{osd-num}
ceph-{osd-num}路径里的ceph值是$cluster-$id,如果集群名字不是ceph，要更改
```

3.删除OSD
```
ceph osd rm {osd-num}
```

4.登录到保存ceph.conf主拷贝的主机
```
ssh {admin-host}
cd /etc/ceph
vim ceph.conf
```

5.从ceph.conf配置文件里删除对应条目
```
[osd.1]
    host = {hostname}
```

6.从保存ceph.conf主拷贝的主机，把更新过得ceph.conf拷贝到集群其他主机的/etc/ceph目录下


## 增加/删除监视器
[http://docs.ceph.org.cn/rados/operations/add-or-rm-mons/](http://docs.ceph.org.cn/rados/operations/add-or-rm-mons/)

### 增加监视器
Ceph监视器是轻量级进程，它维护着集群运行图的主副本。一个集群可以只有一个监视器，生产环境推荐3个监视器。Ceph使用Paxos算法的一种变种对各种图，以及其他对集群来说至关重要的信息达成共识。由于Paxos算法天生要求大部分监视器在运行，以形成法定人数(并因此达成共识)

部署硬件
安装必要软件
增加监视器(手动)
    本步骤创建ceph-mon数据目录，获取监视器运行图和监视器密钥环，增加一个ceph-mon守护进程。如果这导致只有2个监视器守护进程，可以重演次步骤来增加一或多个监视器，直到你拥有足够多ceph-mon达到法定人数

1.在新监视器主机上创建默认目录
```
ssh {new-mon-host}
sudo mkdir /var/lib/ceph/mon/ceph-{mon-id}
```

2.创建临时目录{tmp},用以保存此过程中用到的文件，此目录要不同于前面步骤创建的监视器数据目录，且完成后可删除
```
mdkir {tmp}
```

3.获取监视器密钥环，{tmp}是密钥环文件保存路径，{filename}是包含密钥的文件名
```
ceph auth get mon. -o {tmp}/{key-filename}
```

4.获取监视器运行图，{tmp}是获取到的监视器运行图，{filename}是包含监视器运行图的文件名
```
ceph mon getmap -o {tmp}/{map-filename}
```

5.准备第一步创建的监视器数据目录，必须指定监视器运行图路径，这样才能获取到监视器法定人数和他们fsid的信息，还要指定监视器密钥环路径
```
sudo ceph-mon -i {mon-id} --mkfs --monmap {tmp}/{map-filename} --keyring {tmp}/{key-filename}
```

6.启动新监视器，它会自动加入机器。守护进程需知道绑定到哪个地址，通过--public-addr [ip:port]或在ceph.conf里的相应段设置mon addr可以指定
```
ceph-mon -i {mon-id} --public-addr {ip:port}
```

### 删除监视器
1.停止监视器
```
service ceph -a stop mon.{mon-id}
```

2.从集群中删除监视器
```
ceph mon remove {mon-id}
```

从不健康集群中删除监视器
...

### 更改监视器的IP地址
**现有监视器不应该更改其IP地址**

Ceph客户端及其它Ceph守护进程用ceph.conf发现监视器，而，监视器之间用监视器运行图发现对方，而非ceph.conf。

一致性要求
监视器发现集群内其他监视器时总是参照monmap的本地副本，用monmap而非ceph.conf可避免因配置错误而损坏集群。这因为监视器用monmaps互相发现，且共享客户端和其他Ceph守护进程间，所以monmap给监视器提供了苛刻的一致性保证。

苛刻的一致性要求也是用与monmap的更新，因为任何有关监视器的更新，monmap的更改都通过名为Paxos的分布式一致性算法运行。为保证法定人数里的所有监视器都持有同版本monmap，所有监视器都要赞成monmap的每一次更新，向增加，删除监视器。monmap的更新是增量的，这样监视器都有最近商定的额版本以及一系列之前版本，这样可使一个有较老monmap的监视器赶上集群当前的状态

如果监视器通过ceph配置文件而非monmap相互发现，就会引进额外风险，因为Ceph配置文件不会自动更新和发布。

...


## 控制命令
### 监视器命令
```
ceph [-m monhost] {command}

ceph {subsystem} {command}
```

### 系统命令
```
集群状态
ceph -s
ceph status

集群状态的运行摘要，及主要事件
ceph -w

显示监视器法定人数状态，包括哪些监视器参与者，哪个是首领
ceph quorum_status

查询单个监视器状态，包括法定人数里
ceph [-m monhost] mon_status
```

### 认证子系统
```
添加一个OSD的密钥环
ceph auth add {osd} {--in-file|-i} {path-to-osd-keyring}

列出集群的密钥及其能力
ceph auth list
```

### 归置组子系统
```
显示归置组的统计信息
ceph pg dump [--format {format}]

显示卡在某状态的所有归置组
ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format {format}] [-t|--threshold {seconds}]

删除"丢失"对象，或者恢复到其先前状态，可以是前一版本，或如果刚创建就删除
ceph pg {pgid} mark_unfound_lost revert|delete
```

### OSD子系统
```
查询OSD子系统状态
ceph osd stat

把最新的OSD运行图拷贝到一个文件
ceph osd getmap -o file

从最新OSD运行图拷出CRUSH图
ceph osd getcrushmap -o file

前述功能等价于：
ceph osd getmap -o /tmp/osdmap
osdmaptool /tmp/osdmap --export-crush file

转储OSD运行图，-f的可用格式有plain和json，如未指定--format则转储为纯文本
ceph osd dump [--format {format}]

把OSD运行转储为树，每个OSD一行，包含权重和状态
ceph osd tree [--format {format}]

找出某对象在玛丽或应该在哪里
ceph osd map <pool-name> <object-name>

增加或挪动一个新OSD项目，要给出id/name/weight和位置参数
ceph osd crush set {id} {weight} [{loc1} [{loc2}...]]

从现有CRUSH图删除存储在的条目(OSD)
ceph osd crush remove {name}

从现有CRUSH图删除存在的空桶:
ceph osd crush remove {bucket-name}

把有效的桶从分级结构里的一个位置挪到另一个
ceph osd crush move {id} {loc1} [{loc2} ...]

设置{name}所指条目的权重为{weight}
ceph osd crush reweight {name} {weight}

创建集群快照
ceph osd cluster_snap {name}

把osd标记为丢失，有可能导致永久性数据丢失，慎用
ceph osd lost {id} [--yes-i-really-mean-it]

创建新OSD，如果未指定ID，有可能的话将自动分配个新ID
ceph osd create [{uuid}]

删除指定OSD
ceph osd rm [{id}...]

查询OSD运行图里的max_osd参数
ceph osd getmaxosd

导入指定CRUSH图
ceph osd setcrushmap -i file

设置OSD运行图的max_osd参数，扩展存储集群时有必要
ceph osd setmaxosd

把ID为{osd-num}的OSD标记为down
ceph osd down {osd-num}

把OSD {osd-num}标记为数据分布之外
ceph osd out {osd-num}

把OSD {osd-num} 标记为数据分布之内
ceph osd in {osd-num}

列出Ceph集群载入的类
ceph class list

设置或清空OSD运行图里的暂停标记。若设置了，不会有IO请求发送到任何OSD；用unpause清空此标记会导致重发未决请求
ceph osd pause
ceph osd unpause

给OSD设置一个增益权重，有效值在0和1之间，使得CRUSH重新归置一定数量的，本应该放到此处的数据，不会影响crush图里所分配的权重，在CRUSH分布算法没能理想执行时，可作为一种纠正手段
ceph osd reweight {osd-num} {weight}

重设所有滥用OSD权重，默认会下调到平均利用率120%的那些OSD，除非指定阈值
ceph osd reweight-by-utilization [threshold]

增加/删除黑名单里的地址，可指定有效期，默认1小时。
ceph osd blacklist add ADDRESS[:source_port] [TIME]
ceph osd blacklist rm ADDRESS[:source_port]

创建，删除存储池快照
ceph osd pool mksnap {pool-name} {snap-name}
ceph osd pool rmsnap {pool-name} {snap-name}

创建、删除、重命名存储池
ceph osd pool create {pool-name} pg_num [pgp_num]
ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]
ceph osd pool rename {old-name} {new-name}

更改存储池设置
ceph osd pool set {pool-name} {field} {value}
    可用field：
        size： 设置存储池内数据的副本数
        crash_replay_interval： 允许客户端重放确认而未提交的请求前等待的时间
        pg_num： 归置组数量
        pgp_num： 计算归置组存放的有效数量
        crush_ruleset： 用于归置映射的规则号

获取存储池配置值
ceph osd pool get {pool-name} {field}
    可用field：
        pg_num： 归置组数量
        pgp_num： 计算归置组存放的有效数量
        lpg_num： 本地归置组数量
        lpgp_num: 用于存放本地归置组的数量

向OSD {osd-num}下达一个洗刷命令，用通配符*把命令下达到所有OSD
ceph osd scrub {osd-num}

向osdN下达修复命令，*表示所有OSD
ceph osd repair N

在osdN上进行简单吞吐量测试
ceph tell osd.N bench [NUMBER_OF_OBJECTS] [BUTES_PER_WRITE]
```

### MDS子系统
```
更改运行mds参数
ceph tell mds.{mds-id} injectargs --{switch} {value} [--{switch} {value}]

打开调试消息
ceph mds stat

显示所有元数据服务器状态
ceph mds fail 0

set,dump,getmap,setmap.stop...
```

### 监视器子系统
```
查看监视器状态
ceph mon stat
./ceph quorum_status

刚连接的监视器的状态(用-m host:port另外指定)
ceph mon_status

监视器转储
ceph mon dump
```

## 日志记录和调试
[http://docs.ceph.org.cn/rados/troubleshooting/log-and-debug/](http://docs.ceph.org.cn/rados/troubleshooting/log-and-debug/)

## 监视器故障排除
[http://docs.ceph.org.cn/rados/troubleshooting/troubleshooting-mon/](http://docs.ceph.org.cn/rados/troubleshooting/troubleshooting-mon/)

## OSD故障排除
[http://docs.ceph.org.cn/rados/troubleshooting/troubleshooting-osd/](http://docs.ceph.org.cn/rados/troubleshooting/troubleshooting-osd/)

## 归置组排障
[http://docs.ceph.org.cn/rados/troubleshooting/troubleshooting-pg/](http://docs.ceph.org.cn/rados/troubleshooting/troubleshooting-pg/)

## 内存剖析
[http://docs.ceph.org.cn/rados/troubleshooting/memory-profiling/](http://docs.ceph.org.cn/rados/troubleshooting/memory-profiling/)

## CPU剖析
[http://docs.ceph.org.cn/rados/troubleshooting/cpu-profiling/](http://docs.ceph.org.cn/rados/troubleshooting/cpu-profiling/)

## CEPH存储集群API
[http://docs.ceph.org.cn/rados/api/](http://docs.ceph.org.cn/rados/api/)


------

[CEPH](http://docs.ceph.org.cn/)
















 









