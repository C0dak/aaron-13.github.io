# Ceph集群运维

------

[TOC]

## 操纵集群
### 用UPSTART控制CEPH
用ceph-deploy把Ceph Cuttlefish即更高版本部署到Ubuntu之后，可用基于事件的Upstart来启动，关闭Ceph节点上的守护进程。
列出Ceph作业和例程
`sudo initctl list | grep ceph`

启动所有守护进程
`sudo start ceph-all`

停止所有守护进程
`sudo stop ceph-all`

按类型启动/停止所有守护进程
```
sudo start/stop ceph-osd-all
sudo start/stop ceph-mon-all
sudo start/stop ceph-mds-all
```

启动/停止单个进程
```
sudo start/stop ceph-osd id={id}
sudo start/stop ceph-mon id={hostname}
sudo start/stop ceph-mds id={hostname}
```

### 运行CEPH
`{commandline} [options] [commands] [daemons]`

|选项|简写|描述|
|:-:|:-:|:-:|
|--verbose|-v|详细日志|
|--valgrind|N/A|用Valgrind调试|
|--allhosts|-a|在ceph.conf里配置的所有主机上执行，否则只在本机执行|
|--restart|N/A|核心转储后自动重启|
|--norestart|N/A|核心转储后不自动重启|
|--conf|-c|使用指定配置文件|

|命令|描述|
|:-:|:-:|
|start|启动守护进程|
|stop|停止守护进程|
|forcestop|暴力停止|
|killall|杀死某一类守护进程|
|cleanlogs|清理日志目录|
|cleanalllogs|清理日志目录内所有内容|

+ 通过SYSVINT机制运行CEPH
在CentOS，Redhat，Fedora和SLES发行版上可以通过sysvinit运行ceph

- 启动/停止所有守护进程
```
sudo /etc/init.d/ceph [iptions] [start|restart] [daemonType|daemonID]
sudo /etc/init.d/ceph -a start

sudo /etc/init.d/ceph [options] stop [daemonType|daemonID]
sudo /etc/init.d/ceph -a stop
```

- 启动/停止一类守护进程
```
启动本节点上某一类的所有Ceph守护进程
sudo /etc/init.d/ceph start/stop {daemon-type}
sudo /etc/init.d/ceph start/stop osd

启动费本机节点某一类的所有Ceph守护进程
sudo /etc/init.d/ceph -a start/stop {daemon-type}
sudo /etc/init.d/ceph -a start/stop osd
```

- 启动/停止单个守护进程
```
启动本节点上某个的Ceph守护进程
sudo /etc/init.d/ceph start/stop {daemon-type}.{instance}
sudo /etc/init.d/ceph start/stop osd.0

启动费本机节点某个的Ceph守护进程
sudo /etc/init.d/ceph -a start/stop {daemon-type}.{instance}
sudo /etc/init.d/ceph -a start/stop osd.0
```


## 监控集群
### 交互模式
```
ceph
ceph> health
ceph> status
ceph> quorum_status
ceph> mon_status
```

### 检查集群健康状态
```
ceph health

如果配置文件或密钥环不在默认路径下，需要指定
ceph -c /path/to/conf -k /path/to/keyring health
```

### 观察集群
```
ceph -w
```
输出信息包含：
+ 集群唯一标识符
+ 集群健康状况
+ 监视器图元版本，和监视器法定人数状态
+ OSD图元版本，和OSD状态摘要
+ 归置组图版本
+ 归置组和存储池数量
+ 器内存储的数量和对象数量的粗略统计
+ 数据总量

### 检查集群的使用情况
```
ceph df 
```

### 检查OSD状态
```
ceph osd stat
ceph osd dump
ceph osd tree
```

### 检查监视器状态
```
ceph mon stat
ceph mon dump
ceph quorum_status
```

### 检查MSD状态
```
ceph mds stat
ceph mds dump
```

### 使用管理套接字
```
ceph daemon {daemon-name}
ceph daemon {path-to-socket-file}

ceph daemon osd.0 foo
ceph daemon /var/run/ceph/ceph-osd.0.asok foo

ceph daemon {daemon-name} help
```


## 监控OSD和归置组

高性能和高可靠性要求容错方法来管理软硬件。Ceph没有单点故障，并且能在"降级"模式下继续提供服务。其数据归置引进了一个简介曾，可保证数据不会直接绑死到某个特定OSD地址，这也意味着追踪系统座位的根源要摄入归置组及底层OSD

**集群某一部分失效可能导致不能访问某个对象，但不会牵连其他对象。碰到这种问题时无需恐慌，只需要按步骤检查OSD和归置组，然后排除故障**

### 监控OSD
某OSD的状态可以是在集群内(in)或集群外(out),也可以是活着且在运行(up)或挂了且不在运行(down).
在一些情况下，集群不会返回HEALTH_OK：
+ 还没启动集群
+ 刚启动完，还没准备好，因为归置组正在被创建，OSD们正在相互创建连接
+ 刚增加或删除一个OSD
+ 刚修改完集群运行图

若一个OSD状态为down，启动它：
```
ceph -a start osd.1
```

### 归置组集
CRUSH要把归置组分配到OSD时，先查询这个存储池的副本数设置，再把归置组分配到OSD，这样就把个副本分配到了不同OSD。比如，如果存储池要求归置组有3个副本，CRUSH可能把它们分别分配到osd.1,osd.2,osd.3。考虑到设置于CRUSH运行图中的失败域，实际上CRUSH找出的伪随机位置，所以在大型集群中，很少能看到归置组被分配到了相邻的OSD。把涉及某个特定归置组副本的额一组OSD称为acting set。在一些情况下，位于acting set中的一个OSD down额或者不能为归置组内的对象提供服务，常见原因如下：
+ 增加或拆除一个OSD。然后CRUSH把那个归置组分配到了其他OSD，因此改变了Acting set的构成，并且用backfill进程启动了数据迁移
+ 一OSD down了，重启了，而现在正在恢复(recovering)
+ acting set中的一个OSD挂了，不能提供服务，另一个OSD临时接替其工作

```
获取归置组列表
ceph pg dump

根据指定归置组号查看哪些OSD位于Acting Set或Up Set里，执行：
ceph pg map {pg-num}

其结果会显示osdmap版本(eNNN)，归置组号({pg-num}), Up Set内的OSD(up[])，和ActingSet内的OSD(acting[])
osdmap eNNN pg {pg-num} -> up [0,1,2] acting [0,1,2]
```

### 节点互联
写入数据前，归置组必须处于active，而且应该是clean状态。假设一存储池的归置组有3个副本，为让Ceph确定归置组当前状态，一归置组的主OSD(即acting set内的第一个OSD)会与第二和第三OSD建立连接，并并就归置组的当前状态达成一致

### 监控归置组状态
```
ceph pg stat

ceph pg dump

ceph pg dump -o {filename} --format=json

ceph pg {poolnum}.{pg-id} query
```

#### 存储池在建中
创建存储池时，它会创建指定数量的归置组。Ceph在创建一个或多个归置组时显示creating；创建完成后，在其归置组的Acting Set里的OSD将建立互联；一旦互联完成，归置组状态应该变为active+clean，意思是Ceph客户端可以向归置组写入数据了

#### 互联建立中
Ceph为归置组建立互联时，会让存储归置组副本的OSD之间就其中的对象和元数据状态达成一致。Ceph完成互联，就意味着存储着归置组的OSD酒气当前状态达成一致。然而，互联过程的完成并不能表达各副本都有了数据的最新副本

#### 活跃
Ceph完成互联后，一归置组状态就会变成active。active表示数据以完好的保存到主归置组和副本归置组

#### 整洁
某一归置组处于clean状态时，主OSD和副本OSD已成功互联，并且没有偏离的归置组。Ceph已把归置组中的对象赋值了规定的次数

#### 已降级
当客户端向主OSD写入数据时，由主OSD负责把数据副本写入其余副本。主OSD把对象写入存储器后，在副本OSD创建完对象副本并报告给主OSD之前，主OSD会一直停留在degraded状态

归置组状态处于active+degraded状态，原因在于一OSD即使尚未持有所有对象也可以处于active状态。如果一OSD挂了，Ceph会把分配到此OSD的归置组标记为degraded；那个OSD重生后，它们必须重新互联。然而，客户端仍可以向处于degrade状态的归置组写入对象，只要它还在active状态

归置组也会被降级(degraded)，因为Ceph找不到本应存在于此归置组中的一个或多个对象，这是，不能读写找不到的对象那个，但仍能访问处于降级归置组中的其他对象

#### 恢复中
Ceph被设计为可容错，可抵御一定规模的软硬件问题。当某OSD挂了，其内的归置组会落后与别的归置组副本；此OSD重生时，归置组内容必须更新到当前状态，在此期间，OSD处于recovering状态。

Ceph提供了几个选项来均衡资源竞争，如新服务请求，恢复数据对象和恢复归置组到当前状态。
osd recovery delay start
    允许一OSD在开始恢复进程前，先重启，重建互联，甚至处理一些重放请求；
osd recovery threads
    限制恢复进程的线程数，默认为1线程
osd recovery thread timeout
    设置线程超时
osd recovery max active
    限制一OSD最多同时接受多少请求，以防它压力过大而不能正常服务
osd recovery max chunk
    限制恢复数据块尺寸，以防网路堵塞

#### 回填中
有新的OSD加入集群时，CRUSH会把现有集群内的部分归置组重新分配给它。配置型OSD立即接受重分配的归置组会使之过载，用归置组回填可使这个过程在后台开始。只要回填顺利完成，新OSD就可以对外服务了。

回填期间的几种状态：
backfill_wait
    一回填操作在等待时机，尚未开始
backfill    
    一回填操作正在进行
backfill_too_full
    需要进行回填，但因存储空间不足而不能够完成

某归置组不能回填时，其状态是incomplete

Ceph提供了多个选项来解决重分配归置组给一OSD时相关的负载问题
osd_max_backfills
    把双向的回填并发量都设置为10
osd backfill full \ ratio
    可让一OSD在接近沾满率(85%)时拒绝回填
osd backfill retry interval
    间隔重试时间
osd backfill scan min/max
    扫描间隔(64/512)

#### 被重映射

#### 发蔫

### 找出故障归置组
如果一个归置组状态不是active+clean时未必有问题，一般来说，归置组卡住时Ceph的自修复功能往往无能为力，卡转的状态细分为：
+ Unclean： 归置组有些对象的副本数未达到期望次数，应该在恢复中
+ Inactive： 归置组不能处理读写请求，因为它们在等着一个持有最新数据OSD回到up状态
+ Stale： 归置组处于一种未知状态

`ceph pg dump_stuck [unclean|inactive|stale|undersized|degraded]`

### 定位对象
要把对行啊改数据存入Ceph对象储存，一Ceph客户端必须：
+ 设置对象名
+ 指定一存储池

`ceph osd map {poolname} {objectname}`

### 练习定位一个对象
```
创建一个对象，给rados put命令指定一个对象名，一个包含数据的测试文件路径和一个存储池的名字：
rados put {obejct-name} {file-path} --pool=data
rados put test-object-1 textfile.txt --pool=data

用下列命令确认Ceph对象存储已经包含此对象
rados -p data ls

定位对象
ceph osd map {pool-name} {object-name}
ceph osd map data test-object-1

删除测试对象，用rados rm即可
rados rm test-object-1 --pool=data
```

## 用户管理

### 授权
Ceph用能力(capabilities,caps)这个属于来描述给认证用户的授权，这样才能使用监视器，OSD，和元数据服务器的功能。能力也用于限制对一存储池内的数据或某个名字空间的访问。

```
{daemon-type} 'allow {capability}' [{daemon-type} 'allow {capability}']

监视器能力： 监视器能力包括r,w,x和allow profile {cap}
    mon 'allow rwx'
    mon 'allow profile osd'

OSD能力：OSD能力包括r,w,x,class-read,class-write和profile osd。另外，OSD能力还支持存储池和命名空间的配置
    osd 'allow {capability}' [pool={poolname}] [namespace={namespace-name}]

元数据服务器能力
    mds 'allow'
```

**Ceph对象网关守护进程radosgw是Ceph存储集群的一种客户端，所以它没被表示成一种独立的Ceph存储集群守护进程类型**

### 管理用户
#### 罗列用户
```
ceph auth list
```

#### 获取用户
```
ceph auth get {TYPE.ID}
ceph auth get client.admin
ceph auth export {TYPE.ID}
```

#### 新增用户
```
ceph auth add client.john mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth get-or-create client.paul mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth get-or-create client.george mon 'allow r' osd 'allow rw pool=liverpool' -o george.keyring
ceph auth get-or-create client.ringo mon 'allow r' osd 'allow rw pool=liverpool' -o ringo.key
```

#### 修改用户能力
```
ceph auth caps USERTYPE.USERID {daemon} 'allow [r|w|x|*|...]' [pool={pool-name}] [namespace={namespace-name}]' [{daemon} 'allow [r|w|x|*|...] [pool={pool-name}] [namespace={namespace-name}]']

ceph auth get client.john
ceph auth caps client.john mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth caps client.paul mon 'allow rw' osd 'allow rwx pool=liverpool'
ceph auth caps client.brian-manager mon 'allow *' osd 'allow *'
```

#### 移除用户能力
```
ceph auth caps client.ringo mon '' osd ''
```

#### 删除用户
```
ceph auth del {TYPE}.{ID}
其中{TYPE}是client，osd，mon或mds之一，{ID}是用户名或守护进程ID
```

#### 查看用户密钥
```
将用户的身份验证密钥输出到标准输出
ceph auth primary-key {TYPE}.{ID}

{TYPE}是client，osd，mon，mds中的一种，{ID}是用户或守护进程的ID
当使用用户密钥填充客户端软件时，打印用户密钥很有用
mount -t ceph serverhost:/ mountpoint -o name=client.user,secret=`ceph auth print-key client.user`

```

#### 导入用户
```
ceph auth import -i /path/to/keyring
```

### 密钥环管理
默认路径：/etc/ceph/{$cluster.$name.keyring,$cluster.keyring,keyring,keyring.bin}

#### 创建密钥环
```
ceph-authtool --create-keyring /path/to/keyring
```

#### 将用户加入密钥环
```
ceph auth get client.admin -o /etc/ceph/ceph.client.admin.keyring

将用户导入密钥环时，可使用ceph-authtool指定目标密钥环和源密钥环
ceph-authtool /etc/ceph/ceph.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring
```

#### 创建用户
```
ceph-authtool -n client.ringo --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.keyring
```

### 命令行用法
Ceph支持用户性和密钥的下列用法：
--id | --user
    描述： Ceph用一个类型和ID(如TYPE.ID或client.admin，client.user)来标识用户，id，name和-n选项可用于指定用户名(如admin，user1,foo等)的ID部分，可以用--id指定用户并忽略类型
    ceph --id foo --keyring /path/to/keyring health
    ceph --user foo --keyring /path/to/keyring health

--name | -n
    描述： Ceph用一个类型和ID来标识用户，--name和-n选项可用于指定完整的额用户名，但必须指定用户类型(一般是client)和用户ID
    ceph --name client.foo --keyring /path/to/keyring health
    ceph -n client.foo --keyring /path/to/keyring health

--keyring
    描述： 包含一或多个用户名，密钥的密钥环路径。--secret选项提供了相同的功能。但它不能用于RADOS网关，其--secret另有用途，可以用ceph auth get-or-create获取密钥环并保存在本地，然后就可以改用其他用户而无需指定密钥环路径了
    rbd map --id foo --keyring /path/to/keyring mypool/myimage

## 数据归置概览
Ceph通过RADOS集群动态地存储，复制和重新均衡数据对象，很多不同用户因不同目的把对象存储在不同的存储池里，而他们都坐落于无数的OSD之上，所以Ceph的运营需要些数据归置计划。Ceph的数据归置计划概念主要有：
+ 存储池(Pool): Ceph在存储池内存储数据，它是对象存储的逻辑组；存储池管理者归置组数量，副本数量和存储池规则集。要往存储池里存储数据，用户必须通过认证，且权限合适，存储池可做快照
+ 归置组(Placement Group): Ceph把对象映射到归置组(PG),归置组是一逻辑对象池的片段，这些对象组团后再存储到OSD。归置组减少了各对象存入对应OSD时元数据数量，更多的归置组(如每OSD 100个)使得均衡更好
+ CRUSH图(CRUSH Map): CRUSH是重要组件，它使Ceph能伸缩自如而没有性能瓶颈，没有扩展限制，没有单点故障，它为CRUSH算法提供集群的物理拓扑，以此确定一个对象的数据及它的副本应该在哪里，怎样跨故障域存储，以提升数据安全


## 存储池
当部署时没有创建存储池，Ceph会用默认存储池存数据。存储池提供的功能：
+ 自恢复力：可以设置在不丢数据的前提下允许多少OSD失效，对多副本存储池来说，此值是一对象应达到的副本数。典型配置存储一个对象和它的一个副本(即size=2)，但可以更改副本数；对纠删编码的存储池来说，此值是编码块数(即纠删配置里的m=2)
+ 归置组： 可设置一个存储池的归置组数量。典型配置给每个OSD分配大约100个归置组，这样，不通过多计算资源就能得到较优的均衡。配置多个存储池时，药老绿道这些存储池和整个集群的归置组数量要合理
+ CRUSH规则： 当存储池里存数据的时候，与此存储池相关联的CRUSH规则集可控制CRUSH算法，并以此操纵集群内对象及其副本的赋值，可自定义存储池的CRUSH规则
+ 快照： 用ceph osd pool mksnap创建快照的时候，实际上创建了某一特定存储池的快照
+ 设置所有者： 可设置一个用户ID为一个存储池的所有者

### 列出存储池
```
ceph osd lspools
```

### 创建存储池
```
创建存储池前先看看存储池，归置组和CRUSH配置参考。最好在配置文件里重置默认归置组数量，因为默认值不理想
osd pool default pg num = 100
osd pool default pgp num = 100

要创建一个存储池，执行：
ceph osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated] [crush-ruleset-name] [expected-num-objects]
ceph osd pool create {pool-name} {pg-num} {pgp-num} erasure [erasure-code-profile] [crush-ruleset-name] [expected_num_objects]

{pool-name}
描述： 存储池名称，必须唯一
类型： string
是否必须： 必须

{pg-num}
描述： 存储池拥有的归置组总数
类型： 整数
是否必须： YES
默认值： 8

{pgp-num}
描述： 用于归置的归置组总数，此值应该与归置组总数相等，归置组分隔的情况下除外
类型： 整数
是否必须： 没指定的话读取默认值，或Ceph配置文件里的值
默认值： 8

{replicated|erasure}
描述： 存储池类型，可以是副本(保存多份对象那个副本，以便从丢失的OSD恢复)或纠删(获取类似RAID5的功能)。多副本存储池需要更多原始存储空间，但已实现所有Ceph操作，纠删存储池所需原始存储空间较少，但目前仅实现部分ceph操作
类型： string
是否必须： NO
默认值： replicated

[crush-ruleset-name]
描述：此存储池所用的CRUSH规则集没名字。指定的规则集必须存在
类型：string
是否必须： No
默认值： 对于多副本(replicated)存储池来说，器默认规则集由osd pool default crush replicated ruleset配置决定，此规则集必须存在。对于erasure-code编码的纠删码erasure存储池来说，不同的{pool-name}所使用的默认纠删码是不同的，如果它不存在的话，会显式地创建

[erasure-code-profile=profile]
描述： 仅用于纠删存储池，指定纠删码配置框架，此配置必须已由osd erasure-code-profile set定义
类型： String
是否必须： No

创建存储池时，要设置一个合理的归置组数量（如 100 ）。也要考虑到每 OSD 的归置组总数，因为归置组很耗计算资源，所以很多存储池和很多归置组（如 50 个存储池，各包含 100 归置组）会导致性能下降。收益递减点取决于 OSD 主机的强大

[expected-num-objects]
描述：为这个存储池预估的对象数，设置此值(要同时把filestore merge threshold设置为负数)后，在创建存储池时就会拆分PG文件夹，以免运行时拆分文件夹导致延时增大
类型：integer
是否必须：no
默认值： 0，创建存储池时不拆分目录
```

### 设置存储池配额
```
存储池配额可设置最大字节数，和/或每个存储池最大对象数
ceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}]
ceph osd pool set-quota data max_objects 10000
```

### 删除存储池
```
ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]
```

### 重命名存储池
```
ceph osd pool rename {current-pool-name} {new-pool-name}
如果重命名了一个存储池，且认证用户有每存储池能力，必须用新存储池名字更新用户的能力(caps)
```

### 查看存储池统计信息
```
rados df
```

### 拍下存储池快照
```
ceph osd pool mksnap {pool-name} {snap-name}
```

### 调整存储池选项值
```
ceph osd pool set {pool-name} {key} {value}
```

### 获取存储池选项值
```
ceph osd pool get {pool-name} {key}
```

### 设置对象副本数
```
ceph osd pool set {poolname} size {num-replicas}
ceph osd pool set data size/min_size 3
```

### 获取对象副本数
```
ceph osd dump |grep 'replicated size'
```







